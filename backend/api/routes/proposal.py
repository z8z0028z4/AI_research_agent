"""
ææ¡ˆç”Ÿæˆ API è·¯ç”±
================

è™•ç†ç ”ç©¶ææ¡ˆçš„ç”Ÿæˆã€ç·¨è¼¯å’Œä¿®è¨‚åŠŸèƒ½
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import asyncio
import sys
import os
import tempfile
import io
import requests
from docx import Document as DocxDocument
from docx.shared import Inches
from io import BytesIO
from ..services.docx_utils import (
    clean_text_for_xml,
    clean_markdown_text,
    get_image_stream_from_url,
    get_ghs_icon_stream,
    get_nfpa_icon_image_stream,
)

# æ·»åŠ åŸé …ç›®è·¯å¾‘åˆ° sys.path
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../app'))

# å»¶é²å°å…¥ä»¥é¿å…å¾ªç’°å°å…¥å•é¡Œ
# from knowledge_agent import agent_answer
# from rag_core import build_detail_experimental_plan_prompt
from pubchem_handler import chemical_metadata_extractor
from langchain_core.documents import Document

# SVG è½‰æ›ä¾è³´æª¢æŸ¥
try:
    from svglib.svglib import svg2rlg
    from reportlab.graphics import renderPDF
    SVGLIB_AVAILABLE = True
except ImportError as e:
    SVGLIB_AVAILABLE = False

# PyMuPDF ç”¨æ–¼ PDF åˆ° PNG è½‰æ›
try:
    import fitz
    PYMUPDF_AVAILABLE = True
except ImportError as e:
    PYMUPDF_AVAILABLE = False

import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from PIL import Image

router = APIRouter()

class ProposalRequest(BaseModel):
    """ææ¡ˆç”Ÿæˆè«‹æ±‚æ¨¡å‹"""
    research_goal: str
    user_feedback: Optional[str] = None
    previous_proposal: Optional[str] = None
    retrieval_count: Optional[int] = 10  # é è¨­æª¢ç´¢ 10 å€‹æ–‡æª”

class ProposalResponse(BaseModel):
    """ææ¡ˆç”ŸæˆéŸ¿æ‡‰æ¨¡å‹"""
    proposal: str
    chemicals: List[Dict[str, Any]]
    experiment_detail: Optional[str] = None
    citations: List[Dict[str, str]]
    not_found: List[str]
    # ä»¥å¯åºåˆ—åŒ–çš„çµæ§‹å›å‚³ chunksï¼š[{ page_content, metadata }]
    chunks: List[Dict[str, Any]]
    used_model: Optional[str] = None
    structured_proposal: Optional[Dict[str, Any]] = None
    structured_revision_explain: Optional[Dict[str, Any]] = None

class ProposalRevisionRequest(BaseModel):
    """ææ¡ˆä¿®è¨‚è«‹æ±‚æ¨¡å‹"""
    original_proposal: str
    user_feedback: str
    # ä¾†è‡ªå‰ç«¯çš„å¯åºåˆ—åŒ– chunks
    chunks: List[Dict[str, Any]]


def _serialize_chunks(chunks: List[Any]) -> List[Dict[str, Any]]:
    """å°‡ LangChain Document ç‰©ä»¶åºåˆ—åŒ–ç‚ºå¯å›å‚³çš„ dict çµæ§‹ã€‚"""
    serialized: List[Dict[str, Any]] = []
    for doc in chunks or []:
        try:
            serialized.append({
                "page_content": getattr(doc, "page_content", ""),
                "metadata": getattr(doc, "metadata", {}) or {}
            })
        except Exception:
            continue
    return serialized


def _deserialize_chunks(chunks_like: List[Dict[str, Any]]) -> List[Document]:
    """å°‡å‰ç«¯å‚³ä¾†çš„ dict çµæ§‹é‚„åŸç‚º LangChain Documentã€‚"""
    documents: List[Document] = []
    for item in chunks_like or []:
        page_content = item.get("page_content", "")
        metadata = item.get("metadata", {}) or {}
        documents.append(Document(page_content=page_content, metadata=metadata))
    return documents

@router.post("/proposal/generate", response_model=ProposalResponse)
async def generate_proposal(request: ProposalRequest):
    """
    ç”Ÿæˆç ”ç©¶ææ¡ˆ
    
    Args:
        request: åŒ…å«ç ”ç©¶ç›®æ¨™çš„è«‹æ±‚
        
    Returns:
        ç”Ÿæˆçš„ææ¡ˆå…§å®¹ï¼ŒåŒ…æ‹¬åŒ–å­¸å“ä¿¡æ¯å’Œå¯¦é©—ç´°ç¯€
    """
    import time
    import uuid
    
    # ç”Ÿæˆå”¯ä¸€çš„è«‹æ±‚ ID
    request_id = str(uuid.uuid4())[:8]
    start_time = time.time()
    
    print(f"ğŸš€ [DEBUG-{request_id}] ========== é–‹å§‹è™•ç†ææ¡ˆç”Ÿæˆè«‹æ±‚ ==========")
    print(f"ğŸš€ [DEBUG-{request_id}] æ™‚é–“æˆ³: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸš€ [DEBUG-{request_id}] æ”¶åˆ°è«‹æ±‚ research_goal = '{request.research_goal}'")
    print(f"ğŸš€ [DEBUG-{request_id}] retrieval_count = {request.retrieval_count}")
    print(f"ğŸš€ [DEBUG-{request_id}] è«‹æ±‚ä¾†æº: {request}")
    
    try:
        print(f"ğŸ” [DEBUG-{request_id}] æº–å‚™èª¿ç”¨ agent_answer with mode='make proposal'")
        
        # å»¶é²å°å…¥ä»¥é¿å…å¾ªç’°å°å…¥å•é¡Œ
        from knowledge_agent import agent_answer
        
        # èˆ‡ Streamlit Tab1 å°é½Šï¼šä½¿ç”¨æ¨¡å¼ make proposal ç”Ÿæˆææ¡ˆ
        result = agent_answer(request.research_goal, mode="make proposal", k=request.retrieval_count)
        
        print(f"ğŸ” [DEBUG-{request_id}] agent_answer èª¿ç”¨æˆåŠŸ")
        print(f"ğŸ” [DEBUG-{request_id}] result é¡å‹: {type(result)}")
        print(f"ğŸ” [DEBUG-{request_id}] result éµ: {list(result.keys())}")
        print(f"ğŸ” [DEBUG-{request_id}] result['answer'] é•·åº¦: {len(result.get('answer', ''))}")
        print(f"ğŸ” [DEBUG-{request_id}] result['answer'] å…§å®¹: {result.get('answer', '')[:200]}...")

        # å¾å›ç­”ä¸­æŠ½å–åŒ–å­¸å“è³‡è¨Šèˆ‡ææ¡ˆæ­£æ–‡
        print(f"ğŸ” [DEBUG-{request_id}] æº–å‚™èª¿ç”¨ chemical_metadata_extractor")
        chemical_metadata_list, not_found_list, proposal_answer = chemical_metadata_extractor(
            result.get("answer", "")
        )
        print(f"ğŸ” [DEBUG-{request_id}] chemical_metadata_extractor å®Œæˆ")
        print(f"ğŸ” [DEBUG-{request_id}] proposal_answer é•·åº¦: {len(proposal_answer)}")
        print(f"ğŸ” [DEBUG-{request_id}] chemical_metadata_list æ•¸é‡: {len(chemical_metadata_list)}")

        citations = result.get("citations", [])
        chunks = result.get("chunks", [])
        used_model = result.get("used_model", "unknown")

        # ä¿®å¾© citations ä¸­çš„ page æ¬„ä½é¡å‹å•é¡Œ
        fixed_citations = []
        for citation in citations:
            fixed_citation = citation.copy()
            # ç¢ºä¿ page æ¬„ä½æ˜¯å­—ä¸²
            if "page" in fixed_citation:
                fixed_citation["page"] = str(fixed_citation["page"])
            fixed_citations.append(fixed_citation)

        end_time = time.time()
        duration = end_time - start_time
        
        print(f"âœ… [DEBUG-{request_id}] ========== ææ¡ˆç”Ÿæˆå®Œæˆ ==========")
        print(f"âœ… [DEBUG-{request_id}] ç¸½è€—æ™‚: {duration:.2f} ç§’")
        print(f"âœ… [DEBUG-{request_id}] æª¢ç´¢åˆ°çš„æ–‡æª”æ•¸é‡: {len(chunks)}")
        print(f"âœ… [DEBUG-{request_id}] å¼•ç”¨æ•¸é‡: {len(fixed_citations)}")
        print(f"âœ… [DEBUG-{request_id}] åŒ–å­¸å“æ•¸é‡: {len(chemical_metadata_list)}")

        return ProposalResponse(
            proposal=proposal_answer,
            chemicals=chemical_metadata_list,
            citations=fixed_citations,
            not_found=not_found_list,
            chunks=_serialize_chunks(chunks),
            used_model=used_model
        )
        
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        print(f"âŒ [DEBUG-{request_id}] ========== ææ¡ˆç”Ÿæˆå¤±æ•— ==========")
        print(f"âŒ [DEBUG-{request_id}] ç¸½è€—æ™‚: {duration:.2f} ç§’")
        print(f"âŒ [DEBUG-{request_id}] éŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ææ¡ˆç”Ÿæˆå¤±æ•—: {str(e)}")

@router.post("/proposal/revise", response_model=ProposalResponse)
async def revise_proposal(request: ProposalRevisionRequest):
    """
    æ ¹æ“šç”¨æˆ¶åé¥‹ä¿®è¨‚ææ¡ˆ
    
    Args:
        request: åŒ…å«åŸå§‹ææ¡ˆå’Œç”¨æˆ¶åé¥‹çš„è«‹æ±‚
        
    Returns:
        ä¿®è¨‚å¾Œçš„ææ¡ˆå…§å®¹
    """
    try:
        # å»¶é²å°å…¥ä»¥é¿å…å¾ªç’°å°å…¥å•é¡Œ
        from knowledge_agent import agent_answer
        
        # èˆ‡ Streamlit Tab1 å°é½Šï¼šæ¡ç”¨ generate new idea æ¨¡å¼ï¼Œä¸¦å¸¶å…¥åŸå§‹ææ¡ˆèˆ‡ chunks
        result = agent_answer(
            request.user_feedback,
            mode="generate new idea",
            old_chunks=_deserialize_chunks(request.chunks),
            proposal=request.original_proposal,
        )

        # æª¢æŸ¥æ˜¯å¦æœ‰ç›´æ¥çš„ææ–™åˆ—è¡¨ï¼ˆä¾†è‡ªçµæ§‹åŒ–è¼¸å‡ºï¼‰
        if result.get("materials_list"):
            print(f"ğŸ” [DEBUG] ä½¿ç”¨çµæ§‹åŒ–æ•¸æ“šä¸­çš„ææ–™åˆ—è¡¨: {result['materials_list']}")
            # ç›´æ¥ä½¿ç”¨çµæ§‹åŒ–æ•¸æ“šä¸­çš„ææ–™åˆ—è¡¨
            from pubchem_handler import extract_and_fetch_chemicals, remove_json_chemical_block
            chemical_metadata_list, not_found_list = extract_and_fetch_chemicals(result["materials_list"])
            # æ¸…ç†æ–‡æœ¬ä¸­çš„ JSON åŒ–å­¸å“å¡Š
            proposal_answer = remove_json_chemical_block(result.get("answer", ""))
        else:
            # å›é€€åˆ°å¾æ–‡æœ¬ä¸­æå–
            print(f"ğŸ” [DEBUG] å›é€€åˆ°å¾æ–‡æœ¬ä¸­æå–ææ–™åˆ—è¡¨")
            chemical_metadata_list, not_found_list, proposal_answer = chemical_metadata_extractor(
                result.get("answer", "")
            )

        # ä¿®å¾© citations ä¸­çš„ page æ¬„ä½é¡å‹å•é¡Œ
        fixed_citations = []
        for citation in result.get("citations", []):
            fixed_citation = citation.copy()
            # ç¢ºä¿ page æ¬„ä½æ˜¯å­—ä¸²
            if "page" in fixed_citation:
                fixed_citation["page"] = str(fixed_citation["page"])
            fixed_citations.append(fixed_citation)

        return ProposalResponse(
            proposal=proposal_answer,
            chemicals=chemical_metadata_list,
            citations=fixed_citations,
            not_found=not_found_list,
            chunks=_serialize_chunks(result.get("chunks", [])),
            structured_proposal=result.get("structured_proposal")
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ææ¡ˆä¿®è¨‚å¤±æ•—: {str(e)}")

class ExperimentDetailRequest(BaseModel):
    """ç”Ÿæˆå¯¦é©—ç´°ç¯€è«‹æ±‚æ¨¡å‹"""
    proposal: str
    chunks: List[Dict[str, Any]]


@router.post("/proposal/experiment-detail")
async def generate_experiment_detail(request: ExperimentDetailRequest):
    """
    ç”Ÿæˆå¯¦é©—ç´°ç¯€
    
    Args:
        proposal: ææ¡ˆå…§å®¹
        chunks: ç›¸é—œæ–‡æª”ç‰‡æ®µ
        
    Returns:
        å¯¦é©—ç´°ç¯€å…§å®¹
    """
    try:
        # å»¶é²å°å…¥ä»¥é¿å…å¾ªç’°å°å…¥å•é¡Œ
        from knowledge_agent import agent_answer
        
        # èˆ‡ Streamlit Tab1 å°é½Šï¼šç”± agent ä»¥æŒ‡å®šæ¨¡å¼å±•é–‹å¯¦é©—ç´°ç¯€
        result = agent_answer(
            "",
            mode="expand to experiment detail",
            chunks=_deserialize_chunks(request.chunks),
            proposal=request.proposal,
        )

        return {
            "experiment_detail": result.get("answer", ""),
            "structured_experiment": result.get("structured_experiment", {}),
            "success": True,
            "retry_info": {
                "retry_count": getattr(result, 'retry_count', 0),
                "final_tokens": getattr(result, 'final_tokens', 0)
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯¦é©—ç´°ç¯€ç”Ÿæˆå¤±æ•—: {str(e)}")

@router.get("/proposal/status/{task_id}")
async def get_proposal_status(task_id: str):
    """
    ç²å–ææ¡ˆç”Ÿæˆä»»å‹™ç‹€æ…‹
    
    Args:
        task_id: ä»»å‹™ ID
        
    Returns:
        ä»»å‹™ç‹€æ…‹ä¿¡æ¯
    """
    # TODO: å¯¦ç¾ä»»å‹™ç‹€æ…‹è¿½è¹¤
    return {
        "task_id": task_id,
        "status": "completed",
        "progress": 100
    }


class DocxRequest(BaseModel):
    """DOCX ç”Ÿæˆè«‹æ±‚æ¨¡å‹"""
    proposal: str
    chemicals: List[Dict[str, Any]]
    not_found: List[str]
    experiment_detail: Optional[str] = ""
    citations: List[Dict[str, str]]

@router.post("/proposal/generate-docx")
async def generate_docx(request: DocxRequest):
    """
    ç”Ÿæˆ DOCX æ–‡ä»¶
    
    Args:
        request: åŒ…å«æ‰€æœ‰ææ¡ˆæ•¸æ“šçš„è«‹æ±‚
        
    Returns:
        DOCX æ–‡ä»¶ä¸‹è¼‰éŸ¿æ‡‰
    """
    tmp_path = None
    try:
        print(f"ğŸ” BACKEND DEBUG: é–‹å§‹ç”Ÿæˆ DOCX æ–‡ä»¶")
        print(f"ğŸ” BACKEND DEBUG: proposal é•·åº¦: {len(request.proposal)}")
        print(f"ğŸ” BACKEND DEBUG: chemicals æ•¸é‡: {len(request.chemicals)}")
        print(f"ğŸ” BACKEND DEBUG: experiment_detail é•·åº¦: {len(request.experiment_detail)}")
        print(f"ğŸ” BACKEND DEBUG: citations æ•¸é‡: {len(request.citations)}")
        
        doc = DocxDocument()
        doc.add_heading("AI Generated Research Proposal", 0)

        # Proposal Section
        doc.add_heading("Proposal", level=1)
        proposal_text = clean_text_for_xml(clean_markdown_text(request.proposal))
        doc.add_paragraph(proposal_text)

        # Chemical Table
        doc.add_heading("Chemical Summary Table", level=1)
        table = doc.add_table(rows=1, cols=8)  # å¤šå…©æ¬„ï¼šStructure + Safety
        hdr = table.rows[0].cells
        hdr[0].text = "Structure"
        hdr[1].text = "Name"
        hdr[2].text = "Formula"
        hdr[3].text = "MW"
        hdr[4].text = "Boiling Point (Â°C)"
        hdr[5].text = "Melting Point (Â°C)"
        hdr[6].text = "CAS No."
        hdr[7].text = "Safety Icons"

        for chem in request.chemicals:
            row = table.add_row().cells

            # Structure image
            img_url = chem.get("image_url")
            img_stream = get_image_stream_from_url(img_url)
            if img_stream:
                row[0].paragraphs[0].add_run().add_picture(img_stream, width=Inches(1))
            else:
                row[0].text = "No image"

            # Text fields - using cleaning functions
            row[1].text = clean_text_for_xml(chem.get("name", "-") or "-")
            row[2].text = clean_text_for_xml(chem.get("formula", "-") or "-")
            row[3].text = clean_text_for_xml(str(chem.get("weight", "-") or "-"))
            row[4].text = clean_text_for_xml(str(chem.get("boiling_point_c", "-") or "-"))
            row[5].text = clean_text_for_xml(str(chem.get("melting_point_c", "-") or "-"))
            row[6].text = clean_text_for_xml(chem.get("cas", "-") or "-")

            # Safety icons (GHS + NFPA)
            icons_cell = row[7].paragraphs[0]
            ghs_icons = chem.get("safety_icons", {}).get("ghs_icons", [])
            nfpa_icon_url = chem.get("safety_icons", {}).get("nfpa_image")

            for icon_url in ghs_icons:
                icon_stream = get_ghs_icon_stream(icon_url)
                if icon_stream:
                    run = icons_cell.add_run()
                    run.add_picture(icon_stream, width=Inches(0.3))

            if nfpa_icon_url:
                from urllib.parse import urlparse, parse_qs
                parsed = urlparse(nfpa_icon_url)
                nfpa_code = parse_qs(parsed.query).get("code", [""])[0]
                if nfpa_code:
                    image_stream = get_nfpa_icon_image_stream(nfpa_code)
                    if image_stream:
                        run = icons_cell.add_run()
                        run.add_picture(image_stream, width=Inches(0.3))

        # Not Found Chemicals
        if request.not_found:
            doc.add_heading("Not Found Chemicals", level=2)
            for name in request.not_found:
                doc.add_paragraph(f"- {clean_text_for_xml(name)}")

        # Experiment Details
        doc.add_heading("Experimental Plan", level=1)
        experiment_text = clean_text_for_xml(clean_markdown_text(request.experiment_detail))
        doc.add_paragraph(experiment_text)

        # Citations
        doc.add_heading("Citations", level=1)
        for i, c in enumerate(request.citations, 1):
            title = clean_text_for_xml(c.get('title', ''))
            page = clean_text_for_xml(str(c.get('page', '')))
            snippet = clean_text_for_xml(c.get('snippet', ''))
            doc.add_paragraph(f"[{i}] {title} | Page {page} | Snippet: {snippet}")

        # Save and Download
        with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
            doc.save(tmp.name)
            tmp_path = tmp.name
            print(f"ğŸ” BACKEND DEBUG: DOCX æ–‡ä»¶å·²ä¿å­˜åˆ°: {tmp_path}")

        print(f"ğŸ” BACKEND DEBUG: æº–å‚™è¿”å› FileResponse")
        return FileResponse(
            path=tmp_path,
            filename="proposal_report.docx",
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
        
    except Exception as e:
        print(f"âŒ BACKEND DEBUG: DOCX ç”Ÿæˆå¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.unlink(tmp_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=f"DOCX ç”Ÿæˆå¤±æ•—: {str(e)}") 

@router.post("/proposal/test-docx")
async def test_docx_generation():
    """
    æ¸¬è©¦ DOCX ç”ŸæˆåŠŸèƒ½
    """
    try:
        print(f"ğŸ” BACKEND DEBUG: æ¸¬è©¦ DOCX ç”Ÿæˆ")
        
        doc = DocxDocument()
        doc.add_heading("Test Document", 0)
        doc.add_paragraph("This is a test document to verify DOCX generation works.")

        # Save and Download
        with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
            doc.save(tmp.name)
            tmp_path = tmp.name
            print(f"ğŸ” BACKEND DEBUG: æ¸¬è©¦ DOCX æ–‡ä»¶å·²ä¿å­˜åˆ°: {tmp_path}")

        return FileResponse(
            path=tmp_path,
            filename="test_document.docx",
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
        
    except Exception as e:
        print(f"âŒ BACKEND DEBUG: æ¸¬è©¦ DOCX ç”Ÿæˆå¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"æ¸¬è©¦ DOCX ç”Ÿæˆå¤±æ•—: {str(e)}") 
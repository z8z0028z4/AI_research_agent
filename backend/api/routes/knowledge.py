"""
çŸ¥è­˜åº«æŸ¥è©¢APIè·¯ç”±
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import sys
import os

# æ·»åŠ appç›®éŒ„åˆ°è·¯å¾‘
app_path = os.path.join(os.path.dirname(__file__), "..", "..", "..", "app")
if app_path not in sys.path:
    sys.path.insert(0, app_path)

# ç§»é™¤ç›´æ¥å°å…¥ï¼Œä½¿ç”¨å»¶é²å°å…¥

router = APIRouter(prefix="/knowledge", tags=["knowledge"])

class KnowledgeQueryRequest(BaseModel):
    question: str
    retrieval_count: int = 10
    answer_mode: str = "rigorous"  # "rigorous" æˆ– "inference"

class KnowledgeQueryResponse(BaseModel):
    answer: str
    citations: List[Dict[str, Any]]
    chunks: List[Dict[str, Any]]

@router.post("/query", response_model=KnowledgeQueryResponse)
async def query_knowledge(request: KnowledgeQueryRequest):
    """
    çŸ¥è­˜åº«æŸ¥è©¢æ¥å£
    
    æ”¯æŒå…©ç¨®å›ç­”æ¨¡å¼ï¼š
    - rigorous: åš´è¬¹å¼•ç”¨æ¨¡å¼ï¼ŒåŸºæ–¼æ–‡ç»é€²è¡Œæº–ç¢ºå¼•ç”¨
    - inference: æ¨è«–æ¨¡å¼ï¼Œå…è¨±åŸºæ–¼æ–‡ç»é€²è¡Œæ¨è«–å’Œå‰µæ–°å»ºè­°
    """
    try:
        print(f"ğŸ” çŸ¥è­˜åº«æŸ¥è©¢è«‹æ±‚ï¼š{request.question}")
        print(f"ğŸ” æª¢ç´¢æ•¸é‡ï¼š{request.retrieval_count}")
        print(f"ğŸ” å›ç­”æ¨¡å¼ï¼š{request.answer_mode}")
        
        # å»¶é²å°å…¥rag_coreæ¨¡çµ„
        from rag_core import (
            load_paper_vectorstore,
            retrieve_chunks_multi_query,
            build_prompt,
            build_inference_prompt,
            call_llm
        )
        
        # è¼‰å…¥æ–‡ç»å‘é‡æ•¸æ“šåº«
        vectorstore = load_paper_vectorstore()
        
        # æª¢ç´¢æ–‡æª”ç‰‡æ®µï¼ˆç›´æ¥ä½¿ç”¨ç”¨æˆ¶å•é¡Œï¼Œä¸é€²è¡ŒæŸ¥è©¢æ“´å±•ï¼‰
        chunks = retrieve_chunks_multi_query(
            vectorstore=vectorstore,
            query_list=[request.question],  # ç›´æ¥ä½¿ç”¨ç”¨æˆ¶å•é¡Œ
            k=request.retrieval_count,
            fetch_k=request.retrieval_count * 2,
            score_threshold=0.35
        )
        
        if not chunks:
            raise HTTPException(status_code=404, detail="æœªæ‰¾åˆ°ç›¸é—œæ–‡ç»")
        
        print(f"ğŸ” æª¢ç´¢åˆ° {len(chunks)} å€‹æ–‡æª”ç‰‡æ®µ")
        
        # æ ¹æ“šå›ç­”æ¨¡å¼é¸æ“‡ä¸åŒçš„promptæ§‹å»ºå‡½æ•¸
        if request.answer_mode == "rigorous":
            # åš´è¬¹å¼•ç”¨æ¨¡å¼
            system_prompt, citations = build_prompt(chunks, request.question)
        elif request.answer_mode == "inference":
            # æ¨è«–æ¨¡å¼
            system_prompt, citations = build_inference_prompt(chunks, request.question)
        else:
            raise HTTPException(status_code=400, detail="ç„¡æ•ˆçš„å›ç­”æ¨¡å¼")
        
        # èª¿ç”¨LLMç”Ÿæˆå›ç­”
        answer = call_llm(system_prompt)
        
        if not answer:
            raise HTTPException(status_code=500, detail="ç”Ÿæˆå›ç­”å¤±æ•—")
        
        # è½‰æ›chunksç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_chunks = []
        for chunk in chunks:
            serializable_chunk = {
                "page_content": chunk.page_content,
                "metadata": chunk.metadata
            }
            serializable_chunks.append(serializable_chunk)
        
        return KnowledgeQueryResponse(
            answer=answer,
            citations=citations,
            chunks=serializable_chunks
        )
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ çŸ¥è­˜åº«æŸ¥è©¢å¤±æ•—ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"æŸ¥è©¢å¤±æ•—ï¼š{str(e)}") 
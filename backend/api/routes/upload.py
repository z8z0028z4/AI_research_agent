"""
æ–‡ä»¶ä¸Šå‚³ API è·¯ç”±
================

è™•ç†æ–‡ä»¶ä¸Šå‚³ã€è™•ç†å’Œç‹€æ…‹æŸ¥è©¢åŠŸèƒ½
"""

from fastapi import APIRouter, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import os
import sys
import tempfile
import shutil
import time
import logging
import re
from pathlib import Path
from datetime import datetime

# æ·»åŠ åŸé …ç›®è·¯å¾‘åˆ° sys.path
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../app'))

# ä½¿ç”¨çµ•å°å°å…¥ä¾†é¿å…ç›¸å°å°å…¥å•é¡Œ
import sys
import os
# sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../../app'))  # å·²é‡çµ„ï¼Œä¸å†éœ€è¦

from backend.services.file_service import process_uploaded_files
from backend.services.embedding_service import embed_documents_from_metadata, embed_experiment_txt_batch, get_vectorstore_stats
from backend.services.excel_service import export_new_experiments_to_txt
from backend.config import EXPERIMENT_DIR

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

router = APIRouter()

class FileUploadResponse(BaseModel):
    """æ–‡ä»¶ä¸Šå‚³éŸ¿æ‡‰æ¨¡å‹"""
    success: bool
    message: str
    file_info: Dict[str, Any]
    processing_status: str

class ProcessingStatus(BaseModel):
    """è™•ç†ç‹€æ…‹æ¨¡å‹"""
    task_id: str
    status: str  # pending, processing, completed, failed
    progress: int
    message: str
    results: Optional[Dict[str, Any]] = None

class VectorStatsResponse(BaseModel):
    """å‘é‡çµ±è¨ˆéŸ¿æ‡‰æ¨¡å‹"""
    paper_vectors: int
    experiment_vectors: int
    total_vectors: int

# å­˜å„²è™•ç†ä»»å‹™ç‹€æ…‹
processing_tasks = {}

@router.post("/upload/files", response_model=FileUploadResponse)
async def upload_files(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...)
):
    """
    ä¸Šå‚³ä¸¦è™•ç†æ–‡ä»¶
    
    Args:
        background_tasks: å¾Œå°ä»»å‹™ç®¡ç†å™¨
        files: ä¸Šå‚³çš„æ–‡ä»¶åˆ—è¡¨
        
    Returns:
        ä¸Šå‚³çµæœå’Œè™•ç†ç‹€æ…‹
    """
    try:
        # å‰µå»ºè‡¨æ™‚ç›®éŒ„å­˜å„²ä¸Šå‚³çš„æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        uploaded_files = []
        
        # ä¿å­˜ä¸Šå‚³çš„æ–‡ä»¶
        for file in files:
            if file.filename:
                file_path = os.path.join(temp_dir, file.filename)
                # é‡ç½®æ–‡ä»¶æŒ‡é‡åˆ°é–‹å§‹ä½ç½®ï¼Œç¢ºä¿èƒ½è®€å–å®Œæ•´å…§å®¹
                file.file.seek(0)
                with open(file_path, "wb") as buffer:
                    shutil.copyfileobj(file.file, buffer)
                
                # é©—è­‰æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
                saved_size = os.path.getsize(file_path)
                if saved_size == 0:
                    logger.warning(f"âš ï¸ æ–‡ä»¶ {file.filename} ä¿å­˜å¾Œå¤§å°ç‚º 0 bytes")
                else:
                    logger.info(f"âœ… æ–‡ä»¶ {file.filename} æˆåŠŸä¿å­˜ï¼Œå¤§å°: {saved_size} bytes")
                
                uploaded_files.append(file_path)
        
        # ç”Ÿæˆä»»å‹™ ID
        task_id = f"task_{len(processing_tasks) + 1}"
        
        # åˆå§‹åŒ–ä»»å‹™ç‹€æ…‹
        processing_tasks[task_id] = {
            "status": "pending",
            "progress": 0,
            "message": "æ–‡ä»¶ä¸Šå‚³å®Œæˆï¼Œé–‹å§‹è™•ç†...",
            "results": None
        }
        
        # åœ¨å¾Œå°è™•ç†æ–‡ä»¶
        background_tasks.add_task(
            process_files_background,
            task_id,
            uploaded_files,
            temp_dir
        )
        
        return FileUploadResponse(
            success=True,
            message="æ–‡ä»¶ä¸Šå‚³æˆåŠŸï¼Œé–‹å§‹è™•ç†",
            file_info={
                "task_id": task_id,
                "file_count": len(files),
                "file_names": [f.filename for f in files if f.filename]
            },
            processing_status="pending"
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šå‚³å¤±æ•—: {str(e)}")

def _classify_files_by_type(file_paths: List[str]) -> Dict[str, List[str]]:
    """æ ¹æ“šå‰¯æª”åå°‡æª”æ¡ˆåˆ†é¡ç‚ºè«–æ–‡æˆ–å¯¦é©—è³‡æ–™ã€‚"""
    papers: List[str] = []
    experiments: List[str] = []
    other: List[str] = []

    for path in file_paths:
        ext = os.path.splitext(path)[1].lower()
        if ext in [".pdf", ".docx"]:
            papers.append(path)
        elif ext in [".xlsx", ".xls"]:
            experiments.append(path)
        else:
            other.append(path)

    return {"type": "mixed", "papers": papers, "experiments": experiments, "others": other}


async def process_files_background(task_id: str, file_paths: List[str], temp_dir: str):
    """
    å¾Œå°è™•ç†æ–‡ä»¶
    
    Args:
        task_id: ä»»å‹™ ID
        file_paths: æ–‡ä»¶è·¯å¾‘åˆ—è¡¨
        temp_dir: è‡¨æ™‚ç›®éŒ„
    """
    start_time = time.time()
    logger.info(f"ğŸš€ é–‹å§‹è™•ç†ä»»å‹™ {task_id}ï¼Œå…± {len(file_paths)} å€‹æ–‡ä»¶")
    logger.info(f"ğŸ“ è‡¨æ™‚ç›®éŒ„: {temp_dir}")
    
    try:
        # æ›´æ–°ç‹€æ…‹ç‚ºè™•ç†ä¸­
        processing_tasks[task_id]["status"] = "processing"
        processing_tasks[task_id]["progress"] = 0
        processing_tasks[task_id]["message"] = "é–‹å§‹è™•ç†..."
        
        # åˆ†ææ–‡ä»¶é¡å‹ï¼ˆä¾å‰¯æª”ååˆ†é¡ï¼‰
        logger.info("ğŸ” é–‹å§‹åˆ†ææ–‡ä»¶é¡å‹...")
        file_info = _classify_files_by_type(file_paths)
        
        # è¨˜éŒ„æ–‡ä»¶åˆ†é¡çµæœ
        for file_type, files in file_info.items():
            if files:
                logger.info(f"ğŸ“‚ {file_type}: {len(files)} å€‹æ–‡ä»¶")
                for f in files:
                    file_size = os.path.getsize(f) if os.path.exists(f) else 0
                    logger.info(f"   ğŸ“„ {os.path.basename(f)} ({file_size} bytes)")
        
        # è¨ˆç®—é€²åº¦åˆ†é…
        has_papers = bool(file_info.get("papers"))
        has_experiments = bool(file_info.get("experiments"))
        
        logger.info(f"ğŸ“Š æ–‡ä»¶é¡å‹åˆ†æå®Œæˆ - è«–æ–‡: {has_papers}, å¯¦é©—: {has_experiments}")
        
        # æ ¹æ“šæ–‡ä»¶é¡å‹èª¿æ•´é€²åº¦åˆ†é…
        if has_papers and has_experiments:
            # æ··åˆæ–‡ä»¶ï¼šè«–æ–‡ä½”70%ï¼Œå¯¦é©—ä½”25%
            paper_progress_range = 70
            experiment_progress_range = 25
        elif has_papers:
            # åªæœ‰è«–æ–‡ï¼šè«–æ–‡ä½”95%
            paper_progress_range = 95
            experiment_progress_range = 0
        elif has_experiments:
            # åªæœ‰å¯¦é©—ï¼šå¯¦é©—ä½”95%
            paper_progress_range = 0
            experiment_progress_range = 95
        else:
            # å…¶ä»–æ–‡ä»¶ï¼šç›´æ¥å®Œæˆ
            paper_progress_range = 0
            experiment_progress_range = 0
        
        processing_tasks[task_id]["progress"] = 10
        processing_tasks[task_id]["message"] = "é–‹å§‹è™•ç†è«–æ–‡è³‡æ–™..."
        
        # å®šç¾©é€²åº¦å›èª¿å‡½æ•¸
        def update_progress(message: str, progress_percent: int = None):
            if progress_percent is not None:
                processing_tasks[task_id]["progress"] = progress_percent
            processing_tasks[task_id]["message"] = message
            logger.info(f"ğŸ“ˆ é€²åº¦æ›´æ–°: {processing_tasks[task_id]['progress']}% - {message}")
        
        # è™•ç†è«–æ–‡è³‡æ–™ï¼ˆæå– metadata ä¸¦åµŒå…¥å‘é‡ï¼‰
        paper_results: List[Dict[str, Any]] = []
        
        # åˆå§‹åŒ–é€²åº¦è®Šé‡
        current_progress = 10
        
        if has_papers:
            logger.info("ğŸ“š é–‹å§‹è™•ç†è«–æ–‡è³‡æ–™...")
            
            # è¨ˆç®—è«–æ–‡è™•ç†çš„é€²åº¦åˆ†é…ï¼ˆç¸½å…±100%ï¼Œåˆ†ç‚º4å€‹é—œéµç¯€é»ï¼‰
            # ç¯€é»1: æ–‡ä»¶ä¸Šå‚³é–‹å§‹ (10%)
            # ç¯€é»2: å…ƒæ•¸æ“šæå–é–‹å§‹ (25%)
            # ç¯€é»3: å‘é‡åµŒå…¥é–‹å§‹ (50%)
            # ç¯€é»4: è™•ç†å®Œæˆ (100%)
            
            # ç¯€é»1: æ–‡ä»¶ä¸Šå‚³å·²å®Œæˆ (10%)
            current_progress = 10
            
            # ç¯€é»2: å…ƒæ•¸æ“šæå–é–‹å§‹ (25%)
            logger.info("ğŸ“„ é–‹å§‹å…ƒæ•¸æ“šæå–...")
            metadata_start_time = time.time()
            current_progress = 25  # é€²å…¥å…ƒæ•¸æ“šæå–éšæ®µï¼Œè¨­ç½®ç‚º25%
            update_progress("ğŸ“„ é–‹å§‹å…ƒæ•¸æ“šæå–...", current_progress)
        
        # å‰µå»ºä¸€å€‹é€²åº¦è¿½è¹¤è®Šé‡
        extraction_progress = current_progress
        
        # åˆå§‹åŒ–æ™‚é–“è®Šé‡
        metadata_start_time = time.time()
        metadata_end_time = time.time()
        
        def extraction_progress_callback(msg: str):
                nonlocal extraction_progress
                # æ ¹æ“šæ¶ˆæ¯å…§å®¹æ›´æ–°é€²åº¦
                if "æå–ç¬¬" in msg and "å€‹æ–‡ä»¶å…ƒæ•¸æ“š" in msg:
                    try:
                        # åŒ¹é… "æå–ç¬¬ X/Y å€‹æ–‡ä»¶å…ƒæ•¸æ“šï¼š{filename}" æ ¼å¼
                        match = re.search(r'æå–ç¬¬ (\d+)/(\d+) å€‹æ–‡ä»¶å…ƒæ•¸æ“š', msg)
                        if match:
                            current_file = int(match.group(1))
                            total_files = int(match.group(2))
                            # è¨ˆç®—é€²åº¦ï¼š25% åˆ° 50% ä¹‹é–“
                            progress = 25 + int((current_file / total_files) * 25)
                            extraction_progress = progress
                            update_progress(msg, progress)
                            logger.info(f"ğŸ“ˆ å…ƒæ•¸æ“šæå–é€²åº¦: {current_file}/{total_files} ({progress}%)")
                        else:
                            update_progress(msg, extraction_progress)
                    except Exception as e:
                        logger.warning(f"âš ï¸ é€²åº¦è§£æå¤±æ•—: {e}")
                        update_progress(msg, extraction_progress)
                else:
                    update_progress(msg, extraction_progress)
        
        if has_papers:
            metadata_list: List[Dict[str, Any]] = process_uploaded_files(
                file_info["papers"], 
                status_callback=extraction_progress_callback
            )
            
            metadata_end_time = time.time()
            logger.info(f"âœ… å…ƒæ•¸æ“šæå–å®Œæˆï¼Œè€—æ™‚: {metadata_end_time - metadata_start_time:.2f}ç§’")
            logger.info(f"ğŸ“Š æˆåŠŸæå– {len(metadata_list)} å€‹æ–‡ä»¶çš„å…ƒæ•¸æ“š")
            
            # è¨˜éŒ„æ¯å€‹æ–‡ä»¶çš„å…ƒæ•¸æ“šæå–çµæœ
            for i, metadata in enumerate(metadata_list):
                logger.info(f"   ğŸ“„ {i+1}. {metadata.get('title', 'æœªçŸ¥æ¨™é¡Œ')} - DOI: {metadata.get('doi', 'ç„¡')}")
            
            paper_results.extend(metadata_list)
            update_progress("âœ… å…ƒæ•¸æ“šæå–å®Œæˆ", current_progress)
        else:
            metadata_list = []
        
        # ç¯€é»3: å‘é‡åµŒå…¥é–‹å§‹ (50%)
        current_progress = 50  # é€²å…¥å‘é‡åµŒå…¥éšæ®µï¼Œè¨­ç½®ç‚º50%
        update_progress("âœ… é–‹å§‹å‘é‡åµŒå…¥", current_progress)
        
        # åˆå§‹åŒ–æ™‚é–“è®Šé‡
        embedding_start_time = time.time()
        paper_start_time = time.time()
        
        logger.info("ğŸ”¢ é–‹å§‹å‘é‡åµŒå…¥...")
        update_progress("ğŸ“š é–‹å§‹å‘é‡åµŒå…¥...", current_progress)
        
        # è¨ˆç®—æ¯å€‹æ–‡ä»¶çš„åµŒå…¥é€²åº¦
        def embedding_progress_callback(msg: str):
            nonlocal current_progress  # è²æ˜ä½¿ç”¨å¤–éƒ¨è®Šé‡
            # å¾æ¶ˆæ¯ä¸­æå–ç•¶å‰è™•ç†çš„æ–‡ä»¶ç´¢å¼•
            if "è™•ç†ç¬¬" in msg and "å€‹æ–‡ä»¶" in msg:
                try:
                    # æå– "è™•ç†ç¬¬ X/Y å€‹æ–‡ä»¶" ä¸­çš„ X
                    match = re.search(r'è™•ç†ç¬¬ (\d+)/(\d+) å€‹æ–‡ä»¶', msg)
                    if match:
                        current_file = int(match.group(1))
                        total_files = int(match.group(2))
                        # è¨ˆç®—é€²åº¦ï¼š50% åˆ° 90% ä¹‹é–“
                        progress = 50 + int((current_file / total_files) * 40)
                        update_progress(msg, progress)
                        logger.info(f"ğŸ”¢ å‘é‡åµŒå…¥é€²åº¦: {current_file}/{total_files} ({progress}%)")
                    else:
                        update_progress(msg, current_progress)  # ä½¿ç”¨ç•¶å‰é€²åº¦
                except (ValueError, AttributeError) as parse_error:
                    logger.warning(f"è§£æé€²åº¦ä¿¡æ¯å¤±æ•—: {parse_error}")
                    update_progress(msg, current_progress)  # ä½¿ç”¨ç•¶å‰é€²åº¦
            elif "å‘é‡åµŒå…¥æ‰¹æ¬¡" in msg:
                try:
                    # æå– "å‘é‡åµŒå…¥æ‰¹æ¬¡ X/Y" ä¸­çš„é€²åº¦ä¿¡æ¯
                    match = re.search(r'å‘é‡åµŒå…¥æ‰¹æ¬¡ (\d+)/(\d+)', msg)
                    if match:
                        current_batch = int(match.group(1))
                        total_batches = int(match.group(2))
                        # å‘é‡åµŒå…¥éšæ®µï¼š90% åˆ° 95% ä¹‹é–“
                        progress = 90 + int((current_batch / total_batches) * 5)
                        update_progress(msg, progress)
                        logger.info(f"ğŸ”¢ å‘é‡åµŒå…¥æ‰¹æ¬¡: {current_batch}/{total_batches} ({progress}%)")
                    else:
                        update_progress(msg, current_progress)  # ä½¿ç”¨ç•¶å‰é€²åº¦
                except (ValueError, AttributeError) as parse_error:
                    logger.warning(f"è§£ææ‰¹æ¬¡é€²åº¦ä¿¡æ¯å¤±æ•—: {parse_error}")
                    update_progress(msg, current_progress)  # ä½¿ç”¨ç•¶å‰é€²åº¦
            elif "é–‹å§‹å‘é‡åµŒå…¥" in msg:
                # å‘é‡åµŒå…¥é–‹å§‹ï¼Œè¨­ç½®é€²åº¦ç‚º50%
                update_progress(msg, 50)
                logger.info("ğŸ”¢ å‘é‡åµŒå…¥é–‹å§‹")
            elif "å‘é‡åµŒå…¥å®Œæˆ" in msg:
                # å‘é‡åµŒå…¥å®Œæˆï¼Œè¨­ç½®é€²åº¦ç‚º95%
                update_progress(msg, 95)
                embedding_end_time = time.time()
                logger.info(f"âœ… å‘é‡åµŒå…¥å®Œæˆï¼Œè€—æ™‚: {embedding_end_time - embedding_start_time:.2f}ç§’")
            else:
                update_progress(msg, current_progress)  # ä½¿ç”¨ç•¶å‰é€²åº¦
                logger.info(f"ğŸ“ åµŒå…¥é€²åº¦: {msg}")
        
        logger.info(f"ğŸ”¢ é–‹å§‹å° {len(metadata_list)} å€‹æ–‡ä»¶é€²è¡Œå‘é‡åµŒå…¥...")
        embed_documents_from_metadata(
            metadata_list, 
            status_callback=embedding_progress_callback
        )
        
        paper_end_time = time.time()
        logger.info(f"âœ… è«–æ–‡è™•ç†å®Œæˆï¼Œç¸½è€—æ™‚: {paper_end_time - paper_start_time:.2f}ç§’")
        
        # è™•ç†å¯¦é©—è³‡æ–™ï¼ˆExcel -> txt -> å‘é‡åµŒå…¥ï¼‰
        experiment_results: List[Dict[str, Any]] = []
        if has_experiments:
            logger.info("ğŸ§ª é–‹å§‹è™•ç†å¯¦é©—è³‡æ–™...")
            experiment_start_time = time.time()
            
            # è¨­ç½®å¯¦é©—è™•ç†çš„èµ·å§‹é€²åº¦
            experiment_start_progress = 50  # å¯¦é©—è™•ç†å¾50%é–‹å§‹
            processing_tasks[task_id]["progress"] = experiment_start_progress
            processing_tasks[task_id]["message"] = "è™•ç†å¯¦é©—è³‡æ–™..."
            
            for i, f in enumerate(file_info["experiments"]):
                try:
                    logger.info(f"ğŸ§ª è™•ç†å¯¦é©—æ–‡ä»¶ {i+1}/{len(file_info['experiments'])}: {os.path.basename(f)}")
                    processing_tasks[task_id]["message"] = f"è™•ç†å¯¦é©—æ–‡ä»¶ {i+1}/{len(file_info['experiments'])}..."
                    processing_tasks[task_id]["progress"] = experiment_start_progress + int((i / len(file_info["experiments"])) * experiment_progress_range)
                    
                    # è¨˜éŒ„æ–‡ä»¶å¤§å°
                    file_size = os.path.getsize(f) if os.path.exists(f) else 0
                    logger.info(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} bytes")
                    
                    # è½‰æ›Excelç‚ºTXT
                    logger.info(f"   ğŸ“„ é–‹å§‹è½‰æ›Excelç‚ºTXT...")
                    excel_start_time = time.time()
                    df, txt_paths = export_new_experiments_to_txt(
                        excel_path=f,
                        output_dir=EXPERIMENT_DIR
                    )
                    excel_end_time = time.time()
                    logger.info(f"   âœ… Excelè½‰æ›å®Œæˆï¼Œç”Ÿæˆ {len(txt_paths)} å€‹TXTæ–‡ä»¶ï¼Œè€—æ™‚: {excel_end_time - excel_start_time:.2f}ç§’")
                    
                    # å‘é‡åµŒå…¥
                    logger.info(f"   ğŸ”¢ é–‹å§‹å¯¦é©—æ•¸æ“šå‘é‡åµŒå…¥...")
                    embed_start_time = time.time()
                    result = embed_experiment_txt_batch(txt_paths)
                    embed_end_time = time.time()
                    logger.info(f"   âœ… å¯¦é©—æ•¸æ“šå‘é‡åµŒå…¥å®Œæˆï¼Œè€—æ™‚: {embed_end_time - embed_start_time:.2f}ç§’")
                    
                    experiment_results.append({
                        "file": f,
                        "txt_paths": txt_paths,
                        "embedded_count": len(txt_paths)
                    })
                    
                except Exception as e:
                    logger.error(f"âŒ å¯¦é©—æ–‡ä»¶è™•ç†å¤±æ•— {os.path.basename(f)}: {e}")
                    experiment_results.append({
                        "file": f,
                        "error": str(e)
                    })
            
            experiment_end_time = time.time()
            logger.info(f"âœ… å¯¦é©—è™•ç†å®Œæˆï¼Œç¸½è€—æ™‚: {experiment_end_time - experiment_start_time:.2f}ç§’")
        
        # ç¯€é»4: è™•ç†å®Œæˆ (100%)
        # æ·»åŠ çŸ­æš«å»¶é²ï¼Œè®“å‰ç«¯æœ‰æ©Ÿæœƒçœ‹åˆ°95%çš„é€²åº¦
        time.sleep(0.5)  # å»¶é²0.5ç§’
        
        # æ›´æ–°é€²åº¦åˆ°98%ï¼Œè¡¨ç¤ºæ­£åœ¨å®Œæˆæœ€å¾Œçš„çµ±è¨ˆæ›´æ–°
        processing_tasks[task_id]["progress"] = 98
        processing_tasks[task_id]["message"] = "æ­£åœ¨å®Œæˆè™•ç†..."
        
        # å†å»¶é²ä¸€ä¸‹ï¼Œè®“å‰ç«¯çœ‹åˆ°98%çš„é€²åº¦
        time.sleep(0.3)
        
        processing_tasks[task_id]["progress"] = 100
        processing_tasks[task_id]["message"] = "è™•ç†å®Œæˆ"
        
        # æ›´æ–°å‘é‡çµ±è¨ˆç·©å­˜
        logger.info("ğŸ“Š é–‹å§‹æ›´æ–°å‘é‡çµ±è¨ˆç·©å­˜...")
        vector_stats = {}
        try:
            from main import update_vector_stats_cache, get_cached_vector_stats
            update_vector_stats_cache()
            cached_stats = get_cached_vector_stats()
            vector_stats = {
                "paper_vectors": cached_stats["paper_vectors"],
                "experiment_vectors": cached_stats["experiment_vectors"],
                "total_vectors": cached_stats["total_vectors"]
            }
            logger.info(f"ğŸ“Š å‘é‡çµ±è¨ˆç·©å­˜æ›´æ–° - è«–æ–‡: {vector_stats['paper_vectors']}, å¯¦é©—: {vector_stats['experiment_vectors']}, ç¸½è¨ˆ: {vector_stats['total_vectors']}")
            print(f"ğŸ“Š å‘é‡çµ±è¨ˆç·©å­˜æ›´æ–° - è«–æ–‡: {vector_stats['paper_vectors']}, å¯¦é©—: {vector_stats['experiment_vectors']}, ç¸½è¨ˆ: {vector_stats['total_vectors']}")
        except Exception as e:
            logger.error(f"âš ï¸ ç„¡æ³•æ›´æ–°å‘é‡çµ±è¨ˆç·©å­˜: {e}")
            print(f"âš ï¸ ç„¡æ³•æ›´æ–°å‘é‡çµ±è¨ˆç·©å­˜: {e}")
            vector_stats = {"paper_vectors": 0, "experiment_vectors": 0, "total_vectors": 0}
        
        # æ›´æ–°æœ€çµ‚çµæœ
        end_time = time.time()
        total_time = end_time - start_time
        logger.info(f"ğŸ‰ ä»»å‹™ {task_id} è™•ç†å®Œæˆï¼Œç¸½è€—æ™‚: {total_time:.2f}ç§’")
        
        processing_tasks[task_id]["status"] = "completed"
        # é€²åº¦å·²ç¶“åœ¨ä¹‹å‰è¨­ç½®ç‚º100%ï¼Œé€™è£¡ä¸éœ€è¦é‡è¤‡è¨­ç½®
        processing_tasks[task_id]["message"] = "è™•ç†å®Œæˆ"
        processing_tasks[task_id]["results"] = {
            "paper_results": paper_results,
            "experiment_results": experiment_results,
            "file_info": file_info,
            "vector_stats": vector_stats,
            "processing_time": total_time
        }
        
    except Exception as e:
        error_time = time.time()
        total_time = error_time - start_time
        logger.error(f"âŒ ä»»å‹™ {task_id} è™•ç†å¤±æ•—ï¼Œè€—æ™‚: {total_time:.2f}ç§’ï¼ŒéŒ¯èª¤: {e}")
        processing_tasks[task_id]["status"] = "failed"
        processing_tasks[task_id]["message"] = f"è™•ç†å¤±æ•—: {str(e)}"
    finally:
        # æ¸…ç†è‡¨æ™‚ç›®éŒ„
        try:
            shutil.rmtree(temp_dir)
            logger.info(f"ğŸ§¹ æ¸…ç†è‡¨æ™‚ç›®éŒ„: {temp_dir}")
        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†è‡¨æ™‚ç›®éŒ„å¤±æ•—: {e}")
            pass

@router.get("/upload/status/{task_id}", response_model=ProcessingStatus)
async def get_processing_status(task_id: str):
    """
    ç²å–æ–‡ä»¶è™•ç†ç‹€æ…‹
    
    Args:
        task_id: ä»»å‹™ ID
        
    Returns:
        è™•ç†ç‹€æ…‹ä¿¡æ¯
    """
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")
    
    task = processing_tasks[task_id]
    return ProcessingStatus(
        task_id=task_id,
        status=task["status"],
        progress=task["progress"],
        message=task["message"],
        results=task["results"]
    )

@router.get("/upload/download/{task_id}")
async def download_processed_files(task_id: str):
    """
    ä¸‹è¼‰è™•ç†å¾Œçš„æ–‡ä»¶
    
    Args:
        task_id: ä»»å‹™ ID
        
    Returns:
        è™•ç†å¾Œçš„æ–‡ä»¶
    """
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")
    
    task = processing_tasks[task_id]
    if task["status"] != "completed":
        raise HTTPException(status_code=400, detail="ä»»å‹™å°šæœªå®Œæˆ")
    
    # TODO: å¯¦ç¾æ–‡ä»¶æ‰“åŒ…å’Œä¸‹è¼‰
    return {"message": "ä¸‹è¼‰åŠŸèƒ½å¾…å¯¦ç¾"}

@router.delete("/upload/cancel/{task_id}")
async def cancel_processing(task_id: str):
    """
    å–æ¶ˆæ–‡ä»¶è™•ç†ä»»å‹™
    
    Args:
        task_id: ä»»å‹™ ID
        
    Returns:
        å–æ¶ˆçµæœ
    """
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")
    
    processing_tasks[task_id]["status"] = "cancelled"
    processing_tasks[task_id]["message"] = "ä»»å‹™å·²å–æ¶ˆ"
    
    return {"message": "ä»»å‹™å·²å–æ¶ˆ", "task_id": task_id}

@router.get("/upload/stats", response_model=VectorStatsResponse)
async def get_vector_stats():
    """
    ç²å–å‘é‡æ•¸æ“šåº«çµ±è¨ˆä¿¡æ¯ï¼ˆä½¿ç”¨ç·©å­˜ï¼‰
    
    Returns:
        å‘é‡æ•¸æ“šåº«çµ±è¨ˆä¿¡æ¯
    """
    try:
        # å¾ä¸»æ¨¡å¡Šç²å–ç·©å­˜çš„çµ±è¨ˆä¿¡æ¯
        from main import get_cached_vector_stats
        cached_stats = get_cached_vector_stats()
        
        return VectorStatsResponse(
            paper_vectors=cached_stats["paper_vectors"],
            experiment_vectors=cached_stats["experiment_vectors"],
            total_vectors=cached_stats["total_vectors"]
        )
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å–ç·©å­˜çš„å‘é‡çµ±è¨ˆä¿¡æ¯: {e}")
        return VectorStatsResponse(
            paper_vectors=0,
            experiment_vectors=0,
            total_vectors=0
        )

@router.post("/upload/refresh-stats", response_model=VectorStatsResponse)
async def refresh_vector_stats():
    """
    æ‰‹å‹•åˆ·æ–°å‘é‡çµ±è¨ˆç·©å­˜
    
    Returns:
        æ›´æ–°å¾Œçš„å‘é‡æ•¸æ“šåº«çµ±è¨ˆä¿¡æ¯
    """
    try:
        from main import update_vector_stats_cache, get_cached_vector_stats
        update_vector_stats_cache()
        cached_stats = get_cached_vector_stats()
        
        return VectorStatsResponse(
            paper_vectors=cached_stats["paper_vectors"],
            experiment_vectors=cached_stats["experiment_vectors"],
            total_vectors=cached_stats["total_vectors"]
        )
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•åˆ·æ–°å‘é‡çµ±è¨ˆç·©å­˜: {e}")
        return VectorStatsResponse(
            paper_vectors=0,
            experiment_vectors=0,
            total_vectors=0
        )

@router.get("/documents/{filename:path}")
async def download_document(filename: str):
    """
    ä¸‹è¼‰æ–‡æª”æ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼ˆæ”¯æŒå­ç›®éŒ„è·¯å¾‘ï¼‰
        
    Returns:
        æ–‡ä»¶å…§å®¹
    """
    try:
        import os
        # ç²å–é …ç›®æ ¹ç›®éŒ„ï¼ˆå¾backendç›®éŒ„å‘ä¸Šå…©ç´šåˆ°é …ç›®æ ¹ç›®éŒ„ï¼‰
        current_file_dir = os.path.dirname(__file__)  # backend/api/routes/
        backend_dir = os.path.dirname(os.path.dirname(current_file_dir))  # backend/
        project_root = os.path.dirname(backend_dir)  # é …ç›®æ ¹ç›®éŒ„
        print(f"ğŸ” DEBUG: current_file_dir = {current_file_dir}")
        print(f"ğŸ” DEBUG: backend_dir = {backend_dir}")
        print(f"ğŸ” DEBUG: project_root = {project_root}")
        print(f"ğŸ” DEBUG: filename = {filename}")
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç›´æ¥æ–‡ä»¶åï¼ˆä¸åŒ…å«è·¯å¾‘ï¼‰
        if not os.path.dirname(filename):
            # å¦‚æœæ˜¯ç›´æ¥æ–‡ä»¶åï¼Œå‰‡å‡è¨­åœ¨papersç›®éŒ„ä¸­
            file_path = os.path.join(project_root, "experiment_data", "papers", filename)
        else:
            # å¦‚æœåŒ…å«è·¯å¾‘ï¼Œå‰‡ä½¿ç”¨å®Œæ•´è·¯å¾‘
            file_path = os.path.join(project_root, filename)
        
        # å®‰å…¨æª¢æŸ¥ï¼šç¢ºä¿æ–‡ä»¶è·¯å¾‘åœ¨å…è¨±çš„ç›®éŒ„å…§
        allowed_dirs = [
            os.path.join(project_root, "experiment_data", "papers"),
            os.path.join(project_root, "uploads")
        ]
        
        file_path_abs = os.path.abspath(file_path)
        is_allowed = any(file_path_abs.startswith(allowed_dir) for allowed_dir in allowed_dirs)
        
        if not is_allowed:
            print(f"âŒ è¨ªå•è¢«æ‹’çµ•: {file_path_abs}")
            print(f"âŒ å…è¨±çš„ç›®éŒ„: {allowed_dirs}")
            raise HTTPException(status_code=403, detail="è¨ªå•è¢«æ‹’çµ•")
        
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        print(f"âœ… æä¾›æ–‡ä»¶: {file_path}")
        
        # è¨­ç½®MIMEé¡å‹æ˜ å°„
        mime_types = {
            '.pdf': 'application/pdf',
            '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            '.doc': 'application/msword',
            '.txt': 'text/plain',
            '.html': 'text/html',
            '.htm': 'text/html'
        }
        
        # æ ¹æ“šæ–‡ä»¶é¡å‹è¨­ç½®æ­£ç¢ºçš„MIMEé¡å‹å’ŒéŸ¿æ‡‰é ­
        # ç‰¹æ®Šè™•ç†ï¼šæª¢æŸ¥æ–‡ä»¶åæ˜¯å¦ä»¥_SI.pdfçµå°¾ï¼Œä¹Ÿæ‡‰è©²è­˜åˆ¥ç‚ºPDF
        if filename.lower().endswith('_si.pdf'):
            file_extension = '.pdf'
        else:
            file_extension = os.path.splitext(filename)[1].lower()
        
        media_type = mime_types.get(file_extension, 'application/octet-stream')
        
        # å°æ–¼PDFæ–‡ä»¶ï¼Œè¨­ç½®éŸ¿æ‡‰é ­è®“ç€è¦½å™¨ç›´æ¥é¡¯ç¤ºè€Œä¸æ˜¯ä¸‹è¼‰
        headers = {}
        if file_extension == '.pdf':
            # å¼·åˆ¶è¨­ç½® PDF æ–‡ä»¶çš„éŸ¿æ‡‰é ­
            headers['Content-Disposition'] = 'inline'
            headers['Content-Type'] = 'application/pdf'
            # æ·»åŠ é¡å¤–çš„é ­ä¾†ç¢ºä¿ç€è¦½å™¨æ­£ç¢ºè™•ç†
            headers['X-Content-Type-Options'] = 'nosniff'
            headers['Cache-Control'] = 'public, max-age=3600'
            # å°æ–¼ SI æ–‡ä»¶ï¼Œæ·»åŠ ç‰¹æ®Šæ¨™è­˜
            if filename.lower().endswith('_si.pdf'):
                headers['X-File-Type'] = 'supplementary-information'
        else:
            headers['Content-Disposition'] = f'inline; filename="{os.path.basename(filename)}"'
        
        # è¿”å›æ–‡ä»¶
        return FileResponse(
            path=file_path,
            filename=os.path.basename(filename),
            media_type=media_type,
            headers=headers
        )
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ æ–‡ä»¶ä¸‹è¼‰å¤±æ•—: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸‹è¼‰å¤±æ•—: {str(e)}") 
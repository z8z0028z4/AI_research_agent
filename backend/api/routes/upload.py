"""
æ–‡ä»¶ä¸Šå‚³ API è·¯ç”±
================

è™•ç†æ–‡ä»¶ä¸Šå‚³ã€è™•ç†å’Œç‹€æ…‹æŸ¥è©¢åŠŸèƒ½
"""

from fastapi import APIRouter, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import os
import sys
import tempfile
import shutil
import time
import logging
from pathlib import Path
from datetime import datetime

# æ·»åŠ åŸé …ç›®è·¯å¾‘åˆ° sys.path
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../app'))

from file_upload import process_uploaded_files
from chunk_embedding import embed_documents_from_metadata, embed_experiment_txt_batch, get_vectorstore_stats
from excel_to_txt_by_row import export_new_experiments_to_txt
from config import EXPERIMENT_DIR

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

router = APIRouter()

class FileUploadResponse(BaseModel):
    """æ–‡ä»¶ä¸Šå‚³éŸ¿æ‡‰æ¨¡å‹"""
    success: bool
    message: str
    file_info: Dict[str, Any]
    processing_status: str

class ProcessingStatus(BaseModel):
    """è™•ç†ç‹€æ…‹æ¨¡å‹"""
    task_id: str
    status: str  # pending, processing, completed, failed
    progress: int
    message: str
    results: Optional[Dict[str, Any]] = None

class VectorStatsResponse(BaseModel):
    """å‘é‡çµ±è¨ˆéŸ¿æ‡‰æ¨¡å‹"""
    paper_vectors: int
    experiment_vectors: int
    total_vectors: int

# å­˜å„²è™•ç†ä»»å‹™ç‹€æ…‹
processing_tasks = {}

@router.post("/upload/files", response_model=FileUploadResponse)
async def upload_files(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...)
):
    """
    ä¸Šå‚³ä¸¦è™•ç†æ–‡ä»¶
    
    Args:
        background_tasks: å¾Œå°ä»»å‹™ç®¡ç†å™¨
        files: ä¸Šå‚³çš„æ–‡ä»¶åˆ—è¡¨
        
    Returns:
        ä¸Šå‚³çµæœå’Œè™•ç†ç‹€æ…‹
    """
    try:
        # å‰µå»ºè‡¨æ™‚ç›®éŒ„å­˜å„²ä¸Šå‚³çš„æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        uploaded_files = []
        
        # ä¿å­˜ä¸Šå‚³çš„æ–‡ä»¶
        for file in files:
            if file.filename:
                file_path = os.path.join(temp_dir, file.filename)
                # é‡ç½®æ–‡ä»¶æŒ‡é‡åˆ°é–‹å§‹ä½ç½®ï¼Œç¢ºä¿èƒ½è®€å–å®Œæ•´å…§å®¹
                file.file.seek(0)
                with open(file_path, "wb") as buffer:
                    shutil.copyfileobj(file.file, buffer)
                
                # é©—è­‰æ–‡ä»¶æ˜¯å¦æˆåŠŸä¿å­˜
                saved_size = os.path.getsize(file_path)
                if saved_size == 0:
                    logger.warning(f"âš ï¸ æ–‡ä»¶ {file.filename} ä¿å­˜å¾Œå¤§å°ç‚º 0 bytes")
                else:
                    logger.info(f"âœ… æ–‡ä»¶ {file.filename} æˆåŠŸä¿å­˜ï¼Œå¤§å°: {saved_size} bytes")
                
                uploaded_files.append(file_path)
        
        # ç”Ÿæˆä»»å‹™ ID
        task_id = f"task_{len(processing_tasks) + 1}"
        
        # åˆå§‹åŒ–ä»»å‹™ç‹€æ…‹
        processing_tasks[task_id] = {
            "status": "pending",
            "progress": 0,
            "message": "æ–‡ä»¶ä¸Šå‚³å®Œæˆï¼Œé–‹å§‹è™•ç†...",
            "results": None
        }
        
        # åœ¨å¾Œå°è™•ç†æ–‡ä»¶
        background_tasks.add_task(
            process_files_background,
            task_id,
            uploaded_files,
            temp_dir
        )
        
        return FileUploadResponse(
            success=True,
            message="æ–‡ä»¶ä¸Šå‚³æˆåŠŸï¼Œé–‹å§‹è™•ç†",
            file_info={
                "task_id": task_id,
                "file_count": len(files),
                "file_names": [f.filename for f in files if f.filename]
            },
            processing_status="pending"
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šå‚³å¤±æ•—: {str(e)}")

def _classify_files_by_type(file_paths: List[str]) -> Dict[str, List[str]]:
    """æ ¹æ“šå‰¯æª”åå°‡æª”æ¡ˆåˆ†é¡ç‚ºè«–æ–‡æˆ–å¯¦é©—è³‡æ–™ã€‚"""
    papers: List[str] = []
    experiments: List[str] = []
    other: List[str] = []

    for path in file_paths:
        ext = os.path.splitext(path)[1].lower()
        if ext in [".pdf", ".docx"]:
            papers.append(path)
        elif ext in [".xlsx", ".xls"]:
            experiments.append(path)
        else:
            other.append(path)

    return {"type": "mixed", "papers": papers, "experiments": experiments, "others": other}


async def process_files_background(task_id: str, file_paths: List[str], temp_dir: str):
    """
    å¾Œå°è™•ç†æ–‡ä»¶
    
    Args:
        task_id: ä»»å‹™ ID
        file_paths: æ–‡ä»¶è·¯å¾‘åˆ—è¡¨
        temp_dir: è‡¨æ™‚ç›®éŒ„
    """
    start_time = time.time()
    logger.info(f"ğŸš€ é–‹å§‹è™•ç†ä»»å‹™ {task_id}ï¼Œå…± {len(file_paths)} å€‹æ–‡ä»¶")
    logger.info(f"ğŸ“ è‡¨æ™‚ç›®éŒ„: {temp_dir}")
    
    try:
        # æ›´æ–°ç‹€æ…‹ç‚ºè™•ç†ä¸­
        processing_tasks[task_id]["status"] = "processing"
        processing_tasks[task_id]["progress"] = 5
        processing_tasks[task_id]["message"] = "åˆ†ææ–‡ä»¶é¡å‹..."
        
        # åˆ†ææ–‡ä»¶é¡å‹ï¼ˆä¾å‰¯æª”ååˆ†é¡ï¼‰
        logger.info("ğŸ” é–‹å§‹åˆ†ææ–‡ä»¶é¡å‹...")
        file_info = _classify_files_by_type(file_paths)
        
        # è¨˜éŒ„æ–‡ä»¶åˆ†é¡çµæœ
        for file_type, files in file_info.items():
            if files:
                logger.info(f"ğŸ“‚ {file_type}: {len(files)} å€‹æ–‡ä»¶")
                for f in files:
                    file_size = os.path.getsize(f) if os.path.exists(f) else 0
                    logger.info(f"   ğŸ“„ {os.path.basename(f)} ({file_size} bytes)")
        
        # è¨ˆç®—é€²åº¦åˆ†é…
        has_papers = bool(file_info.get("papers"))
        has_experiments = bool(file_info.get("experiments"))
        
        logger.info(f"ğŸ“Š æ–‡ä»¶é¡å‹åˆ†æå®Œæˆ - è«–æ–‡: {has_papers}, å¯¦é©—: {has_experiments}")
        
        # æ ¹æ“šæ–‡ä»¶é¡å‹èª¿æ•´é€²åº¦åˆ†é…
        if has_papers and has_experiments:
            # æ··åˆæ–‡ä»¶ï¼šè«–æ–‡ä½”70%ï¼Œå¯¦é©—ä½”25%
            paper_progress_range = 70
            experiment_progress_range = 25
        elif has_papers:
            # åªæœ‰è«–æ–‡ï¼šè«–æ–‡ä½”95%
            paper_progress_range = 95
            experiment_progress_range = 0
        elif has_experiments:
            # åªæœ‰å¯¦é©—ï¼šå¯¦é©—ä½”95%
            paper_progress_range = 0
            experiment_progress_range = 95
        else:
            # å…¶ä»–æ–‡ä»¶ï¼šç›´æ¥å®Œæˆ
            paper_progress_range = 0
            experiment_progress_range = 0
        
        processing_tasks[task_id]["progress"] = 10
        processing_tasks[task_id]["message"] = "é–‹å§‹è™•ç†è«–æ–‡è³‡æ–™..."
        
        # è™•ç†è«–æ–‡è³‡æ–™ï¼ˆæå– metadata ä¸¦åµŒå…¥å‘é‡ï¼‰
        paper_results: List[Dict[str, Any]] = []
        if has_papers:
            logger.info("ğŸ“š é–‹å§‹è™•ç†è«–æ–‡è³‡æ–™...")
            paper_start_time = time.time()
            
            # å®šç¾©é€²åº¦å›èª¿å‡½æ•¸
            def update_progress(message: str, progress_percent: int = None):
                if progress_percent is not None:
                    processing_tasks[task_id]["progress"] = progress_percent
                processing_tasks[task_id]["message"] = message
                logger.info(f"ğŸ“ˆ é€²åº¦æ›´æ–°: {processing_tasks[task_id]['progress']}% - {message}")
            
            # è¨ˆç®—è«–æ–‡è™•ç†çš„é€²åº¦åˆ†é…ï¼ˆç¸½å…±95%ï¼‰
            # æ­¥é©Ÿ1-6: 5%, 5%, 10%, 10%, 65%, 5%
            step1_progress = 5   # æ–‡ä»¶åˆ†æ
            step2_progress = 5   # å…ƒæ•¸æ“šæå–
            step3_progress = 10  # å»é‡æª¢æŸ¥
            step4_progress = 10  # æ–‡ä»¶è™•ç†
            step5_progress = 65  # å‘é‡åµŒå…¥
            step6_progress = 5   # å®Œæˆè™•ç†
            
            # æ­¥é©Ÿ1: æ–‡ä»¶åˆ†æå·²å®Œæˆ (5%)
            current_progress = 10
            
            # æ­¥é©Ÿ2: å…ƒæ•¸æ“šæå–
            logger.info("ğŸ“„ é–‹å§‹å…ƒæ•¸æ“šæå–...")
            metadata_start_time = time.time()
            update_progress("ğŸ“„ æå–æ–‡ä»¶å…ƒæ•¸æ“š...", current_progress)
            
            metadata_list: List[Dict[str, Any]] = process_uploaded_files(
                file_info["papers"], 
                status_callback=lambda msg: update_progress(msg, current_progress)  # ä¿æŒç•¶å‰é€²åº¦
            )
            
            metadata_end_time = time.time()
            logger.info(f"âœ… å…ƒæ•¸æ“šæå–å®Œæˆï¼Œè€—æ™‚: {metadata_end_time - metadata_start_time:.2f}ç§’")
            logger.info(f"ğŸ“Š æˆåŠŸæå– {len(metadata_list)} å€‹æ–‡ä»¶çš„å…ƒæ•¸æ“š")
            
            # è¨˜éŒ„æ¯å€‹æ–‡ä»¶çš„å…ƒæ•¸æ“šæå–çµæœ
            for i, metadata in enumerate(metadata_list):
                logger.info(f"   ğŸ“„ {i+1}. {metadata.get('title', 'æœªçŸ¥æ¨™é¡Œ')} - DOI: {metadata.get('doi', 'ç„¡')}")
            
            paper_results.extend(metadata_list)
            current_progress += step2_progress
            update_progress("âœ… å…ƒæ•¸æ“šæå–å®Œæˆ", current_progress)
            
            # æ­¥é©Ÿ3: å»é‡æª¢æŸ¥ (å·²å®Œæˆï¼Œé€²åº¦å·²åŒ…å«åœ¨process_uploaded_filesä¸­)
            current_progress += step3_progress
            update_progress("âœ… å»é‡æª¢æŸ¥å®Œæˆ", current_progress)
            
            # æ­¥é©Ÿ4: æ–‡ä»¶è™•ç† (å·²å®Œæˆï¼Œé€²åº¦å·²åŒ…å«åœ¨process_uploaded_filesä¸­)
            current_progress += step4_progress
            update_progress("âœ… æ–‡ä»¶è™•ç†å®Œæˆ", current_progress)
            
            # æ­¥é©Ÿ5: å‘é‡åµŒå…¥ (65%)
            logger.info("ğŸ”¢ é–‹å§‹å‘é‡åµŒå…¥...")
            embedding_start_time = time.time()
            update_progress("ğŸ“š é–‹å§‹å‘é‡åµŒå…¥...", current_progress)
            
            # è¨ˆç®—æ¯å€‹æ–‡ä»¶çš„åµŒå…¥é€²åº¦
            def embedding_progress_callback(msg: str):
                nonlocal current_progress  # è²æ˜ä½¿ç”¨å¤–éƒ¨è®Šé‡
                # å¾æ¶ˆæ¯ä¸­æå–ç•¶å‰è™•ç†çš„æ–‡ä»¶ç´¢å¼•
                if "è™•ç†ç¬¬" in msg and "å€‹æ–‡ä»¶" in msg:
                    try:
                        # æå– "è™•ç†ç¬¬ X/Y å€‹æ–‡ä»¶" ä¸­çš„ X
                        import re
                        match = re.search(r'è™•ç†ç¬¬ (\d+)/(\d+) å€‹æ–‡ä»¶', msg)
                        if match:
                            current_file = int(match.group(1))
                            total_files = int(match.group(2))
                            # è¨ˆç®—é€²åº¦ï¼šcurrent_progress + (ç•¶å‰æ–‡ä»¶/ç¸½æ–‡ä»¶æ•¸) * step5_progress
                            progress = current_progress + int((current_file / total_files) * step5_progress)
                            update_progress(msg, progress)
                            logger.info(f"ğŸ”¢ å‘é‡åµŒå…¥é€²åº¦: {current_file}/{total_files} ({progress}%)")
                        else:
                            update_progress(msg, current_progress)  # ä½¿ç”¨ç•¶å‰é€²åº¦
                    except:
                        update_progress(msg, current_progress)  # ä½¿ç”¨ç•¶å‰é€²åº¦
                elif "å‘é‡åµŒå…¥æ‰¹æ¬¡" in msg:
                    try:
                        # æå– "å‘é‡åµŒå…¥æ‰¹æ¬¡ X/Y" ä¸­çš„é€²åº¦ä¿¡æ¯
                        import re
                        match = re.search(r'å‘é‡åµŒå…¥æ‰¹æ¬¡ (\d+)/(\d+)', msg)
                        if match:
                            current_batch = int(match.group(1))
                            total_batches = int(match.group(2))
                            # å‘é‡åµŒå…¥éšæ®µï¼šcurrent_progress åˆ° current_progress + step5_progress
                            progress = current_progress + int((current_batch / total_batches) * step5_progress)
                            update_progress(msg, progress)
                            logger.info(f"ğŸ”¢ å‘é‡åµŒå…¥æ‰¹æ¬¡: {current_batch}/{total_batches} ({progress}%)")
                        else:
                            update_progress(msg, current_progress)  # ä½¿ç”¨ç•¶å‰é€²åº¦
                    except:
                        update_progress(msg, current_progress)  # ä½¿ç”¨ç•¶å‰é€²åº¦
                elif "é–‹å§‹å‘é‡åµŒå…¥" in msg:
                    # å‘é‡åµŒå…¥é–‹å§‹ï¼Œè¨­ç½®é€²åº¦ç‚ºç•¶å‰é€²åº¦
                    update_progress(msg, current_progress)
                    logger.info("ğŸ”¢ å‘é‡åµŒå…¥é–‹å§‹")
                elif "å‘é‡åµŒå…¥å®Œæˆ" in msg:
                    # å‘é‡åµŒå…¥å®Œæˆï¼Œè¨­ç½®é€²åº¦ç‚º current_progress + step5_progress
                    update_progress(msg, current_progress + step5_progress)
                    embedding_end_time = time.time()
                    logger.info(f"âœ… å‘é‡åµŒå…¥å®Œæˆï¼Œè€—æ™‚: {embedding_end_time - embedding_start_time:.2f}ç§’")
                else:
                    update_progress(msg, current_progress)  # ä½¿ç”¨ç•¶å‰é€²åº¦
                    logger.info(f"ğŸ“ åµŒå…¥é€²åº¦: {msg}")
            
            logger.info(f"ğŸ”¢ é–‹å§‹å° {len(metadata_list)} å€‹æ–‡ä»¶é€²è¡Œå‘é‡åµŒå…¥...")
            embed_documents_from_metadata(
                metadata_list, 
                status_callback=embedding_progress_callback
            )
            
            paper_end_time = time.time()
            logger.info(f"âœ… è«–æ–‡è™•ç†å®Œæˆï¼Œç¸½è€—æ™‚: {paper_end_time - paper_start_time:.2f}ç§’")
        
        # è™•ç†å¯¦é©—è³‡æ–™ï¼ˆExcel -> txt -> å‘é‡åµŒå…¥ï¼‰
        experiment_results: List[Dict[str, Any]] = []
        if has_experiments:
            logger.info("ğŸ§ª é–‹å§‹è™•ç†å¯¦é©—è³‡æ–™...")
            experiment_start_time = time.time()
            
            # è¨­ç½®å¯¦é©—è™•ç†çš„èµ·å§‹é€²åº¦
            experiment_start_progress = 10 + paper_progress_range
            processing_tasks[task_id]["progress"] = experiment_start_progress
            processing_tasks[task_id]["message"] = "è™•ç†å¯¦é©—è³‡æ–™..."
            
            for i, f in enumerate(file_info["experiments"]):
                try:
                    logger.info(f"ğŸ§ª è™•ç†å¯¦é©—æ–‡ä»¶ {i+1}/{len(file_info['experiments'])}: {os.path.basename(f)}")
                    processing_tasks[task_id]["message"] = f"è™•ç†å¯¦é©—æ–‡ä»¶ {i+1}/{len(file_info['experiments'])}..."
                    processing_tasks[task_id]["progress"] = experiment_start_progress + int((i / len(file_info["experiments"])) * experiment_progress_range)
                    
                    # è¨˜éŒ„æ–‡ä»¶å¤§å°
                    file_size = os.path.getsize(f) if os.path.exists(f) else 0
                    logger.info(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} bytes")
                    
                    # è½‰æ›Excelç‚ºTXT
                    logger.info(f"   ğŸ“„ é–‹å§‹è½‰æ›Excelç‚ºTXT...")
                    excel_start_time = time.time()
                    df, txt_paths = export_new_experiments_to_txt(
                        excel_path=f,
                        output_dir=EXPERIMENT_DIR
                    )
                    excel_end_time = time.time()
                    logger.info(f"   âœ… Excelè½‰æ›å®Œæˆï¼Œç”Ÿæˆ {len(txt_paths)} å€‹TXTæ–‡ä»¶ï¼Œè€—æ™‚: {excel_end_time - excel_start_time:.2f}ç§’")
                    
                    # å‘é‡åµŒå…¥
                    logger.info(f"   ğŸ”¢ é–‹å§‹å¯¦é©—æ•¸æ“šå‘é‡åµŒå…¥...")
                    embed_start_time = time.time()
                    result = embed_experiment_txt_batch(txt_paths)
                    embed_end_time = time.time()
                    logger.info(f"   âœ… å¯¦é©—æ•¸æ“šå‘é‡åµŒå…¥å®Œæˆï¼Œè€—æ™‚: {embed_end_time - embed_start_time:.2f}ç§’")
                    
                    experiment_results.append({
                        "file": f,
                        "txt_paths": txt_paths,
                        "embedded_count": len(txt_paths)
                    })
                    
                except Exception as e:
                    logger.error(f"âŒ å¯¦é©—æ–‡ä»¶è™•ç†å¤±æ•— {os.path.basename(f)}: {e}")
                    experiment_results.append({
                        "file": f,
                        "error": str(e)
                    })
            
            experiment_end_time = time.time()
            logger.info(f"âœ… å¯¦é©—è™•ç†å®Œæˆï¼Œç¸½è€—æ™‚: {experiment_end_time - experiment_start_time:.2f}ç§’")
        
        processing_tasks[task_id]["progress"] = 95
        processing_tasks[task_id]["message"] = "å®Œæˆè™•ç†..."
        
        # æ›´æ–°å‘é‡çµ±è¨ˆç·©å­˜
        logger.info("ğŸ“Š é–‹å§‹æ›´æ–°å‘é‡çµ±è¨ˆç·©å­˜...")
        vector_stats = {}
        try:
            from main import update_vector_stats_cache, get_cached_vector_stats
            update_vector_stats_cache()
            cached_stats = get_cached_vector_stats()
            vector_stats = {
                "paper_vectors": cached_stats["paper_vectors"],
                "experiment_vectors": cached_stats["experiment_vectors"],
                "total_vectors": cached_stats["total_vectors"]
            }
            logger.info(f"ğŸ“Š å‘é‡çµ±è¨ˆç·©å­˜æ›´æ–° - è«–æ–‡: {vector_stats['paper_vectors']}, å¯¦é©—: {vector_stats['experiment_vectors']}, ç¸½è¨ˆ: {vector_stats['total_vectors']}")
            print(f"ğŸ“Š å‘é‡çµ±è¨ˆç·©å­˜æ›´æ–° - è«–æ–‡: {vector_stats['paper_vectors']}, å¯¦é©—: {vector_stats['experiment_vectors']}, ç¸½è¨ˆ: {vector_stats['total_vectors']}")
        except Exception as e:
            logger.error(f"âš ï¸ ç„¡æ³•æ›´æ–°å‘é‡çµ±è¨ˆç·©å­˜: {e}")
            print(f"âš ï¸ ç„¡æ³•æ›´æ–°å‘é‡çµ±è¨ˆç·©å­˜: {e}")
            vector_stats = {"paper_vectors": 0, "experiment_vectors": 0, "total_vectors": 0}
        
        # æ›´æ–°æœ€çµ‚çµæœ
        end_time = time.time()
        total_time = end_time - start_time
        logger.info(f"ğŸ‰ ä»»å‹™ {task_id} è™•ç†å®Œæˆï¼Œç¸½è€—æ™‚: {total_time:.2f}ç§’")
        
        processing_tasks[task_id]["status"] = "completed"
        processing_tasks[task_id]["progress"] = 100
        processing_tasks[task_id]["message"] = "è™•ç†å®Œæˆ"
        processing_tasks[task_id]["results"] = {
            "paper_results": paper_results,
            "experiment_results": experiment_results,
            "file_info": file_info,
            "vector_stats": vector_stats,
            "processing_time": total_time
        }
        
    except Exception as e:
        error_time = time.time()
        total_time = error_time - start_time
        logger.error(f"âŒ ä»»å‹™ {task_id} è™•ç†å¤±æ•—ï¼Œè€—æ™‚: {total_time:.2f}ç§’ï¼ŒéŒ¯èª¤: {e}")
        processing_tasks[task_id]["status"] = "failed"
        processing_tasks[task_id]["message"] = f"è™•ç†å¤±æ•—: {str(e)}"
    finally:
        # æ¸…ç†è‡¨æ™‚ç›®éŒ„
        try:
            shutil.rmtree(temp_dir)
            logger.info(f"ğŸ§¹ æ¸…ç†è‡¨æ™‚ç›®éŒ„: {temp_dir}")
        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†è‡¨æ™‚ç›®éŒ„å¤±æ•—: {e}")
            pass

@router.get("/upload/status/{task_id}", response_model=ProcessingStatus)
async def get_processing_status(task_id: str):
    """
    ç²å–æ–‡ä»¶è™•ç†ç‹€æ…‹
    
    Args:
        task_id: ä»»å‹™ ID
        
    Returns:
        è™•ç†ç‹€æ…‹ä¿¡æ¯
    """
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")
    
    task = processing_tasks[task_id]
    return ProcessingStatus(
        task_id=task_id,
        status=task["status"],
        progress=task["progress"],
        message=task["message"],
        results=task["results"]
    )

@router.get("/upload/download/{task_id}")
async def download_processed_files(task_id: str):
    """
    ä¸‹è¼‰è™•ç†å¾Œçš„æ–‡ä»¶
    
    Args:
        task_id: ä»»å‹™ ID
        
    Returns:
        è™•ç†å¾Œçš„æ–‡ä»¶
    """
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")
    
    task = processing_tasks[task_id]
    if task["status"] != "completed":
        raise HTTPException(status_code=400, detail="ä»»å‹™å°šæœªå®Œæˆ")
    
    # TODO: å¯¦ç¾æ–‡ä»¶æ‰“åŒ…å’Œä¸‹è¼‰
    return {"message": "ä¸‹è¼‰åŠŸèƒ½å¾…å¯¦ç¾"}

@router.delete("/upload/cancel/{task_id}")
async def cancel_processing(task_id: str):
    """
    å–æ¶ˆæ–‡ä»¶è™•ç†ä»»å‹™
    
    Args:
        task_id: ä»»å‹™ ID
        
    Returns:
        å–æ¶ˆçµæœ
    """
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")
    
    processing_tasks[task_id]["status"] = "cancelled"
    processing_tasks[task_id]["message"] = "ä»»å‹™å·²å–æ¶ˆ"
    
    return {"message": "ä»»å‹™å·²å–æ¶ˆ", "task_id": task_id}

@router.get("/upload/stats", response_model=VectorStatsResponse)
async def get_vector_stats():
    """
    ç²å–å‘é‡æ•¸æ“šåº«çµ±è¨ˆä¿¡æ¯ï¼ˆä½¿ç”¨ç·©å­˜ï¼‰
    
    Returns:
        å‘é‡æ•¸æ“šåº«çµ±è¨ˆä¿¡æ¯
    """
    try:
        # å¾ä¸»æ¨¡å¡Šç²å–ç·©å­˜çš„çµ±è¨ˆä¿¡æ¯
        from main import get_cached_vector_stats
        cached_stats = get_cached_vector_stats()
        
        return VectorStatsResponse(
            paper_vectors=cached_stats["paper_vectors"],
            experiment_vectors=cached_stats["experiment_vectors"],
            total_vectors=cached_stats["total_vectors"]
        )
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å–ç·©å­˜çš„å‘é‡çµ±è¨ˆä¿¡æ¯: {e}")
        return VectorStatsResponse(
            paper_vectors=0,
            experiment_vectors=0,
            total_vectors=0
        )

@router.post("/upload/refresh-stats", response_model=VectorStatsResponse)
async def refresh_vector_stats():
    """
    æ‰‹å‹•åˆ·æ–°å‘é‡çµ±è¨ˆç·©å­˜
    
    Returns:
        æ›´æ–°å¾Œçš„å‘é‡æ•¸æ“šåº«çµ±è¨ˆä¿¡æ¯
    """
    try:
        from main import update_vector_stats_cache, get_cached_vector_stats
        update_vector_stats_cache()
        cached_stats = get_cached_vector_stats()
        
        return VectorStatsResponse(
            paper_vectors=cached_stats["paper_vectors"],
            experiment_vectors=cached_stats["experiment_vectors"],
            total_vectors=cached_stats["total_vectors"]
        )
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•åˆ·æ–°å‘é‡çµ±è¨ˆç·©å­˜: {e}")
        return VectorStatsResponse(
            paper_vectors=0,
            experiment_vectors=0,
            total_vectors=0
        )

@router.get("/documents/{filename:path}")
async def download_document(filename: str):
    """
    ä¸‹è¼‰æ–‡æª”æ–‡ä»¶
    
    Args:
        filename: æ–‡ä»¶åï¼ˆæ”¯æŒå­ç›®éŒ„è·¯å¾‘ï¼‰
        
    Returns:
        æ–‡ä»¶å…§å®¹
    """
    try:
        import os
        # ç²å–é …ç›®æ ¹ç›®éŒ„ï¼ˆå¾backendç›®éŒ„å‘ä¸Šå…©ç´šåˆ°é …ç›®æ ¹ç›®éŒ„ï¼‰
        current_file_dir = os.path.dirname(__file__)  # backend/api/routes/
        backend_dir = os.path.dirname(os.path.dirname(current_file_dir))  # backend/
        project_root = os.path.dirname(backend_dir)  # é …ç›®æ ¹ç›®éŒ„
        print(f"ğŸ” DEBUG: current_file_dir = {current_file_dir}")
        print(f"ğŸ” DEBUG: backend_dir = {backend_dir}")
        print(f"ğŸ” DEBUG: project_root = {project_root}")
        print(f"ğŸ” DEBUG: filename = {filename}")
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç›´æ¥æ–‡ä»¶åï¼ˆä¸åŒ…å«è·¯å¾‘ï¼‰
        if not os.path.dirname(filename):
            # å¦‚æœæ˜¯ç›´æ¥æ–‡ä»¶åï¼Œå‰‡å‡è¨­åœ¨papersç›®éŒ„ä¸­
            file_path = os.path.join(project_root, "experiment_data", "papers", filename)
        else:
            # å¦‚æœåŒ…å«è·¯å¾‘ï¼Œå‰‡ä½¿ç”¨å®Œæ•´è·¯å¾‘
            file_path = os.path.join(project_root, filename)
        
        # å®‰å…¨æª¢æŸ¥ï¼šç¢ºä¿æ–‡ä»¶è·¯å¾‘åœ¨å…è¨±çš„ç›®éŒ„å…§
        allowed_dirs = [
            os.path.join(project_root, "experiment_data", "papers"),
            os.path.join(project_root, "uploads")
        ]
        
        file_path_abs = os.path.abspath(file_path)
        is_allowed = any(file_path_abs.startswith(allowed_dir) for allowed_dir in allowed_dirs)
        
        if not is_allowed:
            print(f"âŒ è¨ªå•è¢«æ‹’çµ•: {file_path_abs}")
            print(f"âŒ å…è¨±çš„ç›®éŒ„: {allowed_dirs}")
            raise HTTPException(status_code=403, detail="è¨ªå•è¢«æ‹’çµ•")
        
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        print(f"âœ… æä¾›æ–‡ä»¶: {file_path}")
        
        # è¨­ç½®MIMEé¡å‹æ˜ å°„
        mime_types = {
            '.pdf': 'application/pdf',
            '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            '.doc': 'application/msword',
            '.txt': 'text/plain',
            '.html': 'text/html',
            '.htm': 'text/html'
        }
        
        # æ ¹æ“šæ–‡ä»¶é¡å‹è¨­ç½®æ­£ç¢ºçš„MIMEé¡å‹å’ŒéŸ¿æ‡‰é ­
        # ç‰¹æ®Šè™•ç†ï¼šæª¢æŸ¥æ–‡ä»¶åæ˜¯å¦ä»¥_SI.pdfçµå°¾ï¼Œä¹Ÿæ‡‰è©²è­˜åˆ¥ç‚ºPDF
        if filename.lower().endswith('_si.pdf'):
            file_extension = '.pdf'
        else:
            file_extension = os.path.splitext(filename)[1].lower()
        
        media_type = mime_types.get(file_extension, 'application/octet-stream')
        
        # å°æ–¼PDFæ–‡ä»¶ï¼Œè¨­ç½®éŸ¿æ‡‰é ­è®“ç€è¦½å™¨ç›´æ¥é¡¯ç¤ºè€Œä¸æ˜¯ä¸‹è¼‰
        headers = {}
        if file_extension == '.pdf':
            # å¼·åˆ¶è¨­ç½® PDF æ–‡ä»¶çš„éŸ¿æ‡‰é ­
            headers['Content-Disposition'] = 'inline'
            headers['Content-Type'] = 'application/pdf'
            # æ·»åŠ é¡å¤–çš„é ­ä¾†ç¢ºä¿ç€è¦½å™¨æ­£ç¢ºè™•ç†
            headers['X-Content-Type-Options'] = 'nosniff'
            headers['Cache-Control'] = 'public, max-age=3600'
            # å°æ–¼ SI æ–‡ä»¶ï¼Œæ·»åŠ ç‰¹æ®Šæ¨™è­˜
            if filename.lower().endswith('_si.pdf'):
                headers['X-File-Type'] = 'supplementary-information'
        else:
            headers['Content-Disposition'] = f'inline; filename="{os.path.basename(filename)}"'
        
        # è¿”å›æ–‡ä»¶
        return FileResponse(
            path=file_path,
            filename=os.path.basename(filename),
            media_type=media_type,
            headers=headers
        )
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ æ–‡ä»¶ä¸‹è¼‰å¤±æ•—: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸‹è¼‰å¤±æ•—: {str(e)}") 
"""
AI ç ”ç©¶åŠ©ç† - Semantic Scholar å…ƒæ•¸æ“šæŸ¥è©¢æ¨¡å¡Š
==========================================

é€™å€‹æ¨¡å¡Šè² è²¬å¾ Semantic Scholar API æŸ¥è©¢è«–æ–‡å…ƒæ•¸æ“šï¼ŒåŒ…æ‹¬ï¼š
1. åŸºæ–¼ DOI çš„ç²¾ç¢ºæŸ¥è©¢
2. åŸºæ–¼æ¨™é¡Œçš„æ¨¡ç³ŠæŸ¥è©¢å’ŒåŒ¹é…
3. æ™ºèƒ½é‡è©¦æ©Ÿåˆ¶å’Œé »ç‡é™åˆ¶è™•ç†
4. API é‡‘é‘°æ”¯æ´å’ŒéŒ¯èª¤è™•ç†

æ¶æ§‹èªªæ˜ï¼š
- å„ªå…ˆä½¿ç”¨ DOI æŸ¥è©¢ï¼ˆæ›´æº–ç¢ºï¼‰
- å‚™ç”¨æ¨™é¡ŒæŸ¥è©¢ï¼ˆæ”¯æ´æ¨¡ç³ŠåŒ¹é…ï¼‰
- æ™ºèƒ½å»¶é²é¿å…é »ç‡é™åˆ¶
- è‡ªå‹•é‡è©¦å’ŒéŒ¯èª¤æ¢å¾©
- å¯é…ç½® API é‡‘é‘°æé«˜é™åˆ¶

æŠ€è¡“ç´°ç¯€ï¼š
- åŸºç¤å»¶é²ï¼š2-4 ç§’é¿å…è«‹æ±‚éå¿«
- 429 éŒ¯èª¤å»¶é²ï¼š20-40 ç§’
- è¶…æ™‚è¨­å®šï¼š20 ç§’
- æœ€ä½³åŒ¹é…ç®—æ³•ï¼šåŸºæ–¼è©å½™é‡ç–Šè¨ˆç®—ç›¸ä¼¼åº¦

æœ€ä½³å¯¦è¸éµå¾ªï¼š
- é›†ä¸­é…ç½®ç®¡ç†ï¼ˆæ”¯æ´ç’°å¢ƒè®Šæ•¸ SEMANTIC_SCHOLAR_API_KEYï¼‰
- çµ±ä¸€éŒ¯èª¤è™•ç†å’Œæ—¥èªŒè¨˜éŒ„
- æ™ºèƒ½é‡è©¦ç­–ç•¥é¿å…ç„¡æ•ˆè«‹æ±‚
- æ¨¡çµ„åŒ–è¨­è¨ˆä¾¿æ–¼ç¶­è­·å’Œæ¸¬è©¦
"""

import requests
import time
import random
import os
import logging

# é…ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)

def lookup_semantic_scholar_metadata(doi=None, title=None, max_retries=3, api_key=None):
    """
    æŸ¥è©¢Semantic Scholarå…ƒæ•¸æ“š
    
    Args:
        doi: DOIè™Ÿç¢¼
        title: è«–æ–‡æ¨™é¡Œ
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        api_key: å¯é¸çš„APIé‡‘é‘°ï¼Œç”¨æ–¼æé«˜è«‹æ±‚é™åˆ¶
    
    Returns:
        Dict: å…ƒæ•¸æ“šå­—å…¸
    """
    base_url = "https://api.semanticscholar.org/graph/v1/paper"
    fields = "title,authors,year,venue,url,externalIds"
    
    # å¾ç’°å¢ƒè®Šæ•¸ç²å–APIé‡‘é‘°ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    if not api_key:
        api_key = os.getenv("SEMANTIC_SCHOLAR_API_KEY")

    # å„ªå…ˆä½¿ç”¨DOIæŸ¥è©¢
    if doi:
        return _query_by_doi(base_url, doi, fields, max_retries, api_key)
    
    # å‚™ç”¨æ¨™é¡ŒæŸ¥è©¢
    elif title:
        return _query_by_title(base_url, title, fields, max_retries, api_key)
    
    else:
        logger.warning("âš ï¸ æ²’æœ‰æä¾›DOIæˆ–æ¨™é¡Œï¼Œç„¡æ³•æŸ¥è©¢")
        return {}

def _query_by_doi(base_url, doi, fields, max_retries, api_key=None):
    """ä½¿ç”¨DOIæŸ¥è©¢"""
    url = f"{base_url}/DOI:{doi}?fields={fields}"
    
    # æº–å‚™è«‹æ±‚æ¨™é ­
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    if api_key:
        headers["x-api-key"] = api_key
    
    for attempt in range(max_retries):
        try:
            # æ·»åŠ åŸºç¤å»¶é²é¿å…è«‹æ±‚éå¿«
            if attempt > 0:
                delay = random.uniform(2, 4)
                logger.info(f"   ğŸ’¤ ç­‰å¾… {delay:.1f} ç§’å¾Œé‡è©¦...")
                time.sleep(delay)
            
            response = requests.get(url, headers=headers, verify=False, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                logger.info(f"âœ… DOIæŸ¥è©¢æˆåŠŸ: {doi}")
                return data
            elif response.status_code == 404:
                logger.warning(f"âš ï¸ DOIä¸å­˜åœ¨: {doi}")
                return {}
            elif response.status_code == 429:  # Rate limit
                wait_time = random.uniform(15, 30)  # å¢åŠ ç­‰å¾…æ™‚é–“
                logger.warning(f"âš ï¸ DOIæŸ¥è©¢è¢«é™åˆ¶ (å˜—è©¦ {attempt+1}/{max_retries})ï¼Œç­‰å¾… {wait_time:.1f} ç§’")
                if attempt < max_retries - 1:
                    time.sleep(wait_time)
                    continue
                return {}
            else:
                logger.warning(f"âš ï¸ DOIæŸ¥è©¢å¤±æ•— (å˜—è©¦ {attempt+1}/{max_retries}): {response.status_code}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(3, 6))  # å¢åŠ å»¶é²æ™‚é–“
                    continue
                return {}
                
        except requests.exceptions.Timeout:
            logger.warning(f"âš ï¸ DOIæŸ¥è©¢è¶…æ™‚ (å˜—è©¦ {attempt+1}/{max_retries})")
            if attempt < max_retries - 1:
                time.sleep(random.uniform(3, 6))
                continue
            return {}
        except Exception as e:
            logger.warning(f"âš ï¸ DOIæŸ¥è©¢ç•°å¸¸ (å˜—è©¦ {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(random.uniform(2, 4))
                continue
            return {}
    
    return {}

def _query_by_title(base_url, title, fields, max_retries, api_key=None):
    """ä½¿ç”¨æ¨™é¡ŒæŸ¥è©¢ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰"""
    search_url = f"{base_url}/search"
    
    # æº–å‚™è«‹æ±‚æ¨™é ­
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    if api_key:
        headers["x-api-key"] = api_key
    
    for attempt in range(max_retries):
        try:
            # æ·»åŠ åŸºç¤å»¶é²ï¼Œå³ä½¿æ˜¯ç¬¬ä¸€æ¬¡è«‹æ±‚ä¹Ÿè¦å»¶é²
            base_delay = random.uniform(2, 4)  # åŸºç¤å»¶é² 2-4 ç§’
            if attempt > 0:
                retry_delay = random.uniform(5, 10)  # é‡è©¦é¡å¤–å»¶é²
                total_delay = base_delay + retry_delay
                logger.info(f"   ğŸ’¤ ç¬¬ {attempt+1} æ¬¡å˜—è©¦ï¼Œç­‰å¾… {total_delay:.1f} ç§’...")
                time.sleep(total_delay)
            else:
                logger.info(f"   ğŸ’¤ åŸºç¤å»¶é² {base_delay:.1f} ç§’é¿å…è«‹æ±‚éå¿«...")
                time.sleep(base_delay)
            
            # å˜—è©¦ä¸åŒçš„æŸ¥è©¢åƒæ•¸
            params = {
                "query": title,
                "fields": fields,
                "limit": 3,  # å¢åŠ é™åˆ¶æ•¸é‡ä»¥æé«˜åŒ¹é…æ©Ÿæœƒ
                "offset": 0
            }
            
            logger.info(f"   ğŸ” æŸ¥è©¢æ¨™é¡Œ: {title[:60]}...")
            
            response = requests.get(
                search_url, 
                params=params, 
                headers=headers,
                verify=False, 
                timeout=20  # å¢åŠ è¶…æ™‚æ™‚é–“
            )
            
            if response.status_code == 200:
                data = response.json()
                if data.get("data") and len(data["data"]) > 0:
                    logger.info(f"âœ… æ¨™é¡ŒæŸ¥è©¢æˆåŠŸ: {title[:50]}...")
                    
                    # å˜—è©¦æ‰¾åˆ°æœ€ä½³åŒ¹é…
                    best_match = _find_best_title_match(title, data["data"])
                    return best_match
                else:
                    logger.warning(f"âš ï¸ æ¨™é¡ŒæŸ¥è©¢ç„¡çµæœ: {title[:50]}...")
                    return {}
            elif response.status_code == 429:  # Rate limit
                wait_time = random.uniform(20, 40)  # å¤§å¹…å¢åŠ ç­‰å¾…æ™‚é–“
                logger.warning(f"âš ï¸ æ¨™é¡ŒæŸ¥è©¢è¢«é™åˆ¶ (å˜—è©¦ {attempt+1}/{max_retries})ï¼Œç­‰å¾… {wait_time:.1f} ç§’")
                if attempt < max_retries - 1:
                    time.sleep(wait_time)
                    continue
                return {}
            elif response.status_code == 403:  # Forbidden
                wait_time = random.uniform(10, 20)
                logger.warning(f"âš ï¸ æ¨™é¡ŒæŸ¥è©¢è¢«æ‹’çµ•è¨ªå• (å˜—è©¦ {attempt+1}/{max_retries})ï¼Œç­‰å¾… {wait_time:.1f} ç§’")
                if attempt < max_retries - 1:
                    time.sleep(wait_time)
                    continue
                return {}
            else:
                logger.warning(f"âš ï¸ æ¨™é¡ŒæŸ¥è©¢å¤±æ•— (å˜—è©¦ {attempt+1}/{max_retries}): {response.status_code}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(5, 10))  # å¢åŠ ä¸€èˆ¬éŒ¯èª¤çš„å»¶é²
                    continue
                return {}
                
        except requests.exceptions.Timeout:
            logger.warning(f"âš ï¸ æ¨™é¡ŒæŸ¥è©¢è¶…æ™‚ (å˜—è©¦ {attempt+1}/{max_retries})")
            if attempt < max_retries - 1:
                time.sleep(random.uniform(5, 10))
                continue
            return {}
        except Exception as e:
            logger.warning(f"âš ï¸ æ¨™é¡ŒæŸ¥è©¢ç•°å¸¸ (å˜—è©¦ {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(random.uniform(3, 6))
                continue
            return {}
    
    return {}

def _find_best_title_match(query_title, results):
    """å¾å¤šå€‹çµæœä¸­æ‰¾åˆ°æœ€ä½³æ¨™é¡ŒåŒ¹é…"""
    if not results:
        return {}
    
    # ç°¡å–®çš„ç›¸ä¼¼åº¦è¨ˆç®—
    def calculate_similarity(title1, title2):
        if not title1 or not title2:
            return 0.0
        
        words1 = set(title1.lower().split())
        words2 = set(title2.lower().split())
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0
    
    best_match = results[0]
    best_score = 0.0
    
    for result in results:
        result_title = result.get('title', '')
        score = calculate_similarity(query_title, result_title)
        
        if score > best_score:
            best_score = score
            best_match = result
    
    logger.info(f"   ğŸ“Š æœ€ä½³åŒ¹é…ç›¸ä¼¼åº¦: {best_score:.2f}")
    logger.info(f"   ğŸ“„ åŒ¹é…æ¨™é¡Œ: {best_match.get('title', 'N/A')[:60]}...")
    
    return best_match

if __name__ == "__main__":
    # æ¸¬è©¦ç”¨
    test_doi = "10.1016/j.matchemphys.2019.122601"
    metadata = lookup_semantic_scholar_metadata(doi=test_doi)
    print(metadata)

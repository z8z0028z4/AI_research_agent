"""
AI ç ”ç©¶åŠ©ç† - æ–‡ä»¶ä¸Šå‚³è™•ç†æ¨¡å¡Š
============================

é€™å€‹æ¨¡å¡Šè² è²¬è™•ç†ç”¨æˆ¶ä¸Šå‚³çš„æ–‡ä»¶ï¼ŒåŒ…æ‹¬ï¼š
1. æ–‡ä»¶æ ¼å¼é©—è­‰
2. å…ƒæ•¸æ“šæå–
3. æ–‡ä»¶é‡å‘½åå’Œè¤‡è£½
4. å…ƒæ•¸æ“šè¨»å†Šè¡¨æ›´æ–°

æ¶æ§‹èªªæ˜ï¼š
- æ”¯æŒPDFå’ŒDOCXæ ¼å¼æ–‡ä»¶
- é›†æˆå¤šå€‹å…ƒæ•¸æ“šæå–æº
- æä¾›é€²åº¦å›èª¿æ©Ÿåˆ¶
- è‡ªå‹•æ–‡ä»¶ç®¡ç†å’Œçµ„ç¹”
"""

import os
import time
import logging
from typing import List, Dict, Optional, Callable
# å°å…¥æœå‹™æ¨¡å¡Š
from .metadata_extractor import extract_metadata
from .semantic_lookup import lookup_semantic_scholar_metadata
from .document_renamer import rename_and_copy_file
from .metadata_registry import append_metadata_to_registry, get_existing_metadata

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def check_batch_duplicate(current_metadata: dict, processed_metadata_list: List[Dict]) -> Dict[str, any]:
    """
    æª¢æŸ¥ç•¶å‰æ–‡ä»¶æ˜¯å¦èˆ‡å·²è™•ç†çš„æ‰¹æ¬¡æ–‡ä»¶é‡è¤‡
    
    åƒæ•¸ï¼š
        current_metadata (dict): ç•¶å‰æ–‡ä»¶çš„å…ƒæ•¸æ“š
        processed_metadata_list (List[Dict]): å·²è™•ç†çš„æ‰¹æ¬¡æ–‡ä»¶å…ƒæ•¸æ“šåˆ—è¡¨
    
    è¿”å›ï¼š
        Dict: {
            "is_duplicate": bool,
            "duplicate_type": str,  # "doi", "title", "none"
            "existing_metadata": dict or None
        }
    """
    try:
        for meta in processed_metadata_list:
            # 1. æª¢æŸ¥ type + DOI çµ„åˆé‡è¤‡ï¼ˆæœ€å¯é ï¼‰
            current_doi = (current_metadata.get("doi") or "").strip()
            current_type = (current_metadata.get("type") or "").strip()
            meta_doi = (meta.get("doi") or "").strip()
            meta_type = (meta.get("type") or "").strip()
            
            if current_doi and meta_doi and current_type and meta_type:
                if current_doi == meta_doi and current_type == meta_type:
                    return {
                        "is_duplicate": True,
                        "duplicate_type": "doi",
                        "existing_metadata": meta
                    }
            
            # 2. æª¢æŸ¥ title + type çµ„åˆé‡è¤‡ï¼ˆæ¬¡å¯é ï¼‰
            current_title = (current_metadata.get("title") or "").strip()
            meta_title = (meta.get("title") or "").strip()
            
            if current_title and meta_title and current_type and meta_type:
                if current_title == meta_title and current_type == meta_type:
                    return {
                        "is_duplicate": True,
                        "duplicate_type": "title",
                        "existing_metadata": meta
                    }
        
        return {
            "is_duplicate": False,
            "duplicate_type": "none",
            "existing_metadata": None
        }
    except Exception as e:
        logger.error(f"âŒ æ‰¹æ¬¡å»é‡æª¢æŸ¥éŒ¯èª¤: {e}")
        return {
            "is_duplicate": False,
            "duplicate_type": "error",
            "existing_metadata": None
        }

def check_duplicate_file(file_path: str, metadata: dict) -> Dict[str, any]:
    """
    æª¢æŸ¥æ–‡ä»¶æ˜¯å¦ç‚ºé‡è¤‡æ–‡ä»¶
    
    è¿”å›ï¼š
        Dict: {
            "is_duplicate": bool,
            "duplicate_type": str,  # "doi", "title", "none"
            "existing_metadata": dict or None
        }
    """
    try:
        # ç²å–ç¾æœ‰å…ƒæ•¸æ“š
        existing_metadata = get_existing_metadata()
        
        if existing_metadata is not None and not existing_metadata.empty:
            # 1. æª¢æŸ¥ type + DOI çµ„åˆé‡è¤‡ï¼ˆæœ€å¯é ï¼‰
            doi = (metadata.get("doi") or "").strip()
            doc_type = (metadata.get("type") or "").strip()
            
            if doi and doc_type:
                # æª¢æŸ¥ç›¸åŒtypeå’ŒDOIçš„çµ„åˆ
                type_doi_matches = existing_metadata[
                    (existing_metadata["doi"] == doi) & 
                    (existing_metadata["type"] == doc_type)
                ]
                if not type_doi_matches.empty:
                    return {
                        "is_duplicate": True,
                        "duplicate_type": "doi",
                        "existing_metadata": type_doi_matches.iloc[0].to_dict()
                    }
            
            # 2. æª¢æŸ¥ title + type çµ„åˆé‡è¤‡ï¼ˆæ¬¡å¯é ï¼‰
            title = (metadata.get("title") or "").strip()
            if title and doc_type:
                # æª¢æŸ¥ç›¸åŒtitleå’Œtypeçš„çµ„åˆ
                title_type_matches = existing_metadata[
                    (existing_metadata["title"] == title) & 
                    (existing_metadata["type"] == doc_type)
                ]
                if not title_type_matches.empty:
                    return {
                        "is_duplicate": True,
                        "duplicate_type": "title",
                        "existing_metadata": title_type_matches.iloc[0].to_dict()
                    }
        
        return {
            "is_duplicate": False,
            "duplicate_type": "none",
            "existing_metadata": None
        }
        
    except Exception as e:
        logger.error(f"âŒ å»é‡æª¢æŸ¥å¤±æ•—: {e}")
        return {
            "is_duplicate": False,
            "duplicate_type": "error",
            "existing_metadata": None
        }

def process_uploaded_files(file_paths: List[str], status_callback: Optional[Callable[[str], None]] = None) -> List[Dict]:
    """
    è™•ç†ä¸Šå‚³çš„æ–‡ä»¶ï¼Œæå–å…ƒæ•¸æ“šä¸¦çµ„ç¹”å­˜å„²
    
    åŠŸèƒ½æµç¨‹ï¼š
    1. é©—è­‰æ–‡ä»¶æ ¼å¼ï¼ˆåªæ”¯æŒPDFå’ŒDOCXï¼‰
    2. æ‰¹æ¬¡æå–æ–‡ä»¶å…ƒæ•¸æ“š
    3. æ‰¹æ¬¡èˆ‡è¨»å†Šæ–‡ä»¶å»é‡
    4. æ‰¹æ¬¡é–“å»é‡
    5. æ‰¹æ¬¡è™•ç†å‰©é¤˜æ–‡ä»¶ï¼ˆé‡å‘½åã€è¤‡è£½ã€è¨»å†Šï¼‰
    
    åƒæ•¸ï¼š
        file_paths (List[str]): æ–‡ä»¶è·¯å¾‘åˆ—è¡¨
        status_callback (Optional[Callable]): é€²åº¦å›èª¿å‡½æ•¸ï¼Œç”¨æ–¼æ›´æ–°UIç‹€æ…‹
    
    è¿”å›ï¼š
        List[Dict]: è™•ç†çµæœåˆ—è¡¨ï¼ŒåŒ…å«æ¯å€‹æ–‡ä»¶çš„å…ƒæ•¸æ“šä¿¡æ¯
    """
    start_time = time.time()
    logger.info(f"ğŸš€ é–‹å§‹è™•ç†ä¸Šå‚³æ–‡ä»¶ï¼Œå…± {len(file_paths)} å€‹æ–‡ä»¶")
    
    # ==================== æ–‡ä»¶æ ¼å¼é©—è­‰ ====================
    logger.info("ğŸ” é–‹å§‹æ–‡ä»¶æ ¼å¼é©—è­‰...")
    validation_start_time = time.time()
    
    # å®šç¾©æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
    valid_exts = [".pdf", ".docx"]
    
    # éæ¿¾å‡ºæ”¯æŒæ ¼å¼çš„æ–‡ä»¶
    original_count = len(file_paths)
    file_paths = [
        path for path in file_paths
        if os.path.splitext(path)[1].lower() in valid_exts
    ]
    valid_count = len(file_paths)
    
    validation_end_time = time.time()
    logger.info(f"âœ… æ–‡ä»¶æ ¼å¼é©—è­‰å®Œæˆï¼Œè€—æ™‚: {validation_end_time - validation_start_time:.2f}ç§’")
    logger.info(f"ğŸ“Š é©—è­‰çµæœ - åŸå§‹æ–‡ä»¶: {original_count}, æœ‰æ•ˆæ–‡ä»¶: {valid_count}, è·³é: {original_count - valid_count}")
    
    if not file_paths:
        logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°æ”¯æŒæ ¼å¼çš„æ–‡ä»¶")
        return []
    
    logger.info(f"ğŸ“ é–‹å§‹è™•ç† {len(file_paths)} å€‹æ–‡ä»¶...")
    
    # ==================== æ­¥é©Ÿ1: æ‰¹æ¬¡æå–metadata ====================
    logger.info("ğŸ“„ é–‹å§‹æ‰¹æ¬¡æå–æ–‡ä»¶å…ƒæ•¸æ“š...")
    extraction_start_time = time.time()
    
    if status_callback:
        status_callback("ğŸ“„ æ‰¹æ¬¡æå–æ–‡ä»¶å…ƒæ•¸æ“š...")
    
    batch_metadata = []
    extraction_errors = []
    
    for i, path in enumerate(file_paths, 1):
        file_start_time = time.time()
        filename = os.path.basename(path)
        logger.info(f"ğŸ“„ æå–ç¬¬ {i}/{len(file_paths)} å€‹æ–‡ä»¶å…ƒæ•¸æ“šï¼š{filename}")
        
        # è¨˜éŒ„æ–‡ä»¶ä¿¡æ¯
        file_size = os.path.getsize(path) if os.path.exists(path) else 0
        file_ext = os.path.splitext(path)[1].lower()
        logger.info(f"   ğŸ“Š æ–‡ä»¶ä¿¡æ¯ - å¤§å°: {file_size} bytes, æ ¼å¼: {file_ext}")
        
        if status_callback:
            status_callback(f"ğŸ“„ æå–ç¬¬ {i}/{len(file_paths)} å€‹æ–‡ä»¶å…ƒæ•¸æ“šï¼š{filename}")
        
        try:
            # æå–åŸºæœ¬å…ƒæ•¸æ“š
            extract_start_time = time.time()
            metadata = extract_metadata(path)
            extract_end_time = time.time()
            logger.info(f"   âœ… åŸºæœ¬å…ƒæ•¸æ“šæå–å®Œæˆï¼Œè€—æ™‚: {extract_end_time - extract_start_time:.2f}ç§’")
            logger.info(f"   ğŸ“ æå–çµæœ - æ¨™é¡Œ: {metadata.get('title', 'æœªçŸ¥')}, DOI: {metadata.get('doi', 'ç„¡')}")
            
            # Semantic Scholarè£œå……ä¿¡æ¯
            try:
                logger.info(f"   ğŸ” é–‹å§‹Semantic ScholaræŸ¥è©¢...")
                semantic_start_time = time.time()
                semantic_data = lookup_semantic_scholar_metadata(
                    doi=metadata.get("doi", "") or None,
                    title=metadata.get("title", "") or None
                )
                semantic_end_time = time.time()
                logger.info(f"   âœ… Semantic ScholaræŸ¥è©¢å®Œæˆï¼Œè€—æ™‚: {semantic_end_time - semantic_start_time:.2f}ç§’")
                
                if semantic_data:
                    logger.info(f"   ğŸ“Š Semantic Scholarçµæœ - æ¨™é¡Œ: {semantic_data.get('title', 'ç„¡')}, ä½œè€…æ•¸: {len(semantic_data.get('authors', []))}")
                else:
                    logger.info(f"   âš ï¸ Semantic Scholarç„¡çµæœ")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Semantic Scholar æŸ¥è©¢å¤±æ•— {path}: {e}")
                semantic_data = {}
            
            # å…ƒæ•¸æ“šæ•´åˆ
            merge_start_time = time.time()
            metadata.update({
                "title": semantic_data.get("title", metadata.get("title", "")),
                "authors": "; ".join(a["name"] for a in semantic_data.get("authors", [])),
                "year": semantic_data.get("year", ""),
                "venue": semantic_data.get("venue", ""),
                "url": semantic_data.get("url", ""),
                "original_path": path  # ä¿å­˜åŸå§‹è·¯å¾‘
            })
            merge_end_time = time.time()
            logger.info(f"   âœ… å…ƒæ•¸æ“šæ•´åˆå®Œæˆï¼Œè€—æ™‚: {merge_end_time - merge_start_time:.2f}ç§’")
            
            # å¦‚æœåŸå§‹æ–‡ä»¶æ²’æœ‰DOIä½†Semantic Scholaræ‰¾åˆ°äº†DOIï¼Œå‰‡è£œå›DOI
            if not metadata.get("doi") and semantic_data.get("externalIds", {}).get("DOI"):
                semantic_doi = semantic_data["externalIds"]["DOI"]
                metadata["doi"] = semantic_doi
                logger.info(f"âœ… é€šéSemantic Scholarè£œå›DOI: {semantic_doi}")
            
            batch_metadata.append(metadata)
            file_end_time = time.time()
            logger.info(f"   âœ… æ–‡ä»¶ {filename} å…ƒæ•¸æ“šæå–å®Œæˆï¼Œè€—æ™‚: {file_end_time - file_start_time:.2f}ç§’")
            logger.info(f"   ğŸ“ æœ€çµ‚çµæœ - æ¨™é¡Œ: {metadata.get('title', 'æœªçŸ¥æ¨™é¡Œ')}")
            
        except Exception as e:
            logger.error(f"âŒ å…ƒæ•¸æ“šæå–å¤±æ•— {filename}: {e}")
            extraction_errors.append({
                "file": filename,
                "error": str(e)
            })
            if status_callback:
                status_callback(f"âŒ å…ƒæ•¸æ“šæå–å¤±æ•—ï¼š{filename}")
    
    extraction_end_time = time.time()
    logger.info(f"âœ… æ‰¹æ¬¡å…ƒæ•¸æ“šæå–å®Œæˆï¼Œè€—æ™‚: {extraction_end_time - extraction_start_time:.2f}ç§’")
    logger.info(f"ğŸ“Š æå–çµ±è¨ˆ - æˆåŠŸ: {len(batch_metadata)}, å¤±æ•—: {len(extraction_errors)}")
    
    # ==================== æ­¥é©Ÿ2: æ‰¹æ¬¡èˆ‡è¨»å†Šæ–‡ä»¶å»é‡ ====================
    logger.info("ğŸ” é–‹å§‹èˆ‡è¨»å†Šæ–‡ä»¶å»é‡æª¢æŸ¥...")
    registry_duplicate_start_time = time.time()
    
    if status_callback:
        status_callback("ğŸ” æª¢æŸ¥èˆ‡ç¾æœ‰æ–‡ä»¶é‡è¤‡...")
    
    registry_duplicates = []
    final_metadata = []
    
    for metadata in batch_metadata:
        duplicate_check_start_time = time.time()
        duplicate_result = check_duplicate_file(metadata["original_path"], metadata)
        duplicate_check_end_time = time.time()
        
        if duplicate_result["is_duplicate"]:
            logger.info(f"âš ï¸ ç™¼ç¾é‡è¤‡æ–‡ä»¶: {metadata.get('title', 'æœªçŸ¥æ¨™é¡Œ')} - é¡å‹: {duplicate_result['duplicate_type']}")
            registry_duplicates.append(metadata)
        else:
            final_metadata.append(metadata)
            logger.debug(f"âœ… æ–‡ä»¶ç„¡é‡è¤‡: {metadata.get('title', 'æœªçŸ¥æ¨™é¡Œ')}")
    
    registry_duplicate_end_time = time.time()
    logger.info(f"âœ… è¨»å†Šæ–‡ä»¶å»é‡æª¢æŸ¥å®Œæˆï¼Œè€—æ™‚: {registry_duplicate_end_time - registry_duplicate_start_time:.2f}ç§’")
    logger.info(f"ğŸ“Š å»é‡çµæœ - é‡è¤‡: {len(registry_duplicates)}, æ–°å¢: {len(final_metadata)}")
    
    # ==================== æ­¥é©Ÿ3: æ‰¹æ¬¡é–“å»é‡ ====================
    logger.info("ğŸ” é–‹å§‹æ‰¹æ¬¡é–“å»é‡æª¢æŸ¥...")
    batch_duplicate_start_time = time.time()
    
    if status_callback:
        status_callback("ğŸ” æª¢æŸ¥æ‰¹æ¬¡å…§é‡è¤‡...")
    
    batch_duplicates = []
    unique_metadata = []
    
    for metadata in final_metadata:
        batch_check_start_time = time.time()
        batch_result = check_batch_duplicate(metadata, unique_metadata)
        batch_check_end_time = time.time()
        
        if batch_result["is_duplicate"]:
            logger.info(f"âš ï¸ ç™¼ç¾æ‰¹æ¬¡å…§é‡è¤‡: {metadata.get('title', 'æœªçŸ¥æ¨™é¡Œ')} - é¡å‹: {batch_result['duplicate_type']}")
            batch_duplicates.append(metadata)
        else:
            unique_metadata.append(metadata)
            logger.debug(f"âœ… æ‰¹æ¬¡å…§ç„¡é‡è¤‡: {metadata.get('title', 'æœªçŸ¥æ¨™é¡Œ')}")
    
    batch_duplicate_end_time = time.time()
    logger.info(f"âœ… æ‰¹æ¬¡é–“å»é‡æª¢æŸ¥å®Œæˆï¼Œè€—æ™‚: {batch_duplicate_end_time - batch_duplicate_start_time:.2f}ç§’")
    logger.info(f"ğŸ“Š æ‰¹æ¬¡å»é‡çµæœ - é‡è¤‡: {len(batch_duplicates)}, å”¯ä¸€: {len(unique_metadata)}")
    
    # ==================== æ­¥é©Ÿ4: æ‰¹æ¬¡è™•ç†å‰©é¤˜æ–‡ä»¶ ====================
    logger.info("ğŸ“¦ é–‹å§‹è™•ç†å‰©é¤˜æ–‡ä»¶...")
    processing_start_time = time.time()
    
    if status_callback:
        status_callback(f"ğŸ“¦ é–‹å§‹è™•ç† {len(unique_metadata)} å€‹æ–‡ä»¶...")
    
    results = []
    
    for i, metadata in enumerate(unique_metadata, 1):
        file_start_time = time.time()
        filename = os.path.basename(metadata["original_path"])
        logger.info(f"ğŸ“¦ è™•ç†ç¬¬ {i}/{len(unique_metadata)} å€‹æ–‡ä»¶ï¼š{filename}")
        
        if status_callback:
            status_callback(f"ğŸ“¦ æ­£åœ¨è¤‡è£½æ–‡ä»¶ `{filename}`...")
        
        try:
            # é‡å‘½åä¸¦è¤‡è£½æ–‡ä»¶
            copy_start_time = time.time()
            metadata = rename_and_copy_file(metadata["original_path"], metadata)
            copy_end_time = time.time()
            logger.info(f"   âœ… æ–‡ä»¶è¤‡è£½å®Œæˆï¼Œè€—æ™‚: {copy_end_time - copy_start_time:.2f}ç§’")
            logger.info(f"   ğŸ“„ æ–°æ–‡ä»¶å: {metadata['new_filename']}")
            
            if status_callback:
                status_callback(f"ğŸ“¦ å·²è¤‡è£½ `{filename}` è‡³ papers ç›®éŒ„ï¼Œæ–°æ–‡ä»¶åï¼š`{metadata['new_filename']}`")
            
            # æ›´æ–°å…ƒæ•¸æ“šè¨»å†Šè¡¨
            registry_start_time = time.time()
            append_metadata_to_registry(metadata)
            registry_end_time = time.time()
            logger.info(f"   âœ… å…ƒæ•¸æ“šè¨»å†Šè¡¨æ›´æ–°å®Œæˆï¼Œè€—æ™‚: {registry_end_time - registry_start_time:.2f}ç§’")
            
            if status_callback:
                status_callback(f"âœ… å·²æ›´æ–°å…ƒæ•¸æ“šè¨»å†Šè¡¨ï¼š{metadata['new_filename']}")
            
            results.append(metadata)
            file_end_time = time.time()
            logger.info(f"   âœ… æ–‡ä»¶ {filename} è™•ç†å®Œæˆï¼Œè€—æ™‚: {file_end_time - file_start_time:.2f}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶è™•ç†å¤±æ•— {filename}: {e}")
            if status_callback:
                status_callback(f"âŒ æ–‡ä»¶è™•ç†å¤±æ•—ï¼š{filename}")
    
    processing_end_time = time.time()
    logger.info(f"âœ… æ–‡ä»¶è™•ç†å®Œæˆï¼Œè€—æ™‚: {processing_end_time - processing_start_time:.2f}ç§’")
    
    # ==================== è™•ç†å®Œæˆçµ±è¨ˆ ====================
    end_time = time.time()
    total_time = end_time - start_time
    total_skipped = len(extraction_errors) + len(registry_duplicates) + len(batch_duplicates)
    
    logger.info(f"ğŸ¯ æ–‡ä»¶è™•ç†å®Œæˆï¼ç¸½è€—æ™‚: {total_time:.2f}ç§’")
    logger.info(f"   ğŸ“Š ç¸½æ–‡ä»¶æ•¸ï¼š{original_count}")
    logger.info(f"   âœ… æˆåŠŸè™•ç†ï¼š{len(results)}")
    logger.info(f"   âš ï¸ è·³éæ–‡ä»¶ï¼š{total_skipped}")
    logger.info(f"      - æå–å¤±æ•—ï¼š{len(extraction_errors)}")
    logger.info(f"      - è¨»å†Šé‡è¤‡ï¼š{len(registry_duplicates)}")
    logger.info(f"      - æ‰¹æ¬¡é‡è¤‡ï¼š{len(batch_duplicates)}")
    
    if status_callback:
        status_callback(f"âœ… è™•ç†å®Œæˆï¼æˆåŠŸè™•ç† {len(results)} å€‹æ–‡ä»¶ï¼Œè·³é {total_skipped} å€‹æ–‡ä»¶")
    
    return results


def validate_file_format(file_path: str) -> bool:
    """
    é©—è­‰æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ
    
    åƒæ•¸ï¼š
        file_path (str): æ–‡ä»¶è·¯å¾‘
    
    è¿”å›ï¼š
        bool: æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ
    """
    valid_exts = [".pdf", ".docx"]
    file_ext = os.path.splitext(file_path)[1].lower()
    return file_ext in valid_exts


def get_file_info(file_path: str) -> Dict[str, any]:
    """
    ç²å–æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
    
    åƒæ•¸ï¼š
        file_path (str): æ–‡ä»¶è·¯å¾‘
    
    è¿”å›ï¼š
        Dict[str, any]: æ–‡ä»¶ä¿¡æ¯å­—å…¸
    """
    if not os.path.exists(file_path):
        return {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}
    
    file_info = {
        "filename": os.path.basename(file_path),
        "filepath": file_path,
        "size": os.path.getsize(file_path),
        "extension": os.path.splitext(file_path)[1].lower(),
        "is_valid": validate_file_format(file_path)
    }
    
    return file_info


def batch_process_files(file_paths: List[str], batch_size: int = 5) -> List[Dict]:
    """
    æ‰¹é‡è™•ç†æ–‡ä»¶ï¼Œæ”¯æŒåˆ†æ‰¹è™•ç†å¤§é‡æ–‡ä»¶
    
    åƒæ•¸ï¼š
        file_paths (List[str]): æ–‡ä»¶è·¯å¾‘åˆ—è¡¨
        batch_size (int): æ¯æ‰¹è™•ç†çš„æ–‡ä»¶æ•¸é‡
    
    è¿”å›ï¼š
        List[Dict]: æ‰€æœ‰è™•ç†çµæœ
    """
    all_results = []
    
    # åˆ†æ‰¹è™•ç†æ–‡ä»¶
    for i in range(0, len(file_paths), batch_size):
        batch = file_paths[i:i + batch_size]
        print(f"ğŸ“¦ è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(file_paths) + batch_size - 1)//batch_size}")
        
        try:
            batch_results = process_uploaded_files(batch)
            all_results.extend(batch_results)
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡è™•ç†å¤±æ•—ï¼š{e}")
            # ç¹¼çºŒè™•ç†ä¸‹ä¸€æ‰¹
    
    return all_results


# ==================== æ¸¬è©¦ä»£ç¢¼ ====================
if __name__ == "__main__":
    """
    æ¸¬è©¦æ–‡ä»¶ä¸Šå‚³è™•ç†åŠŸèƒ½
    
    é€™å€‹æ¸¬è©¦ä»£ç¢¼ç”¨æ–¼é©—è­‰æ–‡ä»¶è™•ç†æµç¨‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
    """
    fake_test_file = "test_data/fake_paper.docx"
    
    try:
        print("ğŸ§ª é–‹å§‹æ¸¬è©¦æ–‡ä»¶ä¸Šå‚³è™•ç†...")
        
        # æ¸¬è©¦å–®å€‹æ–‡ä»¶è™•ç†
        result = process_uploaded_files([fake_test_file])
        
        print("âœ… æ¸¬è©¦å®Œæˆ")
        
        # è¼¸å‡ºè™•ç†çµæœ
        for r in result:
            print("ğŸ“ å…ƒæ•¸æ“šï¼š", r)
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—ï¼š{e}")
        print("ï¿½ï¿½ è«‹ç¢ºä¿æ¸¬è©¦æ–‡ä»¶å­˜åœ¨ä¸”æ ¼å¼æ­£ç¢º")

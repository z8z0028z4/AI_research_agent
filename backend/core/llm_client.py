"""
LLM å®¢æˆ¶ç«¯æ¨¡çµ„
==============

è² è²¬ LLM èª¿ç”¨çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œé¿å…å¾ªç’°å°å…¥å•é¡Œ
"""

import time
import json
from typing import Dict, Any, Optional, List
from openai import OpenAI

from backend.utils.logger import get_logger
from backend.utils.exceptions import LLMError, APIRequestError

logger = get_logger(__name__)


class LLMClient:
    """LLM å®¢æˆ¶ç«¯é¡ï¼Œå°è£æ‰€æœ‰ LLM èª¿ç”¨é‚è¼¯"""
    
    def __init__(self):
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯"""
        try:
            self.client = OpenAI()
            
            # æ·»åŠ  SSL é©—è­‰ç¦ç”¨é¸é …ï¼Œè§£æ±ºä¼æ¥­ç¶²è·¯ç’°å¢ƒçš„è­‰æ›¸å•é¡Œ
            import httpx
            import os
            
            # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸ï¼Œå…è¨±ç”¨æˆ¶æ§åˆ¶ SSL é©—è­‰
            disable_ssl_verify = os.getenv('DISABLE_SSL_VERIFY', 'false').lower() == 'true'
            if disable_ssl_verify:
                self.client._client = httpx.Client(verify=False)
                logger.warning("âš ï¸ SSL é©—è­‰å·²ç¦ç”¨ï¼ˆç’°å¢ƒè®Šæ•¸æ§åˆ¶ï¼‰")
            else:
                # å˜—è©¦ä½¿ç”¨é è¨­è¨­ç½®ï¼Œå¦‚æœå¤±æ•—å‰‡è‡ªå‹•ç¦ç”¨
                try:
                    # æ¸¬è©¦é€£æ¥
                    test_client = httpx.Client()
                    test_client.close()
                except Exception as e:
                    if "certificate verify failed" in str(e).lower():
                        self.client._client = httpx.Client(verify=False)
                        logger.warning("âš ï¸ æª¢æ¸¬åˆ° SSL è­‰æ›¸å•é¡Œï¼Œè‡ªå‹•ç¦ç”¨ SSL é©—è­‰")
                    else:
                        raise e
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯å¤±æ•—: {e}")
            raise
    
    def call_llm(self, prompt: str, model: str, llm_params: Dict[str, Any], **kwargs) -> str:
        """
        èª¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬
        
        åƒæ•¸ï¼š
            prompt: æç¤ºè©
            model: æ¨¡å‹åç¨±
            llm_params: æ¨¡å‹åƒæ•¸
            **kwargs: é¡å¤–åƒæ•¸
            
        è¿”å›ï¼š
            str: ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            logger.info(f"èª¿ç”¨ LLMï¼Œæ¨¡å‹ï¼š{model}")
            logger.debug(f"æç¤ºè©é•·åº¦ï¼š{len(prompt)} å­—ç¬¦")
            
            # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ä¸åŒçš„èª¿ç”¨æ–¹å¼
            if model.startswith('gpt-5'):
                return self._call_gpt5_responses_api(prompt, model, llm_params, **kwargs)
            else:
                return self._call_gpt4_chat_api(prompt, model, llm_params, **kwargs)
                
        except Exception as e:
            logger.error(f"LLM èª¿ç”¨å¤±æ•—ï¼š{e}")
            raise LLMError(f"LLM èª¿ç”¨å¤±æ•—ï¼š{str(e)}")
    
    def _call_gpt5_responses_api(self, prompt: str, model: str, llm_params: Dict[str, Any], **kwargs) -> str:
        """
        èª¿ç”¨ GPT-5 Responses API
        
        åƒæ•¸ï¼š
            prompt: æç¤ºè©
            model: æ¨¡å‹åç¨±
            llm_params: æ¨¡å‹åƒæ•¸
            **kwargs: é¡å¤–åƒæ•¸
            
        è¿”å›ï¼š
            str: ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            # æ§‹å»º Responses API åƒæ•¸
            responses_params = {
                "model": model,
                "input": [{"role": "user", "content": prompt}],
                "max_output_tokens": llm_params.get("max_output_tokens", 2000),
                "timeout": llm_params.get("timeout", 60)
            }
            
            # æ·»åŠ å¯é¸åƒæ•¸
            if "reasoning_effort" in llm_params:
                responses_params["reasoning"] = {"effort": llm_params["reasoning_effort"]}
            if "verbosity" in llm_params:
                responses_params["text"] = {"verbosity": llm_params["verbosity"]}
            if "temperature" in llm_params:
                responses_params["temperature"] = llm_params["temperature"]
            
            logger.debug(f"ä½¿ç”¨ Responses APIï¼Œåƒæ•¸ï¼š{responses_params}")
            
            # é‡è©¦æ©Ÿåˆ¶
            max_retries = 3
            base_tokens = llm_params.get("max_output_tokens", 2000)
            
            for retry_count in range(max_retries):
                # æ¯æ¬¡é‡è©¦æ™‚å¢åŠ  1000 tokens
                current_tokens = base_tokens + (retry_count * 1000)
                responses_params["max_output_tokens"] = current_tokens
                
                logger.info(f"å˜—è©¦ {retry_count + 1}/{max_retries}ï¼Œä½¿ç”¨ {current_tokens} tokens")
                
                try:
                    response = self.client.responses.create(**responses_params)
                    
                    # æª¢æŸ¥éŸ¿æ‡‰ç‹€æ…‹
                    if hasattr(response, 'status') and response.status == 'incomplete':
                        logger.warning(f"æª¢æ¸¬åˆ° incomplete ç‹€æ…‹ï¼Œå˜—è©¦æå–éƒ¨åˆ†å…§å®¹")
                        
                        # å°æ–¼éçµæ§‹åŒ–è¼¸å‡ºï¼Œå˜—è©¦æå–éƒ¨åˆ†æ–‡æœ¬
                        if hasattr(response, 'output_text') and response.output_text:
                            output = response.output_text
                            logger.info(f"å¾ incomplete éŸ¿æ‡‰ä¸­æå–éƒ¨åˆ†æ–‡æœ¬: {len(output)} å­—ç¬¦")
                            return output
                        
                        # å¦‚æœç„¡æ³•æå–éƒ¨åˆ†å…§å®¹ï¼Œå‰‡é‡è©¦
                        if retry_count < max_retries - 1:
                            logger.warning(f"ç„¡æ³•æå–éƒ¨åˆ†å…§å®¹ï¼Œé‡è©¦ {retry_count + 1}/{max_retries}")
                            time.sleep(2)
                            continue
                    
                    # æå–æ–‡æœ¬å…§å®¹
                    if hasattr(response, 'output_text') and response.output_text:
                        output = response.output_text
                        logger.info(f"æˆåŠŸæå–æ–‡æœ¬: {len(output)} å­—ç¬¦")
                        return output
                    elif hasattr(response, 'output') and response.output:
                        # å¾ output é™£åˆ—ä¸­æå–æ–‡æœ¬
                        output_parts = []
                        for item in response.output:
                            if hasattr(item, 'message') and hasattr(item.message, 'content'):
                                for content in item.message.content:
                                    if hasattr(content, 'text') and content.text:
                                        output_parts.append(content.text)
                        output = "".join(output_parts)
                        if output:
                            logger.info(f"æˆåŠŸæå–æ–‡æœ¬: {len(output)} å­—ç¬¦")
                            return output
                    
                    # å¦‚æœéƒ½å¤±æ•—äº†ï¼Œå˜—è©¦ä½¿ç”¨ content
                    if hasattr(response, 'content'):
                        output = response.content
                        logger.info(f"ä½¿ç”¨ content æå–æ–‡æœ¬: {len(output)} å­—ç¬¦")
                        return output
                        
                except Exception as e:
                    logger.error(f"API èª¿ç”¨å¤±æ•— (å˜—è©¦ {retry_count + 1}/{max_retries}): {e}")
                    if retry_count < max_retries - 1:
                        time.sleep(2)
                        continue
                    raise
            
            raise APIRequestError("æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—")
            
        except Exception as e:
            logger.error(f"GPT-5 Responses API èª¿ç”¨å¤±æ•—: {e}")
            raise
    
    def _call_gpt4_chat_api(self, prompt: str, model: str, llm_params: Dict[str, Any], **kwargs) -> str:
        """
        èª¿ç”¨ GPT-4 Chat Completions API
        
        åƒæ•¸ï¼š
            prompt: æç¤ºè©
            model: æ¨¡å‹åç¨±
            llm_params: æ¨¡å‹åƒæ•¸
            **kwargs: é¡å¤–åƒæ•¸
            
        è¿”å›ï¼š
            str: ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            # æ§‹å»º Chat Completions API åƒæ•¸
            chat_params = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": llm_params.get("max_tokens", 2000),
                "temperature": llm_params.get("temperature", 0.7)
            }
            
            logger.debug(f"ä½¿ç”¨ Chat Completions APIï¼Œåƒæ•¸ï¼š{chat_params}")
            
            response = self.client.chat.completions.create(**chat_params)
            
            if response.choices and response.choices[0].message:
                output = response.choices[0].message.content
                logger.info(f"Chat API èª¿ç”¨æˆåŠŸï¼Œå›æ‡‰é•·åº¦ï¼š{len(output)} å­—ç¬¦")
                return output
            else:
                raise APIRequestError("Chat API è¿”å›ç©ºéŸ¿æ‡‰")
                
        except Exception as e:
            logger.error(f"Chat Completions API èª¿ç”¨å¤±æ•—ï¼š{e}")
            raise
    
    def call_structured_llm(self, prompt: str, schema: Dict[str, Any], model: str, llm_params: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """
        èª¿ç”¨çµæ§‹åŒ– LLM ç”Ÿæˆ JSON æ ¼å¼å…§å®¹
        
        åƒæ•¸ï¼š
            prompt: æç¤ºè©
            schema: JSON Schema
            model: æ¨¡å‹åç¨±
            llm_params: æ¨¡å‹åƒæ•¸
            **kwargs: é¡å¤–åƒæ•¸
            
        è¿”å›ï¼š
            Dict[str, Any]: çµæ§‹åŒ–æ•¸æ“š
        """
        try:
            logger.info(f"èª¿ç”¨çµæ§‹åŒ– LLMï¼Œæ¨¡å‹ï¼š{model}")
            
            # åªæ”¯æ´ GPT-5 ç³»åˆ—
            if not model.startswith('gpt-5'):
                raise LLMError(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
            
            return self._call_gpt5_structured_api(prompt, schema, model, llm_params, **kwargs)
                
        except Exception as e:
            logger.error(f"çµæ§‹åŒ– LLM èª¿ç”¨å¤±æ•—ï¼š{e}")
            raise LLMError(f"çµæ§‹åŒ– LLM èª¿ç”¨å¤±æ•—ï¼š{str(e)}")
    
    def _call_gpt5_structured_api(self, prompt: str, schema: Dict[str, Any], model: str, llm_params: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """
        èª¿ç”¨ GPT-5 çµæ§‹åŒ– API
        
        åƒæ•¸ï¼š
            prompt: æç¤ºè©
            schema: JSON Schema
            model: æ¨¡å‹åç¨±
            llm_params: æ¨¡å‹åƒæ•¸
            **kwargs: é¡å¤–åƒæ•¸
            
        è¿”å›ï¼š
            Dict[str, Any]: çµæ§‹åŒ–æ•¸æ“š
        """
        try:
            # æ§‹å»º Responses API åƒæ•¸
            responses_params = {
                "model": model,
                "input": [{"role": "user", "content": prompt}],
                "text": {
                    "format": {
                        "type": "json_schema",
                        "name": "ResearchProposal",
                        "strict": True,
                        "schema": schema,
                    },
                    "verbosity": llm_params.get("verbosity", "low")
                },
                "max_output_tokens": llm_params.get("max_output_tokens", 2000),
                "timeout": llm_params.get("timeout", 60)
            }
            
            # è™•ç† reasoning åƒæ•¸
            if 'reasoning' in llm_params:
                logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨é©é…å¾Œçš„ reasoning åƒæ•¸: {llm_params['reasoning']}")
                responses_params['reasoning'] = llm_params['reasoning']
            else:
                logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨é»˜èª reasoning åƒæ•¸")
                responses_params['reasoning'] = {"effort": llm_params.get("reasoning_effort", "medium")}
            
            # è™•ç† text åƒæ•¸
            if 'text' in llm_params:
                logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨é©é…å¾Œçš„ text åƒæ•¸: {llm_params['text']}")
                # ä¿ç•™ JSON Schema æ ¼å¼ä¿¡æ¯ï¼Œåªæ›´æ–° verbosity
                if 'verbosity' in llm_params['text']:
                    responses_params['text']['verbosity'] = llm_params['text']['verbosity']
                logger.info(f"ğŸ” [DEBUG] æ›´æ–°å¾Œçš„ text åƒæ•¸: {responses_params['text']}")
            else:
                logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨é»˜èª text åƒæ•¸")
                responses_params['text']['verbosity'] = llm_params.get("verbosity", "low")
            
            logger.debug(f"ä½¿ç”¨ Responses API with JSON Schemaï¼Œåƒæ•¸ï¼š{responses_params}")
            
            # é‡è©¦æ©Ÿåˆ¶
            max_retries = 3
            base_tokens = llm_params.get("max_output_tokens", 2000)
            
            for retry_count in range(max_retries):
                # æ¯æ¬¡é‡è©¦æ™‚å¢åŠ  1000 tokens
                current_tokens = base_tokens + (retry_count * 1000)
                responses_params["max_output_tokens"] = current_tokens
                
                logger.info(f"å˜—è©¦ {retry_count + 1}/{max_retries}ï¼Œä½¿ç”¨ {current_tokens} tokens")
                
                try:
                    response = self.client.responses.create(**responses_params)
                    
                    # æª¢æŸ¥éŸ¿æ‡‰ç‹€æ…‹
                    if hasattr(response, 'status') and response.status == 'incomplete':
                        logger.warning(f"æª¢æ¸¬åˆ° incomplete ç‹€æ…‹ï¼Œå˜—è©¦æå–éƒ¨åˆ†å…§å®¹")
                        
                        # å˜—è©¦å¾ incomplete éŸ¿æ‡‰ä¸­æå–éƒ¨åˆ† JSON
                        partial_json = self._extract_partial_json_from_response(response)
                        if partial_json:
                            logger.info("æˆåŠŸå¾ incomplete éŸ¿æ‡‰ä¸­æå–éƒ¨åˆ† JSON")
                            return partial_json
                        
                        # å¦‚æœç„¡æ³•æå–éƒ¨åˆ†å…§å®¹ï¼Œå‰‡é‡è©¦
                        if retry_count < max_retries - 1:
                            logger.warning(f"ç„¡æ³•æå–éƒ¨åˆ†å…§å®¹ï¼Œé‡è©¦ {retry_count + 1}/{max_retries}")
                            time.sleep(2)
                            continue
                    
                    # æå– JSON å…§å®¹
                    if hasattr(response, 'output_text') and response.output_text:
                        try:
                            result = json.loads(response.output_text)
                            logger.info("æˆåŠŸè§£æ JSON çµæ§‹åŒ–ææ¡ˆ")
                            return result
                        except json.JSONDecodeError as e:
                            logger.error(f"JSON è§£æå¤±æ•—: {e}")
                            logger.debug(f"å˜—è©¦çš„æ–‡æœ¬: {response.output_text[:200]}...")
                    
                    # å¦‚æœ output_text å¤±æ•—ï¼Œå˜—è©¦å¾ output æå–
                    if hasattr(response, 'output') and response.output:
                        text_content = ""
                        for item in response.output:
                            if hasattr(item, 'message') and hasattr(item.message, 'content'):
                                for content in item.message.content:
                                    if hasattr(content, 'text') and content.text:
                                        text_content += content.text
                        
                        if text_content:
                            try:
                                result = json.loads(text_content)
                                logger.info("æˆåŠŸè§£æ JSON çµæ§‹åŒ–ææ¡ˆ")
                                return result
                            except json.JSONDecodeError as e:
                                logger.error(f"JSON è§£æå¤±æ•—: {e}")
                                logger.debug(f"å˜—è©¦çš„æ–‡æœ¬: {text_content[:200]}...")
                    
                    logger.warning("ç„¡æ³•å¾ Responses API æå– JSON å…§å®¹")
                    
                except Exception as e:
                    logger.error(f"API èª¿ç”¨å¤±æ•— (å˜—è©¦ {retry_count + 1}/{max_retries}): {e}")
                    if retry_count < max_retries - 1:
                        time.sleep(2)
                        continue
                    raise
            
            raise APIRequestError("æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—")
            
        except Exception as e:
            logger.error(f"GPT-5 çµæ§‹åŒ– API èª¿ç”¨å¤±æ•—: {e}")
            raise
    
    def _extract_partial_json_from_response(self, response) -> Optional[Dict[str, Any]]:
        """
        å¾ incomplete éŸ¿æ‡‰ä¸­æå–éƒ¨åˆ† JSON å…§å®¹
        
        Args:
            response: OpenAI Responses API éŸ¿æ‡‰å°è±¡
            
        Returns:
            Optional[Dict[str, Any]]: æå–çš„ JSON å°è±¡ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› None
        """
        try:
            # å˜—è©¦å¾ output_text æå–
            if hasattr(response, 'output_text') and response.output_text:
                text = response.output_text
                logger.debug(f"å˜—è©¦å¾ output_text æå–éƒ¨åˆ† JSON: {text[:200]}...")
                
                # å˜—è©¦æ‰¾åˆ°æœ€å¾Œä¸€å€‹å®Œæ•´çš„ JSON å°è±¡
                brace_count = 0
                last_complete_pos = -1
                
                for i, char in enumerate(text):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            last_complete_pos = i
                
                if last_complete_pos > 0:
                    complete_json = text[:last_complete_pos + 1]
                    try:
                        result = json.loads(complete_json)
                        logger.info(f"æˆåŠŸä¿®å¾©ä¸å®Œæ•´çš„ JSONï¼Œé•·åº¦: {len(complete_json)} å­—ç¬¦")
                        return result
                    except json.JSONDecodeError as e:
                        logger.debug(f"JSON ä¿®å¾©å¤±æ•—: {e}")
            
            # å˜—è©¦å¾ output é™£åˆ—æå–
            if hasattr(response, 'output') and response.output:
                text_content = ""
                for item in response.output:
                    if hasattr(item, 'message') and hasattr(item.message, 'content'):
                        for content in item.message.content:
                            if hasattr(content, 'text') and content.text:
                                text_content += content.text
                
                if text_content:
                    logger.debug(f"å˜—è©¦å¾ output é™£åˆ—æå–éƒ¨åˆ† JSON: {text_content[:200]}...")
                    
                    # åŒæ¨£å˜—è©¦ä¿®å¾©ä¸å®Œæ•´çš„ JSON
                    brace_count = 0
                    last_complete_pos = -1
                    
                    for i, char in enumerate(text_content):
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                last_complete_pos = i
                    
                    if last_complete_pos > 0:
                        complete_json = text_content[:last_complete_pos + 1]
                        try:
                            result = json.loads(complete_json)
                            logger.info(f"æˆåŠŸå¾ output é™£åˆ—ä¿®å¾©ä¸å®Œæ•´çš„ JSONï¼Œé•·åº¦: {len(complete_json)} å­—ç¬¦")
                            return result
                        except json.JSONDecodeError as e:
                            logger.debug(f"output é™£åˆ— JSON ä¿®å¾©å¤±æ•—: {e}")
            
            logger.warning("ç„¡æ³•å¾ incomplete éŸ¿æ‡‰ä¸­æå–æœ‰æ•ˆçš„ JSON å…§å®¹")
            return None
            
        except Exception as e:
            logger.error(f"æå–éƒ¨åˆ† JSON æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None


# å…¨å±€ LLM å®¢æˆ¶ç«¯å¯¦ä¾‹
_llm_client = None


def get_llm_client() -> LLMClient:
    """ç²å–å…¨å±€ LLM å®¢æˆ¶ç«¯å¯¦ä¾‹"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
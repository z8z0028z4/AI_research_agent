"""
å‘é‡åº«ç®¡ç†æ¨¡çµ„
==========

è² è²¬ç®¡ç†å‘é‡æ•¸æ“šåº«çš„è¼‰å…¥ã€æª¢ç´¢å’Œçµ±è¨ˆåŠŸèƒ½
"""

import os
import logging
from typing import List, Dict, Any, Optional
from langchain.schema import Document
from langchain_chroma import Chroma

from ..chunk_embedding import get_chroma_instance
from ..utils import extract_text_snippet

# é…ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)

def load_paper_vectorstore() -> Chroma:
    """
    è¼‰å…¥è«–æ–‡å‘é‡æ•¸æ“šåº«
    
    Returns:
        Chroma: è«–æ–‡å‘é‡æ•¸æ“šåº«å¯¦ä¾‹
    """
    try:
        vectorstore = get_chroma_instance("paper")
        logger.info("è«–æ–‡å‘é‡æ•¸æ“šåº«è¼‰å…¥æˆåŠŸ")
        return vectorstore
    except Exception as e:
        logger.error(f"è«–æ–‡å‘é‡æ•¸æ“šåº«è¼‰å…¥å¤±æ•—: {e}")
        raise

def load_experiment_vectorstore() -> Chroma:
    """
    è¼‰å…¥å¯¦é©—å‘é‡æ•¸æ“šåº«
    
    Returns:
        Chroma: å¯¦é©—å‘é‡æ•¸æ“šåº«å¯¦ä¾‹
    """
    try:
        vectorstore = get_chroma_instance("experiment")
        logger.info("å¯¦é©—å‘é‡æ•¸æ“šåº«è¼‰å…¥æˆåŠŸ")
        return vectorstore
    except Exception as e:
        logger.error(f"å¯¦é©—å‘é‡æ•¸æ“šåº«è¼‰å…¥å¤±æ•—: {e}")
        raise

def search_documents(
    vectorstore: Chroma,
    query: str,
    k: int = 5,
    fetch_k: int = 20,
    filter_dict: Optional[Dict[str, Any]] = None
) -> List[Document]:
    """
    åœ¨å‘é‡æ•¸æ“šåº«ä¸­æœç´¢æ–‡æª”
    
    Args:
        vectorstore: å‘é‡æ•¸æ“šåº«å¯¦ä¾‹
        query: æœç´¢æŸ¥è©¢
        k: è¿”å›çš„æ–‡æª”æ•¸é‡
        fetch_k: æª¢ç´¢çš„æ–‡æª”æ•¸é‡
        filter_dict: éæ¿¾æ¢ä»¶
        
    Returns:
        List[Document]: æœç´¢çµæœæ–‡æª”åˆ—è¡¨
    """
    try:
        if filter_dict:
            docs = vectorstore.similarity_search_with_relevance_scores(
                query, k=fetch_k, filter=filter_dict
            )
        else:
            docs = vectorstore.similarity_search_with_relevance_scores(
                query, k=fetch_k
            )
        
        # æŒ‰ç›¸é—œæ€§æ’åºä¸¦è¿”å›å‰kå€‹
        docs.sort(key=lambda x: x[1], reverse=True)
        return [doc[0] for doc in docs[:k]]
        
    except Exception as e:
        logger.error(f"æ–‡æª”æœç´¢å¤±æ•—: {e}")
        return []

def get_vectorstore_stats(vectorstore_type: str = "paper") -> Dict[str, Any]:
    """
    ç²å–å‘é‡æ•¸æ“šåº«çµ±è¨ˆä¿¡æ¯
    
    Args:
        vectorstore_type: å‘é‡æ•¸æ“šåº«é¡å‹ ("paper" æˆ– "experiment")
        
    Returns:
        Dict[str, Any]: çµ±è¨ˆä¿¡æ¯
    """
    try:
        from ..chunk_embedding import get_vectorstore_stats as get_stats
        return get_stats(vectorstore_type)
    except Exception as e:
        logger.error(f"ç²å–å‘é‡æ•¸æ“šåº«çµ±è¨ˆå¤±æ•—: {e}")
        return {"error": str(e)}

def preview_chunks(chunks: List[Document], title: str, max_preview: int = 5) -> None:
    """
    é è¦½æ–‡æª”å¡Šå…§å®¹
    
    Args:
        chunks: æ–‡æª”å¡Šåˆ—è¡¨
        title: é è¦½æ¨™é¡Œ
        max_preview: æœ€å¤§é è¦½æ•¸é‡
    """
    logger.info(f"ğŸ“„ {title} - æª¢ç´¢åˆ° {len(chunks)} å€‹æ–‡æª”å¡Š")
    
    for i, chunk in enumerate(chunks[:max_preview]):
        try:
            # æå–æ–‡æª”ä¿¡æ¯
            metadata = chunk.metadata
            content_preview = extract_text_snippet(chunk.page_content, max_length=150)
            
            # è¨˜éŒ„é è¦½ä¿¡æ¯
            logger.info(f"  #{i+1} | {metadata.get('filename', 'Unknown')} | {content_preview}")
            
        except Exception as e:
            logger.warning(f"  #{i+1} | é è¦½å¤±æ•—: {e}")
    
    if len(chunks) > max_preview:
        logger.info(f"  ... é‚„æœ‰ {len(chunks) - max_preview} å€‹æ–‡æª”å¡Š")

def combine_search_results(
    paper_chunks: List[Document],
    experiment_chunks: List[Document],
    max_total: int = 10
) -> List[Document]:
    """
    åˆä½µè«–æ–‡å’Œå¯¦é©—çš„æœç´¢çµæœ
    
    Args:
        paper_chunks: è«–æ–‡æ–‡æª”å¡Š
        experiment_chunks: å¯¦é©—æ–‡æª”å¡Š
        max_total: æœ€å¤§ç¸½æ•¸
        
    Returns:
        List[Document]: åˆä½µå¾Œçš„æ–‡æª”å¡Šåˆ—è¡¨
    """
    combined_chunks = []
    
    # æ·»åŠ è«–æ–‡æ–‡æª”å¡Š
    for chunk in paper_chunks:
        if len(combined_chunks) >= max_total:
            break
        combined_chunks.append(chunk)
    
    # æ·»åŠ å¯¦é©—æ–‡æª”å¡Š
    for chunk in experiment_chunks:
        if len(combined_chunks) >= max_total:
            break
        combined_chunks.append(chunk)
    
    logger.info(f"åˆä½µæœç´¢çµæœ: {len(paper_chunks)} è«–æ–‡ + {len(experiment_chunks)} å¯¦é©— = {len(combined_chunks)} ç¸½è¨ˆ")
    
    return combined_chunks

def format_search_results(chunks: List[Document]) -> List[Dict[str, Any]]:
    """
    æ ¼å¼åŒ–æœç´¢çµæœ
    
    Args:
        chunks: æ–‡æª”å¡Šåˆ—è¡¨
        
    Returns:
        List[Dict[str, Any]]: æ ¼å¼åŒ–å¾Œçš„çµæœ
    """
    formatted_results = []
    
    for i, chunk in enumerate(chunks):
        try:
            metadata = chunk.metadata
            content = chunk.page_content
            
            formatted_result = {
                "index": i + 1,
                "filename": metadata.get("filename", "Unknown"),
                "file_path": metadata.get("file_path", ""),
                "page": metadata.get("page", ""),
                "content": content,
                "content_preview": extract_text_snippet(content, max_length=200),
                "metadata": metadata
            }
            
            formatted_results.append(formatted_result)
            
        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–æœç´¢çµæœå¤±æ•— #{i+1}: {e}")
    
    return formatted_results

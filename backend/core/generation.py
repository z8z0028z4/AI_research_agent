"""
ç”Ÿæˆæ¨¡çµ„
========

è² è²¬ LLM èª¿ç”¨å’Œå…§å®¹ç”Ÿæˆ
é‡æ§‹å¾Œä½¿ç”¨æ–°çš„æ¨¡çµ„æ¶æ§‹ï¼Œé¿å…å¾ªç’°å°å…¥å•é¡Œ
"""

import time
import json
from typing import Dict, Any, Optional, List

from backend.utils.logger import get_logger
from backend.utils.exceptions import LLMError, APIRequestError
from backend.core.llm_client import get_llm_client
from backend.core.model_config import get_current_model, get_model_params

logger = get_logger(__name__)


def call_llm(prompt: str, **kwargs) -> str:
    """
    èª¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬
    
    åƒæ•¸ï¼š
        prompt: æç¤ºè©
        **kwargs: é¡å¤–åƒæ•¸
        
    è¿”å›ï¼š
        str: ç”Ÿæˆçš„æ–‡æœ¬
    """
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        
        # ä½¿ç”¨æ–°çš„ LLM å®¢æˆ¶ç«¯
        llm_client = get_llm_client()
        return llm_client.call_llm(prompt, current_model, llm_params, **kwargs)
            
    except Exception as e:
        logger.error(f"LLM èª¿ç”¨å¤±æ•—ï¼š{e}")
        raise LLMError(f"LLM èª¿ç”¨å¤±æ•—ï¼š{str(e)}")


# èˆŠçš„å¯¦ç¾å‡½æ•¸å·²è¢«æ–°çš„ LLM å®¢æˆ¶ç«¯æ›¿ä»£
# _call_gpt5_responses_api å’Œ _call_gpt4_chat_api ç¾åœ¨åœ¨ llm_client.py ä¸­å¯¦ç¾


def call_structured_llm(prompt: str, schema: Dict[str, Any], **kwargs) -> Dict[str, Any]:
    """
    èª¿ç”¨çµæ§‹åŒ– LLM ç”Ÿæˆ JSON æ ¼å¼å…§å®¹
    
    åƒæ•¸ï¼š
        prompt: æç¤ºè©
        schema: JSON Schema
        **kwargs: é¡å¤–åƒæ•¸
        
    è¿”å›ï¼š
        Dict[str, Any]: çµæ§‹åŒ–æ•¸æ“š
    """
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        
        logger.info(f"èª¿ç”¨çµæ§‹åŒ– LLMï¼Œæ¨¡å‹ï¼š{current_model}")
        
        # åªæ”¯æ´ GPT-5 ç³»åˆ—
        if not current_model.startswith('gpt-5'):
            raise LLMError(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
        
        # ä½¿ç”¨æ–°çš„ LLM å®¢æˆ¶ç«¯
        llm_client = get_llm_client()
        return llm_client.call_structured_llm(prompt, schema, current_model, llm_params, **kwargs)
            
    except Exception as e:
        logger.error(f"çµæ§‹åŒ– LLM èª¿ç”¨å¤±æ•—ï¼š{e}")
        raise LLMError(f"çµæ§‹åŒ– LLM èª¿ç”¨å¤±æ•—ï¼š{str(e)}")


# èˆŠçš„ _call_gpt5_structured_api å‡½æ•¸å·²è¢«æ–°çš„ LLM å®¢æˆ¶ç«¯æ›¿ä»£


# _call_gpt4_structured_api å‡½æ•¸å·²ç§»é™¤ - ä¸å†æ”¯æ´ GPT-4 ç³»åˆ—


# generate_proposal_with_fallback å‡½æ•¸å·²ç§»é™¤ - ä¸å†æ”¯æ´éçµæ§‹åŒ–è¼¸å‡º fallback


# èˆŠçš„ _extract_partial_json_from_response å‡½æ•¸å·²è¢«æ–°çš„ LLM å®¢æˆ¶ç«¯æ›¿ä»£


def call_llm_structured_proposal(system_prompt: str, user_prompt: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–ç ”ç©¶ææ¡ˆ
    
    Args:
        system_prompt: ç³»çµ±æç¤ºè©
        user_prompt: ç”¨æˆ¶æç¤ºè©ï¼ˆåŒ…å«æ–‡ç»æ‘˜è¦å’Œç ”ç©¶ç›®æ¨™ï¼‰
    
    Returns:
        Dict[str, Any]: ç¬¦åˆRESEARCH_PROPOSAL_SCHEMAçš„çµæ§‹åŒ–ææ¡ˆ
    """
    logger.info(f"èª¿ç”¨çµæ§‹åŒ–LLMï¼Œç³»çµ±æç¤ºè©é•·åº¦ï¼š{len(system_prompt)} å­—ç¬¦")
    logger.info(f"ç”¨æˆ¶æç¤ºè©é•·åº¦ï¼š{len(user_prompt)} å­—ç¬¦")
    
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        logger.info(f"ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        logger.debug(f"æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        raise LLMError(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{str(e)}")
    
    try:
        # åªæ”¯æ´ GPT-5 ç³»åˆ—ä½¿ç”¨ Responses API
        if not current_model.startswith('gpt-5'):
            raise LLMError(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
        
        from backend.core.schema_manager import create_research_proposal_schema
        
        # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
        current_schema = create_research_proposal_schema()
        
        # æ·»åŠ èª¿è©¦æ—¥èªŒ
        logger.info(f"ğŸ” [DEBUG] ç²å–åˆ°çš„ schema: {current_schema is not None}")
        if current_schema:
            logger.info(f"ğŸ” [DEBUG] Schema é¡å‹: {current_schema.get('type', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema æ¨™é¡Œ: {current_schema.get('title', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema å¿…éœ€å­—æ®µ: {current_schema.get('required', [])}")
        else:
            logger.warning("âš ï¸ [DEBUG] Schema ç‚ºç©ºï¼Œå°‡å›é€€åˆ°å‚³çµ±æ–‡æœ¬ç”Ÿæˆ")
        
        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        return call_structured_llm(full_prompt, current_schema)
        
    except Exception as e:
        logger.error(f"çµæ§‹åŒ–LLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        raise LLMError(f"çµæ§‹åŒ–LLMèª¿ç”¨å¤±æ•—ï¼š{str(e)}")


def call_llm_structured_experimental_detail(chunks: List, proposal: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–å¯¦é©—ç´°ç¯€
    
    Args:
        chunks: æ–‡ç»ç‰‡æ®µ
        proposal: ç ”ç©¶ææ¡ˆ
    
    Returns:
        Dict[str, Any]: ç¬¦åˆEXPERIMENTAL_DETAIL_SCHEMAçš„çµæ§‹åŒ–å¯¦é©—ç´°ç¯€
    """
    logger.info(f"èª¿ç”¨çµæ§‹åŒ–å¯¦é©—ç´°ç¯€LLMï¼Œæ–‡ç»ç‰‡æ®µæ•¸é‡ï¼š{len(chunks)}")
    logger.info(f"ææ¡ˆé•·åº¦ï¼š{len(proposal)} å­—ç¬¦")
    
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        logger.info(f"ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        logger.debug(f"æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        raise LLMError(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{str(e)}")
    
    try:
        # åªæ”¯æ´ GPT-5 ç³»åˆ—ä½¿ç”¨ Responses API
        if not current_model.startswith('gpt-5'):
            raise LLMError(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
        
        from backend.core.schema_manager import create_experimental_detail_schema
        
        # æ§‹å»ºç³»çµ±æç¤ºè©
        system_prompt = f"""
        You are a professional materials synthesis consultant, skilled at generating detailed experimental procedures based on literature and research proposals.

        Based on the following research proposal and literature information, please generate detailed experimental details:

        --- Research Proposal ---
        {proposal}

        Please generate detailed experimental details including the following four sections:
        1. Synthesis Process: Detailed synthesis steps, conditions, durations, etc.
        2. Materials and Conditions: Materials used, concentrations, temperatures, pressures, and other reaction conditions
        3. Analytical Methods: Characterization techniques such as XRD, SEM, NMR, etc.
        4. Precautions: Experimental notes and safety precautions
        """
        
        # æ§‹å»ºç”¨æˆ¶æç¤ºè©ï¼ˆåŒ…å«æ–‡ç»æ‘˜è¦ï¼‰
        context_text = ""
        citations = []
        for i, doc in enumerate(chunks):
            meta = doc.metadata
            title = meta.get("title", "Untitled")
            filename = meta.get("filename") or meta.get("source", "Unknown")
            page = meta.get("page_number") or meta.get("page", "?")
            
            context_text += f"[{i+1}] {title} | Page {page}\n{doc.page_content}\n\n"
            citations.append({
                "label": f"[{i+1}]",
                "title": title,
                "source": filename,
                "page": page
            })
        
        user_prompt = f"""
        Based on the following literature information, generate detailed experimental procedures:

        --- Literature Excerpts ---
        {context_text}
        
        
        """
        
        # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
        current_schema = create_experimental_detail_schema()
        
        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        # èª¿ç”¨çµæ§‹åŒ– LLM
        experimental_data = call_structured_llm(full_prompt, current_schema)
        
        # æ·»åŠ å¼•ç”¨ä¿¡æ¯
        if experimental_data:
            experimental_data['citations'] = citations
        
        return experimental_data
        
    except Exception as e:
        logger.error(f"çµæ§‹åŒ–å¯¦é©—ç´°ç¯€LLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        return {}






def call_llm_structured_revision_proposal(question: str, new_chunks: List, old_chunks: List, proposal: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–ä¿®è¨‚ææ¡ˆ (åŒ…å«ä¿®è¨‚èªªæ˜)
    
    Args:
        question: ç”¨æˆ¶åé¥‹/å•é¡Œ
        new_chunks: æ–°æª¢ç´¢çš„æ–‡æª”å¡Š
        old_chunks: åŸå§‹æ–‡æª”å¡Š
        proposal: åŸå§‹ææ¡ˆ
    
    Returns:
        Dict[str, Any]: ç¬¦åˆREVISION_PROPOSAL_SCHEMAçš„çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆ
    """
    logger.info(f"èª¿ç”¨çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆLLMï¼Œç”¨æˆ¶åé¥‹é•·åº¦ï¼š{len(question)}")
    logger.info(f"æ–°æ–‡æª”å¡Šæ•¸é‡ï¼š{len(new_chunks)}")
    logger.info(f"åŸæ–‡æª”å¡Šæ•¸é‡ï¼š{len(old_chunks)}")
    logger.info(f"åŸå§‹ææ¡ˆé•·åº¦ï¼š{len(proposal)} å­—ç¬¦")
    
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        logger.info(f"ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        logger.debug(f"æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        raise LLMError(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{str(e)}")
    
    try:
        # åªæ”¯æ´ GPT-5 ç³»åˆ—ä½¿ç”¨ Responses API
        if not current_model.startswith('gpt-5'):
            raise LLMError(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
        
        from backend.core.schema_manager import create_revision_proposal_schema
        
        # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
        current_schema = create_revision_proposal_schema()
        
        # æ·»åŠ èª¿è©¦æ—¥èªŒ
        logger.info(f"ğŸ” [DEBUG] ç²å–åˆ°çš„ schema: {current_schema is not None}")
        if current_schema:
            logger.info(f"ğŸ” [DEBUG] Schema é¡å‹: {current_schema.get('type', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema æ¨™é¡Œ: {current_schema.get('title', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema å¿…éœ€å­—æ®µ: {current_schema.get('required', [])}")
        else:
            logger.warning("âš ï¸ [DEBUG] Schema ç‚ºç©ºï¼Œå°‡å›é€€åˆ°å‚³çµ±æ–‡æœ¬ç”Ÿæˆ")
        
        # æ§‹å»ºæç¤ºè©
        system_prompt = """
        You are an experienced materials experiment design consultant. Please help modify parts of the research proposal based on user feedback, original proposal, and literature content.

        Your task is to generate a modified research proposal based on user feedback, original proposal, and literature content. The proposal should be innovative, scientifically rigorous, and feasible.

        IMPORTANT: You must respond in valid JSON format only. Do not include any text before or after the JSON object.

        The JSON must have the following structure:
        {
            "revision_explanation": "Brief explanation of revision logic and key improvements based on user feedback",
            "proposal_title": "Title of the research proposal",
            "need": "Research need and current limitations",
            "solution": "Proposed design and development strategies",
            "differentiation": "Comparison with existing technologies",
            "benefit": "Expected improvements and benefits",
            "experimental_overview": "Experimental approach and methodology",
            "materials_list": ["material1", "material2", "material3"]
        }

        Key requirements:
        1. Prioritize the areas that the user wants to modify and look for possible improvement directions from the literature
        2. Except for the areas that the user is dissatisfied with, other parts should maintain the original proposal content without changes
        3. Maintain scientific rigor, clarity, and avoid vague descriptions
        4. Use only the provided literature labels ([1], [2], etc.) for citations, and do not fabricate sources
        5. Ensure every claim is supported by a cited source or reasonable extension from the literature
        6. For materials_list, include ONLY IUPAC chemical names without any descriptions, notes, or parenthetical explanations. Each item must be a single chemical name only.
        7. The revision_explanation should briefly explain the logic of changes and key improvements based on user feedback
        """
        
        # æ§‹å»ºæ–‡æª”å…§å®¹
        old_text = ""
        for i, doc in enumerate(old_chunks):
            # è™•ç†å¯èƒ½æ˜¯å­—å…¸æ ¼å¼çš„ chunks
            if hasattr(doc, 'metadata'):
                metadata = doc.metadata
                page_content = doc.page_content
            else:
                metadata = doc.get('metadata', {})
                page_content = doc.get('page_content', '')
            
            title = metadata.get("title", "Untitled")
            filename = metadata.get("filename") or metadata.get("source", "Unknown")
            page = metadata.get("page_number") or metadata.get("page", "?")
            snippet = page_content[:80].replace("\n", " ")
            old_text += f"    [{i+1}] {title} | Page {page}\n{snippet}\n\n"
        
        new_text = ""
        for i, doc in enumerate(new_chunks):
            # è™•ç†å¯èƒ½æ˜¯å­—å…¸æ ¼å¼çš„ chunks
            if hasattr(doc, 'metadata'):
                metadata = doc.metadata
                page_content = doc.page_content
            else:
                metadata = doc.get('metadata', {})
                page_content = doc.get('page_content', '')
            
            title = metadata.get("title", "Untitled")
            filename = metadata.get("filename") or metadata.get("source", "Unknown")
            page = metadata.get("page_number") or metadata.get("page", "?")
            snippet = page_content[:80].replace("\n", " ")
            new_text += f"    [{i+1}] {title} | Page {page}\n{snippet}\n\n"
        
        user_prompt = f"""
        --- User Feedback ---
        {question}

        --- Original Proposal Content ---
        {proposal}

        --- Literature Excerpts Based on Original Proposal ---
        {old_text}

        --- New Retrieved Excerpts Based on Feedback ---
        {new_text}
        """
        
        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        return call_structured_llm(full_prompt, current_schema)
        
    except Exception as e:
        logger.error(f"çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆLLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        return {}


def call_llm_structured_revision_experimental_detail(
    question: str, 
    new_chunks: List, 
    old_chunks: List, 
    proposal: str,
    original_experimental_detail: str
) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–ä¿®è¨‚å¯¦é©—ç´°ç¯€
    
    Args:
        question: ç”¨æˆ¶åé¥‹/å•é¡Œ
        new_chunks: æ–°æª¢ç´¢çš„æ–‡æª”å¡Šï¼ˆä¿®æ”¹å¯¦é©—ç´°ç¯€æ™‚ç‚ºç©ºï¼‰
        old_chunks: åŸå§‹æ–‡æª”å¡Š
        proposal: åŸå§‹ææ¡ˆ
        original_experimental_detail: åŸå§‹å¯¦é©—ç´°ç¯€
    
    Returns:
        Dict[str, Any]: ç¬¦åˆREVISION_EXPERIMENTAL_DETAIL_SCHEMAçš„çµæ§‹åŒ–ä¿®è¨‚å¯¦é©—ç´°ç¯€
    """
    logger.info(f"èª¿ç”¨çµæ§‹åŒ–ä¿®è¨‚å¯¦é©—ç´°ç¯€LLMï¼Œç”¨æˆ¶åé¥‹é•·åº¦ï¼š{len(question)}")
    logger.info(f"åŸæ–‡æª”å¡Šæ•¸é‡ï¼š{len(old_chunks)}")
    logger.info(f"åŸå§‹ææ¡ˆé•·åº¦ï¼š{len(proposal)} å­—ç¬¦")
    logger.info(f"åŸå§‹å¯¦é©—ç´°ç¯€é•·åº¦ï¼š{len(original_experimental_detail)} å­—ç¬¦")
    
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        logger.info(f"ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        logger.debug(f"æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        raise LLMError(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{str(e)}")
    
    try:
        # åªæ”¯æ´ GPT-5 ç³»åˆ—ä½¿ç”¨ Responses API
        if not current_model.startswith('gpt-5'):
            raise LLMError(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
        
        from backend.core.schema_manager import create_revision_experimental_detail_schema
        
        # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
        current_schema = create_revision_experimental_detail_schema()
        
        # æ·»åŠ èª¿è©¦æ—¥èªŒ
        logger.info(f"ğŸ” [DEBUG] ç²å–åˆ°çš„ schema: {current_schema is not None}")
        if current_schema:
            logger.info(f"ğŸ” [DEBUG] Schema é¡å‹: {current_schema.get('type', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema æ¨™é¡Œ: {current_schema.get('title', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema å¿…éœ€å­—æ®µ: {current_schema.get('required', [])}")
        else:
            logger.warning("âš ï¸ [DEBUG] Schema ç‚ºç©ºï¼Œå°‡å›é€€åˆ°å‚³çµ±æ–‡æœ¬ç”Ÿæˆ")
        
        # æ§‹å»ºæç¤ºè©
        system_prompt = """
        You are an experienced materials experiment design consultant. Please help modify parts of the experimental details based on user feedback, original proposal, original experimental details, and literature content.

        Your task is to generate modified experimental details based on user feedback, original proposal, original experimental details, and literature content. The experimental details should be scientifically rigorous, feasible, and address the user's specific modification requests.

        IMPORTANT: You must respond in valid JSON format only. Do not include any text before or after the JSON object.

        The JSON must have the following structure:
        {
            "revision_explanation": "Brief explanation of revision logic and key improvements based on user feedback",
            "synthesis_process": "Detailed synthesis steps, conditions, durations, etc. with modifications",
            "materials_and_conditions": "Materials used, concentrations, temperatures, pressures, and other reaction conditions with modifications",
            "analytical_methods": "Characterization techniques such as XRD, SEM, NMR, etc. with modifications",
            "precautions": "Experimental notes and safety precautions with modifications"
        }

        Key requirements:
        1. Prioritize the areas that the user wants to modify and look for possible improvement directions from the literature
        2. Except for the areas that the user is dissatisfied with, other parts should maintain the original experimental detail content without changes
        3. Maintain scientific rigor, clarity, and avoid vague descriptions
        4. Use only the provided literature labels ([1], [2], etc.) for citations, and do not fabricate sources
        5. Ensure every claim is supported by a cited source or reasonable extension from the literature
        6. The revision_explanation should briefly explain the logic of changes and key improvements based on user feedback
        7. Focus on the specific experimental step or section that the user wants to modify
        """
        
        # æ§‹å»ºæ–‡æª”å…§å®¹ï¼ˆåªä½¿ç”¨åŸå§‹chunksï¼‰
        old_text = ""
        for i, doc in enumerate(old_chunks):
            # è™•ç†å¯èƒ½æ˜¯å­—å…¸æ ¼å¼çš„ chunks
            if hasattr(doc, 'metadata'):
                metadata = doc.metadata
                page_content = doc.page_content
            else:
                metadata = doc.get('metadata', {})
                page_content = doc.get('page_content', '')
            
            title = metadata.get("title", "Untitled")
            filename = metadata.get("filename") or metadata.get("source", "Unknown")
            page = metadata.get("page_number") or metadata.get("page", "?")
            
            # é¡¯ç¤ºå®Œæ•´çš„æ–‡æª”å…§å®¹ï¼Œè€Œä¸æ˜¯åªæœ‰å‰80å€‹å­—ç¬¦
            old_text += f"    [{i+1}] {title} | Page {page}\n{page_content}\n\n"
        
        user_prompt = f"""
        --- User Feedback ---
        {question}

        --- Original Proposal Content ---
        {proposal}

        --- Original Experimental Details ---
        {original_experimental_detail}

        --- Literature Excerpts Based on Original Proposal ---
        {old_text}
        """
        
        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        return call_structured_llm(full_prompt, current_schema)
        
    except Exception as e:
        logger.error(f"çµæ§‹åŒ–ä¿®è¨‚å¯¦é©—ç´°ç¯€LLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        return {}

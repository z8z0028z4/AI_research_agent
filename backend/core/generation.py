"""
ç”Ÿæˆæ¨¡çµ„
========

è² è²¬ LLM èª¿ç”¨å’Œå…§å®¹ç”Ÿæˆ
"""

import time
import json
from typing import Dict, Any, Optional, List
from openai import OpenAI

from backend.utils.logger import get_logger
from backend.utils.exceptions import LLMError, APIRequestError
from backend.services.model_service import get_model_params, get_current_model

logger = get_logger(__name__)


def call_llm(prompt: str, **kwargs) -> str:
    """
    èª¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬
    
    åƒæ•¸ï¼š
        prompt: æç¤ºè©
        **kwargs: é¡å¤–åƒæ•¸
        
    è¿”å›ï¼š
        str: ç”Ÿæˆçš„æ–‡æœ¬
    """
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        
        logger.info(f"èª¿ç”¨ LLMï¼Œæ¨¡å‹ï¼š{current_model}")
        logger.debug(f"æç¤ºè©é•·åº¦ï¼š{len(prompt)} å­—ç¬¦")
        
        # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ä¸åŒçš„èª¿ç”¨æ–¹å¼
        if current_model.startswith('gpt-5'):
            return _call_gpt5_responses_api(prompt, llm_params, **kwargs)
        else:
            return _call_gpt4_chat_api(prompt, llm_params, **kwargs)
            
    except Exception as e:
        logger.error(f"LLM èª¿ç”¨å¤±æ•—ï¼š{e}")
        raise LLMError(f"LLM èª¿ç”¨å¤±æ•—ï¼š{str(e)}")


def _call_gpt5_responses_api(prompt: str, llm_params: Dict[str, Any], **kwargs) -> str:
    """
    èª¿ç”¨ GPT-5 Responses API
    
    åƒæ•¸ï¼š
        prompt: æç¤ºè©
        llm_params: æ¨¡å‹åƒæ•¸
        **kwargs: é¡å¤–åƒæ•¸
        
    è¿”å›ï¼š
        str: ç”Ÿæˆçš„æ–‡æœ¬
    """
    try:
        client = OpenAI()
        
        # æ·»åŠ  SSL é©—è­‰ç¦ç”¨é¸é …ï¼Œè§£æ±ºä¼æ¥­ç¶²è·¯ç’°å¢ƒçš„è­‰æ›¸å•é¡Œ
        import httpx
        import os
        
        # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸ï¼Œå…è¨±ç”¨æˆ¶æ§åˆ¶ SSL é©—è­‰
        disable_ssl_verify = os.getenv('DISABLE_SSL_VERIFY', 'false').lower() == 'true'
        if disable_ssl_verify:
            client._client = httpx.Client(verify=False)
            logger.warning("âš ï¸ SSL é©—è­‰å·²ç¦ç”¨ï¼ˆç’°å¢ƒè®Šæ•¸æ§åˆ¶ï¼‰")
        else:
            # å˜—è©¦ä½¿ç”¨é è¨­è¨­ç½®ï¼Œå¦‚æœå¤±æ•—å‰‡è‡ªå‹•ç¦ç”¨
            try:
                # æ¸¬è©¦é€£æ¥
                test_client = httpx.Client()
                test_client.close()
            except Exception as e:
                if "certificate verify failed" in str(e).lower():
                    client._client = httpx.Client(verify=False)
                    logger.warning("âš ï¸ æª¢æ¸¬åˆ° SSL è­‰æ›¸å•é¡Œï¼Œè‡ªå‹•ç¦ç”¨ SSL é©—è­‰")
                else:
                    raise e
        
        # æ§‹å»º Responses API åƒæ•¸ - ä½¿ç”¨ input è€Œä¸æ˜¯ prompt
        responses_params = {
            "model": llm_params.get("model", "gpt-5"),
            "input": [{"role": "user", "content": prompt}],  # ä¿®æ­£ï¼šä½¿ç”¨ input è€Œä¸æ˜¯ prompt
            "max_output_tokens": llm_params.get("max_output_tokens", 2000),
            "timeout": llm_params.get("timeout", 60)
        }
        
        # æ·»åŠ å¯é¸åƒæ•¸
        if "reasoning_effort" in llm_params:
            responses_params["reasoning"] = {"effort": llm_params["reasoning_effort"]}
        if "verbosity" in llm_params:
            responses_params["text"] = {"verbosity": llm_params["verbosity"]}
        if "temperature" in llm_params:
            responses_params["temperature"] = llm_params["temperature"]
        
        logger.debug(f"ä½¿ç”¨ Responses APIï¼Œåƒæ•¸ï¼š{responses_params}")
        
        # é‡è©¦æ©Ÿåˆ¶
        max_retries = 3
        base_tokens = llm_params.get("max_output_tokens", 2000)
        
        for retry_count in range(max_retries):
            # æ¯æ¬¡é‡è©¦æ™‚å¢åŠ  1000 tokens
            current_tokens = base_tokens + (retry_count * 1000)
            responses_params["max_output_tokens"] = current_tokens
            
            logger.info(f"å˜—è©¦ {retry_count + 1}/{max_retries}ï¼Œä½¿ç”¨ {current_tokens} tokens")
            
            try:
                response = client.responses.create(**responses_params)
                
                # æª¢æŸ¥éŸ¿æ‡‰ç‹€æ…‹
                if hasattr(response, 'status') and response.status == 'incomplete':
                    logger.warning(f"æª¢æ¸¬åˆ° incomplete ç‹€æ…‹ï¼Œå˜—è©¦æå–éƒ¨åˆ†å…§å®¹")
                    
                    # å°æ–¼éçµæ§‹åŒ–è¼¸å‡ºï¼Œå˜—è©¦æå–éƒ¨åˆ†æ–‡æœ¬
                    if hasattr(response, 'output_text') and response.output_text:
                        output = response.output_text
                        logger.info(f"å¾ incomplete éŸ¿æ‡‰ä¸­æå–éƒ¨åˆ†æ–‡æœ¬: {len(output)} å­—ç¬¦")
                        return output
                    
                    # å¦‚æœç„¡æ³•æå–éƒ¨åˆ†å…§å®¹ï¼Œå‰‡é‡è©¦
                    if retry_count < max_retries - 1:
                        logger.warning(f"ç„¡æ³•æå–éƒ¨åˆ†å…§å®¹ï¼Œé‡è©¦ {retry_count + 1}/{max_retries}")
                        time.sleep(2)
                        continue
                
                # æå–æ–‡æœ¬å…§å®¹
                if hasattr(response, 'output_text') and response.output_text:
                    output = response.output_text
                    logger.info(f"æˆåŠŸæå–æ–‡æœ¬: {len(output)} å­—ç¬¦")
                    return output
                elif hasattr(response, 'output') and response.output:
                    # å¾ output é™£åˆ—ä¸­æå–æ–‡æœ¬
                    output_parts = []
                    for item in response.output:
                        if hasattr(item, 'message') and hasattr(item.message, 'content'):
                            for content in item.message.content:
                                if hasattr(content, 'text') and content.text:
                                    output_parts.append(content.text)
                    output = "".join(output_parts)
                    if output:
                        logger.info(f"æˆåŠŸæå–æ–‡æœ¬: {len(output)} å­—ç¬¦")
                        return output
                
                # å¦‚æœéƒ½å¤±æ•—äº†ï¼Œå˜—è©¦ä½¿ç”¨ content
                if hasattr(response, 'content'):
                    output = response.content
                    logger.info(f"ä½¿ç”¨ content æå–æ–‡æœ¬: {len(output)} å­—ç¬¦")
                    return output
                    
            except Exception as e:
                logger.error(f"API èª¿ç”¨å¤±æ•— (å˜—è©¦ {retry_count + 1}/{max_retries}): {e}")
                if retry_count < max_retries - 1:
                    time.sleep(2)
                    continue
                raise
        
        raise APIRequestError("æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—")
        
    except Exception as e:
        logger.error(f"GPT-5 Responses API èª¿ç”¨å¤±æ•—: {e}")
        raise


def _call_gpt4_chat_api(prompt: str, llm_params: Dict[str, Any], **kwargs) -> str:
    """
    èª¿ç”¨ GPT-4 Chat Completions API
    
    åƒæ•¸ï¼š
        prompt: æç¤ºè©
        llm_params: æ¨¡å‹åƒæ•¸
        **kwargs: é¡å¤–åƒæ•¸
        
    è¿”å›ï¼š
        str: ç”Ÿæˆçš„æ–‡æœ¬
    """
    try:
        client = OpenAI()
        
        # æ§‹å»º Chat Completions API åƒæ•¸
        chat_params = {
            "model": llm_params.get("model", "gpt-4"),
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": llm_params.get("max_tokens", 2000),
            "temperature": llm_params.get("temperature", 0.7)
        }
        
        logger.debug(f"ä½¿ç”¨ Chat Completions APIï¼Œåƒæ•¸ï¼š{chat_params}")
        
        response = client.chat.completions.create(**chat_params)
        
        if response.choices and response.choices[0].message:
            output = response.choices[0].message.content
            logger.info(f"Chat API èª¿ç”¨æˆåŠŸï¼Œå›æ‡‰é•·åº¦ï¼š{len(output)} å­—ç¬¦")
            return output
        else:
            raise APIRequestError("Chat API è¿”å›ç©ºéŸ¿æ‡‰")
            
    except Exception as e:
        logger.error(f"Chat Completions API èª¿ç”¨å¤±æ•—ï¼š{e}")
        raise


def call_structured_llm(prompt: str, schema: Dict[str, Any], **kwargs) -> Dict[str, Any]:
    """
    èª¿ç”¨çµæ§‹åŒ– LLM ç”Ÿæˆ JSON æ ¼å¼å…§å®¹
    
    åƒæ•¸ï¼š
        prompt: æç¤ºè©
        schema: JSON Schema
        **kwargs: é¡å¤–åƒæ•¸
        
    è¿”å›ï¼š
        Dict[str, Any]: çµæ§‹åŒ–æ•¸æ“š
    """
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        
        # ğŸ” [DEBUG] åƒæ•¸è¿½è¹¤ï¼šæª¢æŸ¥ call_structured_llm ä¸­çš„åƒæ•¸
        logger.info(f"ğŸ” [DEBUG] call_structured_llm åƒæ•¸è¿½è¹¤:")
        logger.info(f"ğŸ” [DEBUG] - current_model: {current_model}")
        logger.info(f"ğŸ” [DEBUG] - llm_params é¡å‹: {type(llm_params)}")
        logger.info(f"ğŸ” [DEBUG] - llm_params å…§å®¹: {llm_params}")
        logger.info(f"ğŸ” [DEBUG] - llm_params.get('reasoning'): {llm_params.get('reasoning')}")
        logger.info(f"ğŸ” [DEBUG] - llm_params.get('reasoning_effort'): {llm_params.get('reasoning_effort')}")
        logger.info(f"ğŸ” [DEBUG] - llm_params.get('text'): {llm_params.get('text')}")
        logger.info(f"ğŸ” [DEBUG] - llm_params.get('verbosity'): {llm_params.get('verbosity')}")
        
        logger.info(f"èª¿ç”¨çµæ§‹åŒ– LLMï¼Œæ¨¡å‹ï¼š{current_model}")
        
        # åªæ”¯æ´ GPT-5 ç³»åˆ—
        if not current_model.startswith('gpt-5'):
            raise LLMError(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
        
        return _call_gpt5_structured_api(prompt, schema, llm_params, **kwargs)
            
    except Exception as e:
        logger.error(f"çµæ§‹åŒ– LLM èª¿ç”¨å¤±æ•—ï¼š{e}")
        raise LLMError(f"çµæ§‹åŒ– LLM èª¿ç”¨å¤±æ•—ï¼š{str(e)}")


def _call_gpt5_structured_api(prompt: str, schema: Dict[str, Any], llm_params: Dict[str, Any], **kwargs) -> Dict[str, Any]:
    """
    èª¿ç”¨ GPT-5 çµæ§‹åŒ– API
    
    åƒæ•¸ï¼š
        prompt: æç¤ºè©
        schema: JSON Schema
        llm_params: æ¨¡å‹åƒæ•¸
        **kwargs: é¡å¤–åƒæ•¸
        
    è¿”å›ï¼š
        Dict[str, Any]: çµæ§‹åŒ–æ•¸æ“š
    """
    try:
        client = OpenAI()
        
        # æ·»åŠ  SSL é©—è­‰ç¦ç”¨é¸é …ï¼Œè§£æ±ºä¼æ¥­ç¶²è·¯ç’°å¢ƒçš„è­‰æ›¸å•é¡Œ
        import httpx
        import os
        
        # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸ï¼Œå…è¨±ç”¨æˆ¶æ§åˆ¶ SSL é©—è­‰
        disable_ssl_verify = os.getenv('DISABLE_SSL_VERIFY', 'false').lower() == 'true'
        if disable_ssl_verify:
            client._client = httpx.Client(verify=False)
            logger.warning("âš ï¸ SSL é©—è­‰å·²ç¦ç”¨ï¼ˆç’°å¢ƒè®Šæ•¸æ§åˆ¶ï¼‰")
        else:
            # å˜—è©¦ä½¿ç”¨é è¨­è¨­ç½®ï¼Œå¦‚æœå¤±æ•—å‰‡è‡ªå‹•ç¦ç”¨
            try:
                # æ¸¬è©¦é€£æ¥
                test_client = httpx.Client()
                test_client.close()
            except Exception as e:
                if "certificate verify failed" in str(e).lower():
                    client._client = httpx.Client(verify=False)
                    logger.warning("âš ï¸ æª¢æ¸¬åˆ° SSL è­‰æ›¸å•é¡Œï¼Œè‡ªå‹•ç¦ç”¨ SSL é©—è­‰")
                else:
                    raise e
        
        # ğŸ” [DEBUG] åƒæ•¸è¿½è¹¤ï¼šæª¢æŸ¥è¼¸å…¥çš„ llm_params
        logger.info(f"ğŸ” [DEBUG] _call_gpt5_structured_api è¼¸å…¥åƒæ•¸:")
        logger.info(f"ğŸ” [DEBUG] - llm_params é¡å‹: {type(llm_params)}")
        logger.info(f"ğŸ” [DEBUG] - llm_params å…§å®¹: {llm_params}")
        logger.info(f"ğŸ” [DEBUG] - llm_params.get('reasoning'): {llm_params.get('reasoning')}")
        logger.info(f"ğŸ” [DEBUG] - llm_params.get('reasoning_effort'): {llm_params.get('reasoning_effort')}")
        logger.info(f"ğŸ” [DEBUG] - llm_params.get('text'): {llm_params.get('text')}")
        logger.info(f"ğŸ” [DEBUG] - llm_params.get('verbosity'): {llm_params.get('verbosity')}")
        
        # æ§‹å»º Responses API åƒæ•¸
        responses_params = {
            "model": llm_params.get("model", "gpt-5"),
            "input": [{"role": "user", "content": prompt}],
            "text": {
                "format": {
                    "type": "json_schema",
                    "name": "ResearchProposal",
                    "strict": True,
                    "schema": schema,
                },
                "verbosity": llm_params.get("verbosity", "low")
            },
            "max_output_tokens": llm_params.get("max_output_tokens", 2000),
            "timeout": llm_params.get("timeout", 60)
        }
        
        # ğŸ” [DEBUG] åƒæ•¸è¿½è¹¤ï¼šæª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰é©é…éçš„åƒæ•¸
        if 'reasoning' in llm_params:
            logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨é©é…å¾Œçš„ reasoning åƒæ•¸: {llm_params['reasoning']}")
            responses_params['reasoning'] = llm_params['reasoning']
        else:
            logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨é»˜èª reasoning åƒæ•¸")
            responses_params['reasoning'] = {"effort": llm_params.get("reasoning_effort", "medium")}
        
        if 'text' in llm_params:
            logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨é©é…å¾Œçš„ text åƒæ•¸: {llm_params['text']}")
            # ä¿ç•™ JSON Schema æ ¼å¼ä¿¡æ¯ï¼Œåªæ›´æ–° verbosity
            if 'verbosity' in llm_params['text']:
                responses_params['text']['verbosity'] = llm_params['text']['verbosity']
            logger.info(f"ğŸ” [DEBUG] æ›´æ–°å¾Œçš„ text åƒæ•¸: {responses_params['text']}")
        else:
            logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨é»˜èª text åƒæ•¸")
            responses_params['text']['verbosity'] = llm_params.get("verbosity", "low")
        
        # ğŸ” [DEBUG] åƒæ•¸è¿½è¹¤ï¼šæª¢æŸ¥æ§‹å»ºçš„ responses_params
        logger.info(f"ğŸ” [DEBUG] æ§‹å»ºçš„ responses_params:")
        logger.info(f"ğŸ” [DEBUG] - responses_params é¡å‹: {type(responses_params)}")
        logger.info(f"ğŸ” [DEBUG] - responses_params å…§å®¹: {responses_params}")
        logger.info(f"ğŸ” [DEBUG] - responses_params['reasoning']: {responses_params['reasoning']}")
        logger.info(f"ğŸ” [DEBUG] - responses_params['text']: {responses_params['text']}")
        
        # ğŸ” [DEBUG] æª¢æŸ¥ JSON Schema æ ¼å¼ä¿¡æ¯
        if 'text' in responses_params and 'format' in responses_params['text']:
            logger.info(f"ğŸ” [DEBUG] JSON Schema æ ¼å¼ä¿¡æ¯å­˜åœ¨:")
            logger.info(f"ğŸ” [DEBUG] - format.type: {responses_params['text']['format'].get('type', 'missing')}")
            logger.info(f"ğŸ” [DEBUG] - format.name: {responses_params['text']['format'].get('name', 'missing')}")
            logger.info(f"ğŸ” [DEBUG] - format.strict: {responses_params['text']['format'].get('strict', 'missing')}")
            logger.info(f"ğŸ” [DEBUG] - schema å­˜åœ¨: {'schema' in responses_params['text']['format']}")
        else:
            logger.warning(f"âš ï¸ [DEBUG] JSON Schema æ ¼å¼ä¿¡æ¯ç¼ºå¤±!")
            logger.warning(f"âš ï¸ [DEBUG] text éµå­˜åœ¨: {'text' in responses_params}")
            if 'text' in responses_params:
                logger.warning(f"âš ï¸ [DEBUG] text å…§å®¹: {responses_params['text']}")
                logger.warning(f"âš ï¸ [DEBUG] format éµå­˜åœ¨: {'format' in responses_params['text']}")
        
        # ğŸ” [DEBUG] åƒæ•¸è¿½è¹¤ï¼šåƒæ•¸å·²ç¶“åœ¨æ§‹å»ºæ™‚æ­£ç¢ºè™•ç†
        
        logger.debug(f"ä½¿ç”¨ Responses API with JSON Schemaï¼Œåƒæ•¸ï¼š{responses_params}")
        
        # é‡è©¦æ©Ÿåˆ¶
        max_retries = 3
        base_tokens = llm_params.get("max_output_tokens", 2000)
        
        for retry_count in range(max_retries):
            # æ¯æ¬¡é‡è©¦æ™‚å¢åŠ  1000 tokens
            current_tokens = base_tokens + (retry_count * 1000)
            responses_params["max_output_tokens"] = current_tokens
            
            logger.info(f"å˜—è©¦ {retry_count + 1}/{max_retries}ï¼Œä½¿ç”¨ {current_tokens} tokens")
            
            try:
                response = client.responses.create(**responses_params)
                
                # æª¢æŸ¥éŸ¿æ‡‰ç‹€æ…‹
                if hasattr(response, 'status') and response.status == 'incomplete':
                    logger.warning(f"æª¢æ¸¬åˆ° incomplete ç‹€æ…‹ï¼Œå˜—è©¦æå–éƒ¨åˆ†å…§å®¹")
                    
                    # å˜—è©¦å¾ incomplete éŸ¿æ‡‰ä¸­æå–éƒ¨åˆ† JSON
                    partial_json = _extract_partial_json_from_response(response)
                    if partial_json:
                        logger.info("æˆåŠŸå¾ incomplete éŸ¿æ‡‰ä¸­æå–éƒ¨åˆ† JSON")
                        return partial_json
                    
                    # å¦‚æœç„¡æ³•æå–éƒ¨åˆ†å…§å®¹ï¼Œå‰‡é‡è©¦
                    if retry_count < max_retries - 1:
                        logger.warning(f"ç„¡æ³•æå–éƒ¨åˆ†å…§å®¹ï¼Œé‡è©¦ {retry_count + 1}/{max_retries}")
                        time.sleep(2)
                        continue
                
                # æå– JSON å…§å®¹
                if hasattr(response, 'output_text') and response.output_text:
                    try:
                        result = json.loads(response.output_text)
                        logger.info("æˆåŠŸè§£æ JSON çµæ§‹åŒ–ææ¡ˆ")
                        return result
                    except json.JSONDecodeError as e:
                        logger.error(f"JSON è§£æå¤±æ•—: {e}")
                        logger.debug(f"å˜—è©¦çš„æ–‡æœ¬: {response.output_text[:200]}...")
                
                # å¦‚æœ output_text å¤±æ•—ï¼Œå˜—è©¦å¾ output æå–
                if hasattr(response, 'output') and response.output:
                    text_content = ""
                    for item in response.output:
                        if hasattr(item, 'message') and hasattr(item.message, 'content'):
                            for content in item.message.content:
                                if hasattr(content, 'text') and content.text:
                                    text_content += content.text
                    
                    if text_content:
                        try:
                            result = json.loads(text_content)
                            logger.info("æˆåŠŸè§£æ JSON çµæ§‹åŒ–ææ¡ˆ")
                            return result
                        except json.JSONDecodeError as e:
                            logger.error(f"JSON è§£æå¤±æ•—: {e}")
                            logger.debug(f"å˜—è©¦çš„æ–‡æœ¬: {text_content[:200]}...")
                
                logger.warning("ç„¡æ³•å¾ Responses API æå– JSON å…§å®¹")
                
            except Exception as e:
                logger.error(f"API èª¿ç”¨å¤±æ•— (å˜—è©¦ {retry_count + 1}/{max_retries}): {e}")
                if retry_count < max_retries - 1:
                    time.sleep(2)
                    continue
                raise
        
        raise APIRequestError("æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—")
        
    except Exception as e:
        logger.error(f"GPT-5 çµæ§‹åŒ– API èª¿ç”¨å¤±æ•—: {e}")
        raise


# _call_gpt4_structured_api å‡½æ•¸å·²ç§»é™¤ - ä¸å†æ”¯æ´ GPT-4 ç³»åˆ—


# generate_proposal_with_fallback å‡½æ•¸å·²ç§»é™¤ - ä¸å†æ”¯æ´éçµæ§‹åŒ–è¼¸å‡º fallback


def _extract_partial_json_from_response(response) -> Optional[Dict[str, Any]]:
    """
    å¾ incomplete éŸ¿æ‡‰ä¸­æå–éƒ¨åˆ† JSON å…§å®¹
    
    Args:
        response: OpenAI Responses API éŸ¿æ‡‰å°è±¡
        
    Returns:
        Optional[Dict[str, Any]]: æå–çš„ JSON å°è±¡ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› None
    """
    try:
        # å˜—è©¦å¾ output_text æå–
        if hasattr(response, 'output_text') and response.output_text:
            text = response.output_text
            logger.debug(f"å˜—è©¦å¾ output_text æå–éƒ¨åˆ† JSON: {text[:200]}...")
            
            # å˜—è©¦æ‰¾åˆ°æœ€å¾Œä¸€å€‹å®Œæ•´çš„ JSON å°è±¡
            brace_count = 0
            last_complete_pos = -1
            
            for i, char in enumerate(text):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        last_complete_pos = i
            
            if last_complete_pos > 0:
                complete_json = text[:last_complete_pos + 1]
                try:
                    result = json.loads(complete_json)
                    logger.info(f"æˆåŠŸä¿®å¾©ä¸å®Œæ•´çš„ JSONï¼Œé•·åº¦: {len(complete_json)} å­—ç¬¦")
                    return result
                except json.JSONDecodeError as e:
                    logger.debug(f"JSON ä¿®å¾©å¤±æ•—: {e}")
        
        # å˜—è©¦å¾ output é™£åˆ—æå–
        if hasattr(response, 'output') and response.output:
            text_content = ""
            for item in response.output:
                if hasattr(item, 'message') and hasattr(item.message, 'content'):
                    for content in item.message.content:
                        if hasattr(content, 'text') and content.text:
                            text_content += content.text
            
            if text_content:
                logger.debug(f"å˜—è©¦å¾ output é™£åˆ—æå–éƒ¨åˆ† JSON: {text_content[:200]}...")
                
                # åŒæ¨£å˜—è©¦ä¿®å¾©ä¸å®Œæ•´çš„ JSON
                brace_count = 0
                last_complete_pos = -1
                
                for i, char in enumerate(text_content):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            last_complete_pos = i
                
                if last_complete_pos > 0:
                    complete_json = text_content[:last_complete_pos + 1]
                    try:
                        result = json.loads(complete_json)
                        logger.info(f"æˆåŠŸå¾ output é™£åˆ—ä¿®å¾©ä¸å®Œæ•´çš„ JSONï¼Œé•·åº¦: {len(complete_json)} å­—ç¬¦")
                        return result
                    except json.JSONDecodeError as e:
                        logger.debug(f"output é™£åˆ— JSON ä¿®å¾©å¤±æ•—: {e}")
        
        logger.warning("ç„¡æ³•å¾ incomplete éŸ¿æ‡‰ä¸­æå–æœ‰æ•ˆçš„ JSON å…§å®¹")
        return None
        
    except Exception as e:
        logger.error(f"æå–éƒ¨åˆ† JSON æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


def call_llm_structured_proposal(system_prompt: str, user_prompt: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–ç ”ç©¶ææ¡ˆ
    
    Args:
        system_prompt: ç³»çµ±æç¤ºè©
        user_prompt: ç”¨æˆ¶æç¤ºè©ï¼ˆåŒ…å«æ–‡ç»æ‘˜è¦å’Œç ”ç©¶ç›®æ¨™ï¼‰
    
    Returns:
        Dict[str, Any]: ç¬¦åˆRESEARCH_PROPOSAL_SCHEMAçš„çµæ§‹åŒ–ææ¡ˆ
    """
    logger.info(f"èª¿ç”¨çµæ§‹åŒ–LLMï¼Œç³»çµ±æç¤ºè©é•·åº¦ï¼š{len(system_prompt)} å­—ç¬¦")
    logger.info(f"ç”¨æˆ¶æç¤ºè©é•·åº¦ï¼š{len(user_prompt)} å­—ç¬¦")
    
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        logger.info(f"ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        logger.debug(f"æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        raise LLMError(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{str(e)}")
    
    try:
        # åªæ”¯æ´ GPT-5 ç³»åˆ—ä½¿ç”¨ Responses API
        if not current_model.startswith('gpt-5'):
            raise LLMError(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
        
        from backend.core.schema_manager import create_research_proposal_schema
        
        # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
        current_schema = create_research_proposal_schema()
        
        # æ·»åŠ èª¿è©¦æ—¥èªŒ
        logger.info(f"ğŸ” [DEBUG] ç²å–åˆ°çš„ schema: {current_schema is not None}")
        if current_schema:
            logger.info(f"ğŸ” [DEBUG] Schema é¡å‹: {current_schema.get('type', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema æ¨™é¡Œ: {current_schema.get('title', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema å¿…éœ€å­—æ®µ: {current_schema.get('required', [])}")
        else:
            logger.warning("âš ï¸ [DEBUG] Schema ç‚ºç©ºï¼Œå°‡å›é€€åˆ°å‚³çµ±æ–‡æœ¬ç”Ÿæˆ")
        
        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        return call_structured_llm(full_prompt, current_schema)
        
    except Exception as e:
        logger.error(f"çµæ§‹åŒ–LLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        raise LLMError(f"çµæ§‹åŒ–LLMèª¿ç”¨å¤±æ•—ï¼š{str(e)}")


def call_llm_structured_experimental_detail(chunks: List, proposal: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–å¯¦é©—ç´°ç¯€
    
    Args:
        chunks: æ–‡ç»ç‰‡æ®µ
        proposal: ç ”ç©¶ææ¡ˆ
    
    Returns:
        Dict[str, Any]: ç¬¦åˆEXPERIMENTAL_DETAIL_SCHEMAçš„çµæ§‹åŒ–å¯¦é©—ç´°ç¯€
    """
    logger.info(f"èª¿ç”¨çµæ§‹åŒ–å¯¦é©—ç´°ç¯€LLMï¼Œæ–‡ç»ç‰‡æ®µæ•¸é‡ï¼š{len(chunks)}")
    logger.info(f"ææ¡ˆé•·åº¦ï¼š{len(proposal)} å­—ç¬¦")
    
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        logger.info(f"ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        logger.debug(f"æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        raise LLMError(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{str(e)}")
    
    try:
        # åªæ”¯æ´ GPT-5 ç³»åˆ—ä½¿ç”¨ Responses API
        if not current_model.startswith('gpt-5'):
            raise LLMError(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
        
        from backend.core.schema_manager import create_experimental_detail_schema
        
        # æ§‹å»ºç³»çµ±æç¤ºè©
        system_prompt = f"""
        You are a professional materials synthesis consultant, skilled at generating detailed experimental procedures based on literature and research proposals.

        Based on the following research proposal and literature information, please generate detailed experimental details:

        --- Research Proposal ---
        {proposal}

        Please generate detailed experimental details including the following four sections:
        1. Synthesis Process: Detailed synthesis steps, conditions, durations, etc.
        2. Materials and Conditions: Materials used, concentrations, temperatures, pressures, and other reaction conditions
        3. Analytical Methods: Characterization techniques such as XRD, SEM, NMR, etc.
        4. Precautions: Experimental notes and safety precautions
        """
        
        # æ§‹å»ºç”¨æˆ¶æç¤ºè©ï¼ˆåŒ…å«æ–‡ç»æ‘˜è¦ï¼‰
        context_text = ""
        citations = []
        for i, doc in enumerate(chunks):
            meta = doc.metadata
            title = meta.get("title", "Untitled")
            filename = meta.get("filename") or meta.get("source", "Unknown")
            page = meta.get("page_number") or meta.get("page", "?")
            
            context_text += f"[{i+1}] {title} | Page {page}\n{doc.page_content}\n\n"
            citations.append({
                "label": f"[{i+1}]",
                "title": title,
                "source": filename,
                "page": page
            })
        
        user_prompt = f"""
        Based on the following literature information, generate detailed experimental procedures:

        --- Literature Excerpts ---
        {context_text}
        
        
        """
        
        # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
        current_schema = create_experimental_detail_schema()
        
        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        # èª¿ç”¨çµæ§‹åŒ– LLM
        experimental_data = call_structured_llm(full_prompt, current_schema)
        
        # æ·»åŠ å¼•ç”¨ä¿¡æ¯
        if experimental_data:
            experimental_data['citations'] = citations
        
        return experimental_data
        
    except Exception as e:
        logger.error(f"çµæ§‹åŒ–å¯¦é©—ç´°ç¯€LLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        return {}






def call_llm_structured_revision_proposal(question: str, new_chunks: List, old_chunks: List, proposal: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–ä¿®è¨‚ææ¡ˆ (åŒ…å«ä¿®è¨‚èªªæ˜)
    
    Args:
        question: ç”¨æˆ¶åé¥‹/å•é¡Œ
        new_chunks: æ–°æª¢ç´¢çš„æ–‡æª”å¡Š
        old_chunks: åŸå§‹æ–‡æª”å¡Š
        proposal: åŸå§‹ææ¡ˆ
    
    Returns:
        Dict[str, Any]: ç¬¦åˆREVISION_PROPOSAL_SCHEMAçš„çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆ
    """
    logger.info(f"èª¿ç”¨çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆLLMï¼Œç”¨æˆ¶åé¥‹é•·åº¦ï¼š{len(question)}")
    logger.info(f"æ–°æ–‡æª”å¡Šæ•¸é‡ï¼š{len(new_chunks)}")
    logger.info(f"åŸæ–‡æª”å¡Šæ•¸é‡ï¼š{len(old_chunks)}")
    logger.info(f"åŸå§‹ææ¡ˆé•·åº¦ï¼š{len(proposal)} å­—ç¬¦")
    
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        logger.info(f"ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        logger.debug(f"æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        raise LLMError(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{str(e)}")
    
    try:
        # åªæ”¯æ´ GPT-5 ç³»åˆ—ä½¿ç”¨ Responses API
        if not current_model.startswith('gpt-5'):
            raise LLMError(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
        
        from backend.core.schema_manager import create_revision_proposal_schema
        
        # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
        current_schema = create_revision_proposal_schema()
        
        # æ·»åŠ èª¿è©¦æ—¥èªŒ
        logger.info(f"ğŸ” [DEBUG] ç²å–åˆ°çš„ schema: {current_schema is not None}")
        if current_schema:
            logger.info(f"ğŸ” [DEBUG] Schema é¡å‹: {current_schema.get('type', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema æ¨™é¡Œ: {current_schema.get('title', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema å¿…éœ€å­—æ®µ: {current_schema.get('required', [])}")
        else:
            logger.warning("âš ï¸ [DEBUG] Schema ç‚ºç©ºï¼Œå°‡å›é€€åˆ°å‚³çµ±æ–‡æœ¬ç”Ÿæˆ")
        
        # æ§‹å»ºæç¤ºè©
        system_prompt = """
        You are an experienced materials experiment design consultant. Please help modify parts of the research proposal based on user feedback, original proposal, and literature content.

        Your task is to generate a modified research proposal based on user feedback, original proposal, and literature content. The proposal should be innovative, scientifically rigorous, and feasible.

        IMPORTANT: You must respond in valid JSON format only. Do not include any text before or after the JSON object.

        The JSON must have the following structure:
        {
            "revision_explanation": "Brief explanation of revision logic and key improvements based on user feedback",
            "proposal_title": "Title of the research proposal",
            "need": "Research need and current limitations",
            "solution": "Proposed design and development strategies",
            "differentiation": "Comparison with existing technologies",
            "benefit": "Expected improvements and benefits",
            "experimental_overview": "Experimental approach and methodology",
            "materials_list": ["material1", "material2", "material3"]
        }

        Key requirements:
        1. Prioritize the areas that the user wants to modify and look for possible improvement directions from the literature
        2. Except for the areas that the user is dissatisfied with, other parts should maintain the original proposal content without changes
        3. Maintain scientific rigor, clarity, and avoid vague descriptions
        4. Use only the provided literature labels ([1], [2], etc.) for citations, and do not fabricate sources
        5. Ensure every claim is supported by a cited source or reasonable extension from the literature
        6. For materials_list, include ONLY IUPAC chemical names without any descriptions, notes, or parenthetical explanations. Each item must be a single chemical name only.
        7. The revision_explanation should briefly explain the logic of changes and key improvements based on user feedback
        """
        
        # æ§‹å»ºæ–‡æª”å…§å®¹
        old_text = ""
        for i, doc in enumerate(old_chunks):
            # è™•ç†å¯èƒ½æ˜¯å­—å…¸æ ¼å¼çš„ chunks
            if hasattr(doc, 'metadata'):
                metadata = doc.metadata
                page_content = doc.page_content
            else:
                metadata = doc.get('metadata', {})
                page_content = doc.get('page_content', '')
            
            title = metadata.get("title", "Untitled")
            filename = metadata.get("filename") or metadata.get("source", "Unknown")
            page = metadata.get("page_number") or metadata.get("page", "?")
            snippet = page_content[:80].replace("\n", " ")
            old_text += f"    [{i+1}] {title} | Page {page}\n{snippet}\n\n"
        
        new_text = ""
        for i, doc in enumerate(new_chunks):
            # è™•ç†å¯èƒ½æ˜¯å­—å…¸æ ¼å¼çš„ chunks
            if hasattr(doc, 'metadata'):
                metadata = doc.metadata
                page_content = doc.page_content
            else:
                metadata = doc.get('metadata', {})
                page_content = doc.get('page_content', '')
            
            title = metadata.get("title", "Untitled")
            filename = metadata.get("filename") or metadata.get("source", "Unknown")
            page = metadata.get("page_number") or metadata.get("page", "?")
            snippet = page_content[:80].replace("\n", " ")
            new_text += f"    [{i+1}] {title} | Page {page}\n{snippet}\n\n"
        
        user_prompt = f"""
        --- User Feedback ---
        {question}

        --- Original Proposal Content ---
        {proposal}

        --- Literature Excerpts Based on Original Proposal ---
        {old_text}

        --- New Retrieved Excerpts Based on Feedback ---
        {new_text}
        """
        
        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        return call_structured_llm(full_prompt, current_schema)
        
    except Exception as e:
        logger.error(f"çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆLLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        return {}


def call_llm_structured_revision_experimental_detail(
    question: str, 
    new_chunks: List, 
    old_chunks: List, 
    proposal: str,
    original_experimental_detail: str
) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–ä¿®è¨‚å¯¦é©—ç´°ç¯€
    
    Args:
        question: ç”¨æˆ¶åé¥‹/å•é¡Œ
        new_chunks: æ–°æª¢ç´¢çš„æ–‡æª”å¡Šï¼ˆä¿®æ”¹å¯¦é©—ç´°ç¯€æ™‚ç‚ºç©ºï¼‰
        old_chunks: åŸå§‹æ–‡æª”å¡Š
        proposal: åŸå§‹ææ¡ˆ
        original_experimental_detail: åŸå§‹å¯¦é©—ç´°ç¯€
    
    Returns:
        Dict[str, Any]: ç¬¦åˆREVISION_EXPERIMENTAL_DETAIL_SCHEMAçš„çµæ§‹åŒ–ä¿®è¨‚å¯¦é©—ç´°ç¯€
    """
    logger.info(f"èª¿ç”¨çµæ§‹åŒ–ä¿®è¨‚å¯¦é©—ç´°ç¯€LLMï¼Œç”¨æˆ¶åé¥‹é•·åº¦ï¼š{len(question)}")
    logger.info(f"åŸæ–‡æª”å¡Šæ•¸é‡ï¼š{len(old_chunks)}")
    logger.info(f"åŸå§‹ææ¡ˆé•·åº¦ï¼š{len(proposal)} å­—ç¬¦")
    logger.info(f"åŸå§‹å¯¦é©—ç´°ç¯€é•·åº¦ï¼š{len(original_experimental_detail)} å­—ç¬¦")
    
    try:
        current_model = get_current_model()
        llm_params = get_model_params()
        logger.info(f"ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        logger.debug(f"æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        logger.error(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        raise LLMError(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{str(e)}")
    
    try:
        # åªæ”¯æ´ GPT-5 ç³»åˆ—ä½¿ç”¨ Responses API
        if not current_model.startswith('gpt-5'):
            raise LLMError(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
        
        from backend.core.schema_manager import create_revision_experimental_detail_schema
        
        # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
        current_schema = create_revision_experimental_detail_schema()
        
        # æ·»åŠ èª¿è©¦æ—¥èªŒ
        logger.info(f"ğŸ” [DEBUG] ç²å–åˆ°çš„ schema: {current_schema is not None}")
        if current_schema:
            logger.info(f"ğŸ” [DEBUG] Schema é¡å‹: {current_schema.get('type', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema æ¨™é¡Œ: {current_schema.get('title', 'unknown')}")
            logger.info(f"ğŸ” [DEBUG] Schema å¿…éœ€å­—æ®µ: {current_schema.get('required', [])}")
        else:
            logger.warning("âš ï¸ [DEBUG] Schema ç‚ºç©ºï¼Œå°‡å›é€€åˆ°å‚³çµ±æ–‡æœ¬ç”Ÿæˆ")
        
        # æ§‹å»ºæç¤ºè©
        system_prompt = """
        You are an experienced materials experiment design consultant. Please help modify parts of the experimental details based on user feedback, original proposal, original experimental details, and literature content.

        Your task is to generate modified experimental details based on user feedback, original proposal, original experimental details, and literature content. The experimental details should be scientifically rigorous, feasible, and address the user's specific modification requests.

        IMPORTANT: You must respond in valid JSON format only. Do not include any text before or after the JSON object.

        The JSON must have the following structure:
        {
            "revision_explanation": "Brief explanation of revision logic and key improvements based on user feedback",
            "synthesis_process": "Detailed synthesis steps, conditions, durations, etc. with modifications",
            "materials_and_conditions": "Materials used, concentrations, temperatures, pressures, and other reaction conditions with modifications",
            "analytical_methods": "Characterization techniques such as XRD, SEM, NMR, etc. with modifications",
            "precautions": "Experimental notes and safety precautions with modifications"
        }

        Key requirements:
        1. Prioritize the areas that the user wants to modify and look for possible improvement directions from the literature
        2. Except for the areas that the user is dissatisfied with, other parts should maintain the original experimental detail content without changes
        3. Maintain scientific rigor, clarity, and avoid vague descriptions
        4. Use only the provided literature labels ([1], [2], etc.) for citations, and do not fabricate sources
        5. Ensure every claim is supported by a cited source or reasonable extension from the literature
        6. The revision_explanation should briefly explain the logic of changes and key improvements based on user feedback
        7. Focus on the specific experimental step or section that the user wants to modify
        """
        
        # æ§‹å»ºæ–‡æª”å…§å®¹ï¼ˆåªä½¿ç”¨åŸå§‹chunksï¼‰
        old_text = ""
        for i, doc in enumerate(old_chunks):
            # è™•ç†å¯èƒ½æ˜¯å­—å…¸æ ¼å¼çš„ chunks
            if hasattr(doc, 'metadata'):
                metadata = doc.metadata
                page_content = doc.page_content
            else:
                metadata = doc.get('metadata', {})
                page_content = doc.get('page_content', '')
            
            title = metadata.get("title", "Untitled")
            filename = metadata.get("filename") or metadata.get("source", "Unknown")
            page = metadata.get("page_number") or metadata.get("page", "?")
            
            # é¡¯ç¤ºå®Œæ•´çš„æ–‡æª”å…§å®¹ï¼Œè€Œä¸æ˜¯åªæœ‰å‰80å€‹å­—ç¬¦
            old_text += f"    [{i+1}] {title} | Page {page}\n{page_content}\n\n"
        
        user_prompt = f"""
        --- User Feedback ---
        {question}

        --- Original Proposal Content ---
        {proposal}

        --- Original Experimental Details ---
        {original_experimental_detail}

        --- Literature Excerpts Based on Original Proposal ---
        {old_text}
        """
        
        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        return call_structured_llm(full_prompt, current_schema)
        
    except Exception as e:
        logger.error(f"çµæ§‹åŒ–ä¿®è¨‚å¯¦é©—ç´°ç¯€LLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        return {}

"""
è™•ç†å™¨æ¨¡çµ„
========

åŒ…å«å„ç¨®æ¨¡å¼çš„å…·é«”è™•ç†é‚è¼¯ï¼Œæ¯å€‹è™•ç†å™¨è² è²¬ä¸€ç¨®ç‰¹å®šçš„è™•ç†æ¨¡å¼
"""

import time
import logging
from typing import Dict, List, Any, Optional, Tuple

from .vector_store import load_paper_vectorstore, load_experiment_vectorstore, search_documents
from .llm_manager import get_default_llm_manager
from .schema_manager import get_schema_by_type
from ..utils import extract_text_snippet
from ..utils.exceptions import LLMError, VectorStoreError

# é…ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)


class BaseProcessor:
    """è™•ç†å™¨åŸºé¡"""
    
    def __init__(self):
        self.llm_manager = get_default_llm_manager()
    
    def _build_context(self, chunks: List[Any]) -> str:
        """æ§‹å»ºä¸Šä¸‹æ–‡æ–‡æœ¬"""
        context_parts = []
        
        for i, chunk in enumerate(chunks, 1):
            try:
                metadata = chunk.metadata
                content = chunk.page_content
                
                filename = metadata.get("filename", "Unknown")
                page = metadata.get("page", "")
                
                clean_content = extract_text_snippet(content, max_length=300)
                
                context_part = f"æ–‡æª” {i}: {filename} (é ç¢¼: {page})\n{clean_content}\n"
                context_parts.append(context_part)
                
            except Exception as e:
                logger.warning(f"è™•ç†æ–‡æª”å¡Š {i} å¤±æ•—: {e}")
        
        return "\n".join(context_parts)


class AdvancedInferenceProcessor(BaseProcessor):
    """é«˜ç´šæ¨è«–è™•ç†å™¨ - æ•´åˆæ–‡ç»å’Œå¯¦é©—æ•¸æ“š"""
    
    def process(self, question: str, k: int = 5) -> Dict[str, Any]:
        """è™•ç†é«˜ç´šæ¨è«–æ¨¡å¼"""
        logger.info("ğŸ§ª å•Ÿç”¨æ¨¡å¼ï¼šç´å…¥å¯¦é©—è³‡æ–™ + æ¨è«–")
        
        # è¼‰å…¥é›™é‡å‘é‡åº«
        paper_vectorstore = load_paper_vectorstore()
        experiment_vectorstore = load_experiment_vectorstore()
        
        logger.info(f"ğŸ“¦ Paper å‘é‡åº«ï¼š{paper_vectorstore._collection.count()}")
        logger.info(f"ğŸ“¦ Experiment å‘é‡åº«ï¼š{experiment_vectorstore._collection.count()}")
        
        # æª¢ç´¢æ–‡ç»å’Œå¯¦é©—æ•¸æ“š
        chunks_paper = search_documents(paper_vectorstore, question, k=k)
        experiment_chunks = search_documents(experiment_vectorstore, question, k=k)
        
        # æ§‹å»ºé›™é‡æ¨è«–æç¤ºè©
        prompt = self._build_dual_inference_prompt(question, chunks_paper, experiment_chunks)
        
        # èª¿ç”¨ LLM
        response = self.llm_manager.generate_response(prompt)
        
        return {
            "answer": response,
            "citations": self._extract_citations(chunks_paper + experiment_chunks),
            "chunks": chunks_paper + experiment_chunks,
            "paper_chunks": chunks_paper,
            "experiment_chunks": experiment_chunks
        }
    
    def _build_dual_inference_prompt(self, question: str, paper_chunks: List[Any], experiment_chunks: List[Any]) -> str:
        """æ§‹å»ºé›™é‡æ¨è«–æç¤ºè©"""
        paper_context = self._build_context(paper_chunks)
        experiment_context = self._build_context(experiment_chunks)
        
        return f"""
åŸºæ–¼ä»¥ä¸‹æ–‡ç»å’Œå¯¦é©—æ•¸æ“šï¼Œå°å•é¡Œé€²è¡Œç¶œåˆåˆ†æå’Œæ¨è«–ï¼š

å•é¡Œï¼š{question}

ç›¸é—œæ–‡ç»ï¼š
{paper_context}

ç›¸é—œå¯¦é©—æ•¸æ“šï¼š
{experiment_context}

è«‹çµåˆæ–‡ç»ç†è«–å’Œå¯¦é©—æ•¸æ“šï¼Œæä¾›ï¼š
1. ç¶œåˆåˆ†æ
2. æ¨è«–å’Œå»ºè­°
3. å¯èƒ½çš„æ”¹é€²æ–¹å‘
4. ç›¸é—œçš„å¯¦é©—é©—è­‰å»ºè­°

è«‹ç¢ºä¿å›ç­”æ—¢æœ‰ç†è«–åŸºç¤ï¼Œåˆæœ‰å¯¦é©—æ”¯æ’ã€‚
"""
    
    def _extract_citations(self, chunks: List[Any]) -> List[str]:
        """æå–å¼•ç”¨ä¿¡æ¯"""
        citations = []
        for chunk in chunks:
            try:
                filename = chunk.metadata.get("filename", "")
                page = chunk.metadata.get("page", "")
                if filename:
                    citations.append(f"{filename} (é ç¢¼: {page})")
            except Exception as e:
                logger.warning(f"æå–å¼•ç”¨ä¿¡æ¯å¤±æ•—: {e}")
        
        return list(set(citations))


class ProposalProcessor(BaseProcessor):
    """ç ”ç©¶ææ¡ˆè™•ç†å™¨"""
    
    def process(self, question: str, k: int = 10) -> Dict[str, Any]:
        """è™•ç†ç ”ç©¶ææ¡ˆç”Ÿæˆ"""
        logger.info("ğŸ“ å•Ÿç”¨æ¨¡å¼ï¼šmake proposal (çµæ§‹åŒ–è¼¸å‡º)")
        
        paper_vectorstore = load_paper_vectorstore()
        logger.info(f"ğŸ“¦ Paper å‘é‡åº«ï¼š{paper_vectorstore._collection.count()}")
        
        chunks = search_documents(paper_vectorstore, question, k=k)
        logger.info(f"ğŸ“„ æª¢ç´¢åˆ° {len(chunks)} å€‹æ–‡æª”å¡Š")
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context = self._build_context(chunks)
        
        # ç²å– schema
        schema = get_schema_by_type("research_proposal")
        if not schema:
            raise ValueError("ç„¡æ³•ç²å–ç ”ç©¶ææ¡ˆ schema")
        
        # æ§‹å»ºæç¤º
        prompt = self._build_proposal_prompt(question, context)
        
        # ç”Ÿæˆçµæ§‹åŒ–å›æ‡‰
        structured_data = self.llm_manager.generate_structured_response(
            prompt=prompt,
            schema=schema,
            system_message="ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç ”ç©¶ææ¡ˆç”ŸæˆåŠ©æ‰‹ï¼Œæ“…é•·åŸºæ–¼æ–‡ç»åˆ†æç”Ÿæˆé«˜è³ªé‡çš„ç ”ç©¶ææ¡ˆã€‚"
        )
        
        # é©—è­‰å›æ‡‰
        if not self.llm_manager.validate_response(structured_data, schema):
            raise LLMError("ç”Ÿæˆçš„ææ¡ˆä¸ç¬¦åˆ schema è¦æ±‚")
        
        # è½‰æ›ç‚ºæ–‡æœ¬æ ¼å¼
        text_proposal = self._structured_proposal_to_text(structured_data)
        
        return {
            "answer": text_proposal,
            "citations": structured_data.get('citations', []),
            "chunks": chunks,
            "structured_proposal": structured_data
        }
    
    def _build_proposal_prompt(self, question: str, context: str) -> str:
        """æ§‹å»ºç ”ç©¶ææ¡ˆæç¤º"""
        return f"""
åŸºæ–¼ä»¥ä¸‹ç ”ç©¶ä¸»é¡Œå’Œç›¸é—œæ–‡ç»ï¼Œç”Ÿæˆä¸€å€‹è©³ç´°çš„ç ”ç©¶ææ¡ˆï¼š

ç ”ç©¶ä¸»é¡Œï¼š{question}

ç›¸é—œæ–‡ç»ï¼š
{context}

è«‹ç”Ÿæˆä¸€å€‹åŒ…å«ä»¥ä¸‹è¦ç´ çš„ç ”ç©¶ææ¡ˆï¼š
1. ç ”ç©¶èƒŒæ™¯å’Œéœ€æ±‚åˆ†æ
2. å‰µæ–°è§£æ±ºæ–¹æ¡ˆ
3. èˆ‡ç¾æœ‰ç ”ç©¶çš„å·®ç•°åŒ–
4. é æœŸæ•ˆç›Š
5. å¯¦é©—æ¦‚è¿°
6. æ‰€éœ€ææ–™æ¸…å–®

è«‹ç¢ºä¿ææ¡ˆå…·æœ‰å‰µæ–°æ€§ã€å¯è¡Œæ€§å’Œå¯¦ç”¨åƒ¹å€¼ã€‚
"""
    
    def _structured_proposal_to_text(self, structured_data: Dict[str, Any]) -> str:
        """å°‡çµæ§‹åŒ–ææ¡ˆè½‰æ›ç‚ºæ–‡æœ¬æ ¼å¼"""
        if not structured_data:
            return ""
        
        text_parts = []
        
        # æ¨™é¡Œ
        if structured_data.get('proposal_title'):
            text_parts.append(f"# {structured_data['proposal_title']}\n")
        
        # éœ€æ±‚åˆ†æ
        if structured_data.get('need'):
            text_parts.append(f"## ç ”ç©¶éœ€æ±‚\n{structured_data['need']}\n")
        
        # è§£æ±ºæ–¹æ¡ˆ
        if structured_data.get('solution'):
            text_parts.append(f"## è§£æ±ºæ–¹æ¡ˆ\n{structured_data['solution']}\n")
        
        # å·®ç•°åŒ–
        if structured_data.get('differentiation'):
            text_parts.append(f"## èˆ‡ç¾æœ‰ç ”ç©¶çš„å·®ç•°åŒ–\n{structured_data['differentiation']}\n")
        
        # é æœŸæ•ˆç›Š
        if structured_data.get('benefit'):
            text_parts.append(f"## é æœŸæ•ˆç›Š\n{structured_data['benefit']}\n")
        
        # å¯¦é©—æ¦‚è¿°
        if structured_data.get('experimental_overview'):
            text_parts.append(f"## å¯¦é©—æ¦‚è¿°\n{structured_data['experimental_overview']}\n")
        
        # ææ–™æ¸…å–®
        if structured_data.get('materials_list'):
            text_parts.append(f"## æ‰€éœ€ææ–™æ¸…å–®\n")
            for material in structured_data['materials_list']:
                text_parts.append(f"- {material}")
            text_parts.append("")
        
        return "\n".join(text_parts)


class InferenceProcessor(BaseProcessor):
    """æ¨è«–è™•ç†å™¨"""
    
    def process(self, question: str, k: int = 30) -> Dict[str, Any]:
        """è™•ç†æ¨è«–æ¨¡å¼"""
        logger.info("ğŸ§  å•Ÿç”¨æ¨¡å¼ï¼šæ¨è«–æ¨¡å¼ï¼ˆä¸ç´å…¥å¯¦é©—è³‡æ–™ï¼‰")
        
        paper_vectorstore = load_paper_vectorstore()
        chunks = search_documents(paper_vectorstore, question, k=k)
        
        logger.info(f"ğŸ“¦ Paper å‘é‡åº«ï¼š{paper_vectorstore._collection.count()}")
        
        prompt = self._build_inference_prompt(question, chunks)
        response = self.llm_manager.generate_response(prompt)
        
        return {
            "answer": response,
            "citations": self._extract_citations(chunks),
            "chunks": chunks
        }
    
    def _build_inference_prompt(self, question: str, chunks: List[Any]) -> str:
        """æ§‹å»ºæ¨è«–æç¤ºè©"""
        context = self._build_context(chunks)
        
        return f"""
åŸºæ–¼ä»¥ä¸‹æ–‡ç»è³‡æ–™ï¼Œå°å•é¡Œé€²è¡Œæ·±å…¥åˆ†æå’Œæ¨è«–ï¼š

å•é¡Œï¼š{question}

ç›¸é—œæ–‡ç»ï¼š
{context}

è«‹æä¾›ï¼š
1. åŸºæ–¼æ–‡ç»çš„æ·±å…¥åˆ†æ
2. åˆç†çš„æ¨è«–å’Œå»¶ä¼¸
3. å¯èƒ½çš„æ‡‰ç”¨å ´æ™¯
4. æœªä¾†ç ”ç©¶æ–¹å‘å»ºè­°

è«‹ç¢ºä¿æ¨è«–æœ‰ç†æœ‰æ“šï¼ŒåŸºæ–¼æ–‡ç»ä½†ä¸å±€é™æ–¼æ–‡ç»ã€‚
"""
    
    def _extract_citations(self, chunks: List[Any]) -> List[str]:
        """æå–å¼•ç”¨ä¿¡æ¯"""
        citations = []
        for chunk in chunks:
            try:
                filename = chunk.metadata.get("filename", "")
                page = chunk.metadata.get("page", "")
                if filename:
                    citations.append(f"{filename} (é ç¢¼: {page})")
            except Exception as e:
                logger.warning(f"æå–å¼•ç”¨ä¿¡æ¯å¤±æ•—: {e}")
        
        return list(set(citations))


class StrictProcessor(BaseProcessor):
    """åš´è¬¹è™•ç†å™¨"""
    
    def process(self, question: str, k: int = 20) -> Dict[str, Any]:
        """è™•ç†åš´è¬¹æ¨¡å¼"""
        logger.info("ğŸ“š å•Ÿç”¨æ¨¡å¼ï¼šåš´è¬¹æ¨¡å¼ï¼ˆåƒ…æ–‡ç»ï¼Œç„¡æ¨è«–ï¼‰")
        
        paper_vectorstore = load_paper_vectorstore()
        chunks = search_documents(paper_vectorstore, question, k=k)
        
        logger.info(f"ğŸ“¦ Paper å‘é‡åº«ï¼š{paper_vectorstore._collection.count()}")
        
        prompt = self._build_strict_prompt(question, chunks)
        response = self.llm_manager.generate_response(prompt)
        
        return {
            "answer": response,
            "citations": self._extract_citations(chunks),
            "chunks": chunks
        }
    
    def _build_strict_prompt(self, question: str, chunks: List[Any]) -> str:
        """æ§‹å»ºåš´è¬¹æç¤ºè©"""
        context = self._build_context(chunks)
        
        return f"""
åŸºæ–¼ä»¥ä¸‹æ–‡ç»è³‡æ–™ï¼Œåš´æ ¼å›ç­”å•é¡Œï¼š

å•é¡Œï¼š{question}

ç›¸é—œæ–‡ç»ï¼š
{context}

è«‹åš´æ ¼åŸºæ–¼ä¸Šè¿°æ–‡ç»è³‡æ–™å›ç­”å•é¡Œï¼Œä¸è¦é€²è¡Œä»»ä½•æ¨è«–æˆ–å»¶ä¼¸ã€‚
å¦‚æœæ–‡ç»è³‡æ–™ä¸è¶³ä»¥å›ç­”å•é¡Œï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚
"""
    
    def _extract_citations(self, chunks: List[Any]) -> List[str]:
        """æå–å¼•ç”¨ä¿¡æ¯"""
        citations = []
        for chunk in chunks:
            try:
                filename = chunk.metadata.get("filename", "")
                page = chunk.metadata.get("page", "")
                if filename:
                    citations.append(f"{filename} (é ç¢¼: {page})")
            except Exception as e:
                logger.warning(f"æå–å¼•ç”¨ä¿¡æ¯å¤±æ•—: {e}")
        
        return list(set(citations))


class ExperimentDetailProcessor(BaseProcessor):
    """å¯¦é©—ç´°ç¯€è™•ç†å™¨"""
    
    def process(self, question: str, chunks: List[Any], proposal: str) -> Dict[str, Any]:
        """è™•ç†å¯¦é©—ç´°ç¯€æ“´å±•"""
        logger.info("ğŸ”¬ å•Ÿç”¨æ¨¡å¼ï¼šexpand to experiment detail (çµæ§‹åŒ–è¼¸å‡º)")
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context = self._build_context(chunks)
        
        # ç²å– schema
        schema = get_schema_by_type("experimental_detail")
        if not schema:
            raise ValueError("ç„¡æ³•ç²å–å¯¦é©—è©³æƒ… schema")
        
        # æ§‹å»ºæç¤º
        prompt = self._build_experiment_detail_prompt(question, context, proposal)
        
        # ç”Ÿæˆçµæ§‹åŒ–å›æ‡‰
        structured_data = self.llm_manager.generate_structured_response(
            prompt=prompt,
            schema=schema,
            system_message="ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å¯¦é©—è¨­è¨ˆåŠ©æ‰‹ï¼Œæ“…é•·åŸºæ–¼æ–‡ç»åˆ†æç”Ÿæˆè©³ç´°çš„å¯¦é©—æ–¹æ¡ˆã€‚"
        )
        
        # é©—è­‰å›æ‡‰
        if not self.llm_manager.validate_response(structured_data, schema):
            raise LLMError("ç”Ÿæˆçš„å¯¦é©—è©³æƒ…ä¸ç¬¦åˆ schema è¦æ±‚")
        
        # è½‰æ›ç‚ºæ–‡æœ¬æ ¼å¼
        text_experiment = self._structured_experiment_to_text(structured_data)
        
        return {
            "answer": text_experiment,
            "citations": structured_data.get('citations', []),
            "chunks": chunks,
            "structured_experiment": structured_data
        }
    
    def _build_experiment_detail_prompt(self, question: str, context: str, proposal: str) -> str:
        """æ§‹å»ºå¯¦é©—ç´°ç¯€æç¤º"""
        return f"""
åŸºæ–¼ä»¥ä¸‹å¯¦é©—ä¸»é¡Œã€ç›¸é—œæ–‡ç»å’Œææ¡ˆï¼Œç”Ÿæˆè©³ç´°çš„å¯¦é©—æ–¹æ¡ˆï¼š

å¯¦é©—ä¸»é¡Œï¼š{question}

ç›¸é—œæ–‡ç»ï¼š
{context}

ç ”ç©¶ææ¡ˆï¼š
{proposal}

è«‹ç”Ÿæˆä¸€å€‹åŒ…å«ä»¥ä¸‹è¦ç´ çš„å¯¦é©—æ–¹æ¡ˆï¼š
1. å¯¦é©—ç›®æ¨™
2. å¯¦é©—æ–¹æ³•
3. æ‰€éœ€ææ–™æ¸…å–®
4. è©³ç´°å¯¦é©—æ­¥é©Ÿ
5. é æœŸçµæœ

è«‹ç¢ºä¿å¯¦é©—æ–¹æ¡ˆå…·æœ‰å¯æ“ä½œæ€§å’Œå¯é‡è¤‡æ€§ã€‚
"""
    
    def _structured_experiment_to_text(self, structured_data: Dict[str, Any]) -> str:
        """å°‡çµæ§‹åŒ–å¯¦é©—è©³æƒ…è½‰æ›ç‚ºæ–‡æœ¬æ ¼å¼"""
        if not structured_data:
            return ""
        
        text_parts = []
        
        # å¯¦é©—ç›®æ¨™
        if structured_data.get('experiment_objective'):
            text_parts.append(f"## å¯¦é©—ç›®æ¨™\n{structured_data['experiment_objective']}\n")
        
        # å¯¦é©—æ–¹æ³•
        if structured_data.get('experiment_method'):
            text_parts.append(f"## å¯¦é©—æ–¹æ³•\n{structured_data['experiment_method']}\n")
        
        # ææ–™æ¸…å–®
        if structured_data.get('materials_list'):
            text_parts.append(f"## æ‰€éœ€ææ–™æ¸…å–®\n")
            for material in structured_data['materials_list']:
                text_parts.append(f"- {material}")
            text_parts.append("")
        
        # å¯¦é©—æ­¥é©Ÿ
        if structured_data.get('experiment_steps'):
            text_parts.append(f"## è©³ç´°å¯¦é©—æ­¥é©Ÿ\n{structured_data['experiment_steps']}\n")
        
        # é æœŸçµæœ
        if structured_data.get('expected_results'):
            text_parts.append(f"## é æœŸçµæœ\n{structured_data['expected_results']}\n")
        
        return "\n".join(text_parts)


class InnovationProcessor(BaseProcessor):
    """å‰µæ–°æƒ³æ³•è™•ç†å™¨"""
    
    def process(self, question: str, old_chunks: List[Any], proposal: str, k: int = 5) -> Dict[str, Any]:
        """è™•ç†æ–°æƒ³æ³•ç”Ÿæˆ"""
        logger.info("ğŸ’¡ å•Ÿç”¨æ¨¡å¼ï¼šgenerate new idea (çµæ§‹åŒ–è¼¸å‡º)")
        
        paper_vectorstore = load_paper_vectorstore()
        logger.info(f"ğŸ“¦ Paper å‘é‡åº«ï¼š{paper_vectorstore._collection.count()}")
        
        # æª¢ç´¢æ–°çš„æ–‡æª”å¡Š
        new_chunks = search_documents(paper_vectorstore, question, k=k)
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        new_context = self._build_context(new_chunks)
        old_context = self._build_context(old_chunks)
        
        # ç²å– schema
        schema = get_schema_by_type("revision_proposal")
        if not schema:
            raise ValueError("ç„¡æ³•ç²å–ä¿®è¨‚ææ¡ˆ schema")
        
        # æ§‹å»ºæç¤º
        prompt = self._build_innovation_prompt(question, new_context, old_context, proposal)
        
        # ç”Ÿæˆçµæ§‹åŒ–å›æ‡‰
        structured_data = self.llm_manager.generate_structured_response(
            prompt=prompt,
            schema=schema,
            system_message="ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç ”ç©¶å‰µæ–°åŠ©æ‰‹ï¼Œæ“…é•·åŸºæ–¼æ–°æ–‡ç»ç”Ÿæˆå‰µæ–°çš„ç ”ç©¶æƒ³æ³•ã€‚"
        )
        
        # é©—è­‰å›æ‡‰
        if not self.llm_manager.validate_response(structured_data, schema):
            raise LLMError("ç”Ÿæˆçš„ä¿®è¨‚ææ¡ˆä¸ç¬¦åˆ schema è¦æ±‚")
        
        # è½‰æ›ç‚ºæ–‡æœ¬æ ¼å¼
        text_proposal = self._structured_revision_to_text(structured_data)
        
        return {
            "answer": text_proposal,
            "citations": structured_data.get('citations', []),
            "chunks": new_chunks + old_chunks,
            "structured_proposal": structured_data,
            "materials_list": structured_data.get('materials_list', [])
        }
    
    def _build_innovation_prompt(self, question: str, new_context: str, old_context: str, proposal: str) -> str:
        """æ§‹å»ºå‰µæ–°æç¤º"""
        return f"""
åŸºæ–¼æ–°çš„æ–‡ç»è³‡æ–™å’Œç¾æœ‰ææ¡ˆï¼Œç”Ÿæˆå‰µæ–°çš„ç ”ç©¶æƒ³æ³•ï¼š

ç ”ç©¶ä¸»é¡Œï¼š{question}

æ–°ç™¼ç¾çš„æ–‡ç»ï¼š
{new_context}

åŸæœ‰æ–‡ç»ï¼š
{old_context}

ç¾æœ‰ææ¡ˆï¼š
{proposal}

è«‹ç”Ÿæˆä¸€å€‹åŒ…å«ä»¥ä¸‹è¦ç´ çš„å‰µæ–°ææ¡ˆï¼š
1. ä¿®è¨‚èªªæ˜ï¼ˆåŸºæ–¼æ–°æ–‡ç»çš„æ”¹é€²ï¼‰
2. æ–°çš„ç ”ç©¶èƒŒæ™¯å’Œéœ€æ±‚åˆ†æ
3. æ”¹é€²çš„è§£æ±ºæ–¹æ¡ˆ
4. èˆ‡åŸææ¡ˆçš„å·®ç•°åŒ–
5. é æœŸæ•ˆç›Š
6. å¯¦é©—æ¦‚è¿°
7. æ‰€éœ€ææ–™æ¸…å–®

è«‹ç¢ºä¿æ–°ææ¡ˆå…·æœ‰å‰µæ–°æ€§å’Œå¯è¡Œæ€§ã€‚
"""
    
    def _structured_revision_to_text(self, structured_data: Dict[str, Any]) -> str:
        """å°‡çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆè½‰æ›ç‚ºæ–‡æœ¬æ ¼å¼"""
        if not structured_data:
            return ""
        
        text_parts = []
        
        # ä¿®è¨‚èªªæ˜
        if structured_data.get('revision_explanation'):
            text_parts.append(f"## ä¿®è¨‚èªªæ˜\n{structured_data['revision_explanation']}\n")
        
        # ææ¡ˆæ¨™é¡Œ
        if structured_data.get('proposal_title'):
            text_parts.append(f"# {structured_data['proposal_title']}\n")
        
        # éœ€æ±‚åˆ†æ
        if structured_data.get('need'):
            text_parts.append(f"## ç ”ç©¶éœ€æ±‚\n{structured_data['need']}\n")
        
        # è§£æ±ºæ–¹æ¡ˆ
        if structured_data.get('solution'):
            text_parts.append(f"## è§£æ±ºæ–¹æ¡ˆ\n{structured_data['solution']}\n")
        
        # å·®ç•°åŒ–
        if structured_data.get('differentiation'):
            text_parts.append(f"## èˆ‡ç¾æœ‰ç ”ç©¶çš„å·®ç•°åŒ–\n{structured_data['differentiation']}\n")
        
        # é æœŸæ•ˆç›Š
        if structured_data.get('benefit'):
            text_parts.append(f"## é æœŸæ•ˆç›Š\n{structured_data['benefit']}\n")
        
        # å¯¦é©—æ¦‚è¿°
        if structured_data.get('experimental_overview'):
            text_parts.append(f"## å¯¦é©—æ¦‚è¿°\n{structured_data['experimental_overview']}\n")
        
        # ææ–™æ¸…å–®
        if structured_data.get('materials_list'):
            text_parts.append(f"## æ‰€éœ€ææ–™æ¸…å–®\n")
            for material in structured_data['materials_list']:
                text_parts.append(f"- {material}")
            text_parts.append("")
        
        return "\n".join(text_parts)


# è™•ç†å™¨å·¥å» 
PROCESSOR_MAP = {
    "ç´å…¥å¯¦é©—è³‡æ–™ï¼Œé€²è¡Œæ¨è«–èˆ‡å»ºè­°": AdvancedInferenceProcessor,
    "make proposal": ProposalProcessor,
    "å…è¨±å»¶ä¼¸èˆ‡æ¨è«–": InferenceProcessor,
    "åƒ…åš´è¬¹æ–‡ç»æº¯æº": StrictProcessor,
    "expand to experiment detail": ExperimentDetailProcessor,
    "generate new idea": InnovationProcessor
}


def get_processor(mode: str) -> Optional[BaseProcessor]:
    """
    æ ¹æ“šæ¨¡å¼ç²å–å°æ‡‰çš„è™•ç†å™¨
    
    Args:
        mode (str): è™•ç†æ¨¡å¼
        
    Returns:
        Optional[BaseProcessor]: å°æ‡‰çš„è™•ç†å™¨å¯¦ä¾‹ï¼Œå¦‚æœæ¨¡å¼ä¸å­˜åœ¨å‰‡è¿”å› None
    """
    processor_class = PROCESSOR_MAP.get(mode)
    if processor_class:
        return processor_class()
    return None

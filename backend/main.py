"""
AI Research Assistant Backend API
================================

FastAPI å¾Œç«¯æœå‹™ï¼Œç‚º React å‰ç«¯æä¾› API æ¥å£
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import uvicorn
import os
import sys
import time
import logging
from dotenv import load_dotenv


# ----------------------------------------------------------------
# Get the specific logger that's causing the noise
telemetry_logger = logging.getLogger("chromadb.telemetry.product.posthog")

# Prevent its messages from propagating to the root logger
telemetry_logger.propagate = False

# Add a handler that does nothing (sends the logs to nowhere)
telemetry_logger.addHandler(logging.NullHandler())
# ----------------------------------------------------------------


# æ·»åŠ åŸé …ç›®è·¯å¾‘åˆ° sys.path
sys.path.append(os.path.join(os.path.dirname(__file__), '../app'))

# ==================== å•Ÿå‹•å‰ .env æª”æ¡ˆæª¢æŸ¥ ====================
def check_and_create_env_file_before_startup():
    """
    åœ¨ FastAPI æ‡‰ç”¨å•Ÿå‹•å‰æª¢æŸ¥ä¸¦å‰µå»º .env æª”æ¡ˆ
    """
    try:
        # ç²å–é …ç›®æ ¹ç›®éŒ„ï¼ˆbackend çš„çˆ¶ç›®éŒ„ï¼‰
        current_file = os.path.abspath(__file__)
        backend_dir = os.path.dirname(current_file)
        project_root = os.path.dirname(backend_dir)  # backend çš„çˆ¶ç›®éŒ„
        env_file = os.path.join(project_root, ".env")
        
        print(f"ğŸ” æª¢æŸ¥ .env æª”æ¡ˆè·¯å¾‘: {env_file}")
        
        # æª¢æŸ¥ .env æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(env_file):
            print("ğŸ“ .env æª”æ¡ˆä¸å­˜åœ¨ï¼Œæ­£åœ¨å‰µå»º dummy æª”æ¡ˆ...")
            
            # å‰µå»º dummy .env æª”æ¡ˆ
            dummy_content = "OPENAI_API_KEY=sk-dummy-key-placeholder\n"
            
            try:
                with open(env_file, 'w', encoding='utf-8') as f:
                    f.write(dummy_content)
                print("âœ… Dummy .env æª”æ¡ˆå‰µå»ºæˆåŠŸ")
                print("ğŸ’¡ è«‹åœ¨è¨­å®šé é¢é…ç½®çœŸå¯¦çš„ API Key")
            except Exception as e:
                print(f"âŒ å‰µå»º dummy .env æª”æ¡ˆå¤±æ•—: {e}")
        else:
            print("ğŸ“ .env æª”æ¡ˆå·²å­˜åœ¨")
            
    except Exception as e:
        print(f"âŒ æª¢æŸ¥ .env æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("ğŸ’¡ ç³»çµ±å°‡ç¹¼çºŒå•Ÿå‹•ï¼Œä½†æŸäº›åŠŸèƒ½å¯èƒ½ç„¡æ³•æ­£å¸¸é‹ä½œ")

# åœ¨è¼‰å…¥ç’°å¢ƒè®Šé‡å‰å…ˆæª¢æŸ¥ .env æª”æ¡ˆ
check_and_create_env_file_before_startup()

# è¼‰å…¥ç’°å¢ƒè®Šé‡
load_dotenv()

# éæ¿¾è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", message=".*LangChainDeprecationWarning.*")
warnings.filterwarnings("ignore", message=".*chromadb.telemetry.*")
warnings.filterwarnings("ignore", message=".*Failed to send telemetry event.*")
warnings.filterwarnings("ignore", message=".*capture.*takes 1 positional argument.*")

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€è®Šé‡å­˜å„²å‘é‡çµ±è¨ˆä¿¡æ¯
vector_stats_cache = {
    "paper_vectors": 0,
    "experiment_vectors": 0,
    "total_vectors": 0,
    "last_updated": None,
    "is_initialized": False
}

def initialize_vector_stats():
    """
    åˆå§‹åŒ–å‘é‡çµ±è¨ˆä¿¡æ¯
    åœ¨å¾Œç«¯å•Ÿå‹•æ™‚é å…ˆè¨ˆç®—ä¸¦ç·©å­˜çµ±è¨ˆä¿¡æ¯
    """
    global vector_stats_cache
    
    try:
        logger.info("ğŸš€ é–‹å§‹åˆå§‹åŒ–å‘é‡çµ±è¨ˆä¿¡æ¯...")
        start_time = time.time()
        
        # å°å…¥çµ±è¨ˆå‡½æ•¸
        from chunk_embedding import get_vectorstore_stats
        
        # ç²å–è«–æ–‡å’Œå¯¦é©—å‘é‡çµ±è¨ˆ
        paper_stats = get_vectorstore_stats("paper")
        experiment_stats = get_vectorstore_stats("experiment")
        
        # æå–çµ±è¨ˆæ•¸æ“š
        paper_count = paper_stats.get("total_documents", 0) if isinstance(paper_stats, dict) else 0
        experiment_count = experiment_stats.get("total_documents", 0) if isinstance(experiment_stats, dict) else 0
        
        # æ›´æ–°ç·©å­˜
        vector_stats_cache.update({
            "paper_vectors": paper_count,
            "experiment_vectors": experiment_count,
            "total_vectors": paper_count + experiment_count,
            "last_updated": time.time(),
            "is_initialized": True
        })
        
        end_time = time.time()
        logger.info(f"âœ… å‘é‡çµ±è¨ˆåˆå§‹åŒ–å®Œæˆï¼Œè€—æ™‚: {end_time - start_time:.2f}ç§’")
        logger.info(f"ğŸ“Š çµ±è¨ˆçµæœ - è«–æ–‡: {paper_count}, å¯¦é©—: {experiment_count}, ç¸½è¨ˆ: {paper_count + experiment_count}")
        
    except Exception as e:
        logger.error(f"âŒ å‘é‡çµ±è¨ˆåˆå§‹åŒ–å¤±æ•—: {e}")
        # è¨­ç½®é»˜èªå€¼
        vector_stats_cache.update({
            "paper_vectors": 0,
            "experiment_vectors": 0,
            "total_vectors": 0,
            "last_updated": time.time(),
            "is_initialized": True
        })

def get_cached_vector_stats():
    """
    ç²å–ç·©å­˜çš„å‘é‡çµ±è¨ˆä¿¡æ¯
    """
    return vector_stats_cache

def update_vector_stats_cache():
    """
    æ›´æ–°å‘é‡çµ±è¨ˆç·©å­˜
    åœ¨æ–‡ä»¶è™•ç†å®Œæˆå¾Œèª¿ç”¨æ­¤å‡½æ•¸
    """
    global vector_stats_cache
    
    try:
        logger.info("ğŸ”„ é–‹å§‹æ›´æ–°å‘é‡çµ±è¨ˆç·©å­˜...")
        from chunk_embedding import get_vectorstore_stats
        
        paper_stats = get_vectorstore_stats("paper")
        experiment_stats = get_vectorstore_stats("experiment")
        
        paper_count = paper_stats.get("total_documents", 0) if isinstance(paper_stats, dict) else 0
        experiment_count = experiment_stats.get("total_documents", 0) if isinstance(experiment_stats, dict) else 0
        
        vector_stats_cache.update({
            "paper_vectors": paper_count,
            "experiment_vectors": experiment_count,
            "total_vectors": paper_count + experiment_count,
            "last_updated": time.time()
        })
        
        logger.info(f"âœ… å‘é‡çµ±è¨ˆç·©å­˜æ›´æ–°å®Œæˆ - è«–æ–‡: {paper_count}, å¯¦é©—: {experiment_count}, ç¸½è¨ˆ: {paper_count + experiment_count}")
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°å‘é‡çµ±è¨ˆç·©å­˜å¤±æ•—: {e}")

# å‰µå»º FastAPI æ‡‰ç”¨
app = FastAPI(
    title="AI Research Assistant API",
    description="AI ç ”ç©¶åŠ©ç†å¾Œç«¯ API æœå‹™",
    version="1.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # React é–‹ç™¼æœå‹™å™¨
        "http://localhost:5173",  # Vite é–‹ç™¼æœå‹™å™¨
        "http://127.0.0.1:3000",
        "http://127.0.0.1:5173",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ›è¼‰éœæ…‹æ–‡ä»¶
if os.path.exists("static"):
    app.mount("/static", StaticFiles(directory="static"), name="static")

# å°å…¥è·¯ç”±
from backend.api.routes import routers

# è¨»å†Šè·¯ç”±
for router in routers:
    app.include_router(router, prefix="/api/v1")

@app.on_event("startup")
async def startup_event():
    """
    æ‡‰ç”¨å•Ÿå‹•æ™‚çš„äº‹ä»¶è™•ç†
    """
    logger.info("ğŸš€ AI Research Assistant å¾Œç«¯æœå‹™å•Ÿå‹•ä¸­...")
    
    # åˆå§‹åŒ–å‘é‡çµ±è¨ˆä¿¡æ¯
    initialize_vector_stats()
    
    logger.info("âœ… å¾Œç«¯æœå‹™å•Ÿå‹•å®Œæˆ")

@app.get("/")
async def root():
    """æ ¹è·¯å¾‘ï¼Œè¿”å› API ä¿¡æ¯"""
    return {
        "message": "AI Research Assistant API",
        "version": "1.0.0",
        "docs": "/api/docs",
        "vector_stats_initialized": vector_stats_cache["is_initialized"]
    }

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {
        "status": "healthy", 
        "service": "ai-research-assistant",
        "vector_stats_initialized": vector_stats_cache["is_initialized"]
    }

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    ) 
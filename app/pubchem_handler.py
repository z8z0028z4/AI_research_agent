import requests
import os
import json
from typing import List, Dict
import re
from typing import Optional
# å…¼å®¹æ€§å°å…¥ï¼šæ”¯æŒç›¸å°å°å…¥å’Œçµ•å°å°å…¥
try:
    from .config import PARSED_CHEMICAL_DIR
except ImportError:
    # ç•¶ä½œç‚ºæ¨¡çµ„å°å…¥æ™‚ä½¿ç”¨çµ•å°å°å…¥
    from config import PARSED_CHEMICAL_DIR

BASE_URL = "https://pubchem.ncbi.nlm.nih.gov/rest/pug"

def search_source(keywords: List[str], limit: int = 5) -> List[Dict]:
    """
    Search PubChem by keyword and return metadata with CID
    """
    results = []
    for keyword in keywords:
        url = f"{BASE_URL}/compound/name/{keyword}/cids/JSON"
        try:
            r = requests.get(url, timeout=10,verify=False)
            if r.status_code == 200 and 'IdentifierList' in r.json():
                cids = r.json()['IdentifierList']['CID'][:limit]
                for cid in cids:
                    results.append({
                        "cid": cid,
                        "query": keyword,
                        "source": "PubChem"
                    })
        except Exception as e:
            print(f"PubChem search failed for '{keyword}': {e}")
    return results

def download_and_store(result: Dict, storage_dir: str) -> str:
    """
    ä¸‹è¼‰ PubChem JSON metadata
    """
    cid = result.get("cid")
    if not cid:
        print("âŒ ç„¡ CIDï¼Œè·³é")
        return ""

    url = f"{BASE_URL}/compound/cid/{cid}/JSON"
    os.makedirs(storage_dir, exist_ok=True)
    filepath = os.path.join(storage_dir, f"pubchem_cid{cid}.json")

    try:
        r = requests.get(url, timeout=15, verify=False)
        if r.ok:
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(r.text)
            print(f"âœ… Stored PubChem CID {cid} to {filepath}")
            return filepath
        else:
            print(f"âš ï¸ PubChem å›å‚³éŒ¯èª¤ï¼š{r.status_code}")
    except Exception as e:
        print(f"âŒ Failed to download CID {cid}: {e}")

    return ""

def extract_json_chemical_list_from_llm(text: str) -> list:
    # é¦–å…ˆå˜—è©¦æå–å®Œæ•´çš„ JSON ç‰©ä»¶
    json_match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
    if json_match:
        try:
            json_obj = json.loads(json_match.group(1))
            if 'materials_list' in json_obj and isinstance(json_obj['materials_list'], list):
                print(f"âœ… å¾å®Œæ•´ JSON ç‰©ä»¶ä¸­æå–åˆ° materials_list: {json_obj['materials_list']}")
                return json_obj['materials_list']
        except Exception as e:
            print(f"âŒ å®Œæ•´ JSON ç‰©ä»¶è§£æå¤±æ•—ï¼š{e}")
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°å®Œæ•´ JSON ç‰©ä»¶ï¼Œå˜—è©¦æå–å–®ç¨çš„é™£åˆ—
    array_match = re.search(r"```json\s*(\[[^\]]+\])\s*```", text, re.DOTALL)
    if array_match:
        try:
            return json.loads(array_match.group(1))
        except Exception as e:
            print("âŒ JSON chemical list è§£æå¤±æ•—ï¼š", e)
    
    # å¦‚æœéƒ½æ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦ç›´æ¥è§£ææ•´å€‹æ–‡æœ¬ç‚º JSON
    try:
        # ç§»é™¤å¯èƒ½çš„ markdown æ ¼å¼
        cleaned_text = re.sub(r"```json\s*", "", text)
        cleaned_text = re.sub(r"\s*```", "", cleaned_text)
        json_obj = json.loads(cleaned_text)
        if 'materials_list' in json_obj and isinstance(json_obj['materials_list'], list):
            print(f"âœ… å¾æ¸…ç†å¾Œçš„æ–‡æœ¬ä¸­æå–åˆ° materials_list: {json_obj['materials_list']}")
            return json_obj['materials_list']
    except Exception as e:
        print(f"âŒ ç›´æ¥ JSON è§£æå¤±æ•—ï¼š{e}")
    
    return []

def parse_pubchem_json(json_data: dict) -> dict:
    props = json_data["PC_Compounds"][0].get("props", [])
    cid = json_data["PC_Compounds"][0]["id"]["id"]["cid"]

    def find_prop(label: str, name: str = None):
        for p in props:
            urn = p.get("urn", {})
            if urn.get("label") == label and (name is None or urn.get("name") == name):
                return p.get("value", {}).get("sval") or p.get("value", {}).get("fval")
        return None

    def clean_text_for_xml(text):
        """æ¸…ç†æ–‡æœ¬ä»¥ç¢ºä¿XMLå…¼å®¹æ€§"""
        if not text:
            return ""
        # ç§»é™¤NULLå­—ç¯€å’Œæ§åˆ¶å­—ç¬¦
        cleaned = "".join(char for char in str(text) if ord(char) >= 32 or char in '\n\r\t')
        # ç§»é™¤å…¶ä»–å¯èƒ½å°è‡´XMLå•é¡Œçš„å­—ç¬¦
        cleaned = cleaned.replace('\x00', '')  # NULLå­—ç¯€
        cleaned = cleaned.replace('\x01', '')  # SOH
        cleaned = cleaned.replace('\x02', '')  # STX
        cleaned = cleaned.replace('\x03', '')  # ETX
        cleaned = cleaned.replace('\x04', '')  # EOT
        cleaned = cleaned.replace('\x05', '')  # ENQ
        cleaned = cleaned.replace('\x06', '')  # ACK
        cleaned = cleaned.replace('\x07', '')  # BEL
        cleaned = cleaned.replace('\x08', '')  # BS
        cleaned = cleaned.replace('\x0b', '')  # VT
        cleaned = cleaned.replace('\x0c', '')  # FF
        cleaned = cleaned.replace('\x0e', '')  # SO
        cleaned = cleaned.replace('\x0f', '')  # SI
        return cleaned

    # å„ªå…ˆä½¿ç”¨IUPACåç¨±ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨å…¶ä»–åç¨±
    iupac_name = find_prop("IUPAC Name", "Preferred") or find_prop("IUPAC Name", "Traditional")
    if not iupac_name:
        # å¦‚æœæ²’æœ‰IUPACåç¨±ï¼Œå˜—è©¦å…¶ä»–åç¨±
        iupac_name = find_prop("IUPAC Name") or find_prop("Title") or "Unknown"
    
    return {
        "cid": cid,
        "name": clean_text_for_xml(iupac_name),
        "iupac_name": clean_text_for_xml(iupac_name),  # æ˜ç¢ºæ¨™è¨˜IUPACåç¨±
        "formula": clean_text_for_xml(find_prop("Molecular Formula")),
        "weight": clean_text_for_xml(find_prop("Molecular Weight")),
        "smiles": clean_text_for_xml(find_prop("SMILES", "Absolute") or find_prop("SMILES", "Connectivity")),
        "image_url": f"https://pubchem.ncbi.nlm.nih.gov/image/imgsrv.fcgi?cid={cid}&t=l"
    }

def get_boiling_and_melting_point(cid: int) -> dict:
    url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/{cid}/JSON"
    try:
        r = requests.get(url, timeout=15, verify=False)
        if not r.ok:
            print(f"âš ï¸ View API å›å‚³å¤±æ•—ï¼šCID {cid}, status: {r.status_code}")
            return {}

        data = r.json()
        sections = data.get("Record", {}).get("Section", [])
        result = {}

        def find_in_sections(sections, keyword):
            for section in sections:
                toc = section.get("TOCHeading", "")
                if keyword.lower() in toc.lower():
                    for info in section.get("Information", []):
                        val = info.get("Value", {}).get("StringWithMarkup", [])
                        if val:
                            return val[0].get("String", "").strip()
                sub = section.get("Section")
                if sub:
                    found = find_in_sections(sub, keyword)
                    if found:
                        return found
            return None

        def extract_celsius(temp_str: str):
            if not temp_str:
                return None
            match = re.search(r"([-+]?[0-9.]+)\s*Â°F", temp_str)
            if match:
                f = float(match.group(1))
                c = round((f - 32) * 5 / 9, 1)
                return f"{c} Â°C"
            match = re.search(r"([-+]?[0-9.]+)\s*Â°C", temp_str)
            if match:
                return match.group(0)
            return None

        def clean_text_for_xml(text):
            """æ¸…ç†æ–‡æœ¬ä»¥ç¢ºä¿XMLå…¼å®¹æ€§"""
            if not text:
                return ""
            # ç§»é™¤NULLå­—ç¯€å’Œæ§åˆ¶å­—ç¬¦
            cleaned = "".join(char for char in str(text) if ord(char) >= 32 or char in '\n\r\t')
            # ç§»é™¤å…¶ä»–å¯èƒ½å°è‡´XMLå•é¡Œçš„å­—ç¬¦
            cleaned = cleaned.replace('\x00', '')  # NULLå­—ç¯€
            cleaned = cleaned.replace('\x01', '')  # SOH
            cleaned = cleaned.replace('\x02', '')  # STX
            cleaned = cleaned.replace('\x03', '')  # ETX
            cleaned = cleaned.replace('\x04', '')  # EOT
            cleaned = cleaned.replace('\x05', '')  # ENQ
            cleaned = cleaned.replace('\x06', '')  # ACK
            cleaned = cleaned.replace('\x07', '')  # BEL
            cleaned = cleaned.replace('\x08', '')  # BS
            cleaned = cleaned.replace('\x0b', '')  # VT
            cleaned = cleaned.replace('\x0c', '')  # FF
            cleaned = cleaned.replace('\x0e', '')  # SO
            cleaned = cleaned.replace('\x0f', '')  # SI
            return cleaned

        bp_raw = find_in_sections(sections, "Boiling Point")
        mp_raw = find_in_sections(sections, "Melting Point")

        result["boiling_point"] = clean_text_for_xml(bp_raw)
        result["melting_point"] = clean_text_for_xml(mp_raw)
        result["boiling_point_c"] = clean_text_for_xml(extract_celsius(bp_raw) if bp_raw else None)
        result["melting_point_c"] = clean_text_for_xml(extract_celsius(mp_raw) if mp_raw else None)

        return result

    except Exception as e:
        print(f"âŒ æ“·å–ç†”é»æ²¸é»å¤±æ•— CID {cid}: {e}")
        return {}

def get_safety_info(cid: int) -> dict:
    """
    å¾ PubChem PUG View API æ“·å–ï¼š
    - GHS hazard icon URL list
    - NFPA diamond image URL
    - CAS No.ï¼ˆç¬¬ä¸€çµ„åˆæ³•æ ¼å¼ï¼‰
    """
    url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/{cid}/JSON"
    try:
        r = requests.get(url, timeout=15, verify=False)
        if not r.ok:
            print(f"âš ï¸ PubChem View æŸ¥è©¢å¤±æ•—ï¼šCID {cid} / {r.status_code}")
            return {"ghs_icons": [], "nfpa_image": None, "cas": None}

        json_data = r.json()
        sections = json_data.get("Record", {}).get("Section", [])

        ghs_urls = set()
        nfpa_url = None
        cas_number = None

        def clean_text_for_xml(text):
            """æ¸…ç†æ–‡æœ¬ä»¥ç¢ºä¿XMLå…¼å®¹æ€§"""
            if not text:
                return ""
            # ç§»é™¤NULLå­—ç¯€å’Œæ§åˆ¶å­—ç¬¦
            cleaned = "".join(char for char in str(text) if ord(char) >= 32 or char in '\n\r\t')
            # ç§»é™¤å…¶ä»–å¯èƒ½å°è‡´XMLå•é¡Œçš„å­—ç¬¦
            cleaned = cleaned.replace('\x00', '')  # NULLå­—ç¯€
            cleaned = cleaned.replace('\x01', '')  # SOH
            cleaned = cleaned.replace('\x02', '')  # STX
            cleaned = cleaned.replace('\x03', '')  # ETX
            cleaned = cleaned.replace('\x04', '')  # EOT
            cleaned = cleaned.replace('\x05', '')  # ENQ
            cleaned = cleaned.replace('\x06', '')  # ACK
            cleaned = cleaned.replace('\x07', '')  # BEL
            cleaned = cleaned.replace('\x08', '')  # BS
            cleaned = cleaned.replace('\x0b', '')  # VT
            cleaned = cleaned.replace('\x0c', '')  # FF
            cleaned = cleaned.replace('\x0e', '')  # SO
            cleaned = cleaned.replace('\x0f', '')  # SI
            return cleaned

        def walk(sections):
            nonlocal ghs_urls, nfpa_url, cas_number
            for sec in sections:
                heading = sec.get("TOCHeading", "")

                # GHS åœ–ç¤º
                if heading == "GHS Classification":
                    for info in sec.get("Information", []):
                        if info.get("Name") == "Pictogram(s)":
                            for val in info.get("Value", {}).get("StringWithMarkup", []):
                                for markup in val.get("Markup", []):
                                    if markup.get("Type") == "Icon":
                                        ghs_urls.add(markup.get("URL"))

                elif heading == "NFPA Hazard Classification":
                    for info in sec.get("Information", []):
                        if info.get("Name") == "NFPA 704 Diamond":
                            for val in info.get("Value", {}).get("StringWithMarkup", []):
                                for markup in val.get("Markup", []):
                                    if markup.get("Type") == "Icon":
                                        nfpa_url = markup.get("URL")

                # CAS Number
                elif heading == "CAS" and not cas_number:
                    for info in sec.get("Information", []):
                        val = info.get("Value", {}).get("StringWithMarkup", [])
                        if val:
                            for entry in val:
                                maybe_cas = entry.get("String", "")
                                if "-" in maybe_cas and maybe_cas.count("-") == 2:
                                    cas_number = clean_text_for_xml(maybe_cas)
                                    break

                # éè¿´æ·±å…¥å­å€å¡Š
                if "Section" in sec:
                    walk(sec["Section"])

        walk(sections)

        return {
            "ghs_icons": sorted(ghs_urls),
            "nfpa_image": nfpa_url,
            "cas": cas_number
        }

    except Exception as e:
        print(f"âŒ æ“·å– hazard icon / CAS No. å¤±æ•— CID {cid}: {e}")
        return {"ghs_icons": [], "nfpa_image": None, "cas": None}




def extract_and_fetch_chemicals(name_list: List[str], save_dir=PARSED_CHEMICAL_DIR) -> List[dict]:
    """
    æ¥æ”¶ä¸€çµ„ GPT å‚³å›çš„åŒ–å­¸å“åç¨±æ¸…å–®ï¼Œé€ä¸€æŸ¥è©¢ã€æ“·å–ã€å„²å­˜ä¹¾æ·¨çš„ JSONã€‚
    åƒ…ç•™ä¸‹ parse éçš„ parsed_cid{cid}.json
    """
    os.makedirs(save_dir, exist_ok=True)
    summaries = []
    not_found = []

    for name in name_list:
        results = search_source([name], limit=2)
        if not results:
            print(f"âŒ æ‰¾ä¸åˆ°åŒ–å­¸å“ï¼š{name}")
            not_found.append(name)
            continue

        cid = results[0].get("cid")
        if not cid:
            print(f"âŒ ç„¡ CIDï¼š{name}")
            not_found.append(name)
            continue

        try:
            # Step 1: General compound JSON
            url_main = f"{BASE_URL}/compound/cid/{cid}/JSON"
            r_main = requests.get(url_main, timeout=15, verify=False)
            if not r_main.ok:
                print(f"âš ï¸ ä¸»æŸ¥è©¢å¤±æ•— CID {cid}: {r_main.status_code}")
                not_found.append(name)
                continue

            json_data = r_main.json()
            parsed = parse_pubchem_json(json_data)

            #åŠ å…¥pubchemè¶…é€£çµ:
            parsed["pubchem_url"] = f"https://pubchem.ncbi.nlm.nih.gov/compound/{cid}"

            # Step 2: è£œå……ç†”/æ²¸é»è³‡è¨Šï¼ˆå«è½‰æ”æ°ï¼‰
            bp_info = get_boiling_and_melting_point(cid)
            parsed.update(bp_info)

            # Step 3: è£œå…… GHS icon URLs
            safety_info = get_safety_info(cid)

            parsed["safety_icons"] = {
                "ghs_icons": safety_info.get("ghs_icons", []),
                "nfpa_image": safety_info.get("nfpa_image")
            }
            parsed["cas"] = safety_info.get("cas")

            # Step 4: å„²å­˜ä¹¾æ·¨ç‰ˆæœ¬
            save_path = os.path.join(save_dir, f"parsed_cid{cid}.json")
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(parsed, f, indent=2)
            # print(f"âœ… Saved parsed info: {save_path}")
            summaries.append(parsed)

        except Exception as e:
            print(f"âŒ Failed to process {name} (CID {cid}): {e}")
            not_found.append(name)

    return summaries, not_found

def remove_json_chemical_block(text: str) -> str:
    return re.sub(r"```json\s*\[[^\]]+\]\s*```", "", text, flags=re.DOTALL)

    
def chemical_metadata_extractor(proposal_text: str):
    print(f"ğŸ” é–‹å§‹æå–åŒ–å­¸å“ï¼Œææ¡ˆæ–‡æœ¬é•·åº¦ï¼š{len(proposal_text)} å­—ç¬¦")
    print(f"ğŸ“ ææ¡ˆæ–‡æœ¬é è¦½ï¼š{proposal_text[:300]}...")
    
    name_list = extract_json_chemical_list_from_llm(proposal_text)
    print(f"ğŸ” æ“·å–åˆ°çš„åŒ–å­¸å“ï¼š{name_list}")
    
    if not name_list:
        print("âš ï¸ æ²’æœ‰æ‰¾åˆ°åŒ–å­¸å“åˆ—è¡¨ï¼Œé€™å¯èƒ½æ˜¯å› ç‚º LLM æ²’æœ‰ç”Ÿæˆ JSON æ ¼å¼çš„åŒ–å­¸å“åˆ—è¡¨")
        print("ğŸ” æª¢æŸ¥ææ¡ˆæ–‡æœ¬ä¸­æ˜¯å¦åŒ…å« JSON æ ¼å¼çš„åŒ–å­¸å“åˆ—è¡¨...")
        json_match = re.search(r"```json\s*\[[^\]]+\]\s*```", proposal_text, re.DOTALL)
        if json_match:
            print(f"âœ… æ‰¾åˆ° JSON æ ¼å¼ï¼š{json_match.group(0)}")
        else:
            print("âŒ æ²’æœ‰æ‰¾åˆ° JSON æ ¼å¼çš„åŒ–å­¸å“åˆ—è¡¨")
    
    summaries, not_found = extract_and_fetch_chemicals(name_list)
    cleaned_proposal_text = remove_json_chemical_block(proposal_text)
    return summaries, not_found, cleaned_proposal_text
import os
import pickle
import shutil
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from chromadb import Client
from chromadb.config import Settings
from config import VECTOR_INDEX_DIR
from pdf_read_and_chunk_page_get import load_and_parse_file, get_page_number_for_chunk
from langchain_community.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain_community.embeddings import HuggingFaceEmbeddings

def embed_documents_from_metadata(metadata_list):
    # æ–‡å­—åˆ‡å‰²å™¨
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", ".", "ã€‚", " ", ""]
    )

    
    
    # æ•´ç†è¦åµŒå…¥çš„æ–‡å­—èˆ‡å°æ‡‰ metadata
    texts, metadatas = [], []
    for metadata in tqdm(metadata_list, desc="ğŸ“š Chunking & Metadata"):
        doc_type = metadata["type"]
        title = metadata.get("title", "")
        tracing_number = metadata.get("tracing_number")
        filename = metadata["new_filename"]
        file_path = metadata["new_path"]

        doc_chunks = load_and_parse_file(file_path)
        for i, chunk in enumerate(splitter.split_text(doc_chunks)):
            page_num = get_page_number_for_chunk(file_path, chunk)
            texts.append(chunk)
            metadatas.append({
                "title": title,
                "type": doc_type,
                "tracing_number": tracing_number,
                "filename": filename,
                "chunk_index": i,
                "page_number": page_num
            })

    print("ğŸ“¡ åµŒå…¥ä¸­...")
    embedding_model = HuggingFaceEmbeddings(
    model_name="nomic-ai/nomic-embed-text-v1.5",
    model_kwargs={"trust_remote_code": True}
    )


    # ğŸ›¡ å‚™ä»½åŸæœ‰è³‡æ–™
    if os.path.exists(VECTOR_INDEX_DIR) and os.listdir(VECTOR_INDEX_DIR):
        backup_dir = Path(VECTOR_INDEX_DIR) / "backups" / datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir.mkdir(parents=True, exist_ok=True)
        for fname in ["chroma.sqlite3", "embedding_cache.pkl"]:
            fpath = Path(VECTOR_INDEX_DIR) / fname
            if fpath.exists():
                shutil.copy2(fpath, backup_dir / fname)
        print(f"ğŸ›¡ å·²å‚™ä»½åŸæœ‰å‘é‡è³‡æ–™åº«è‡³ {backup_dir}")

    # âœ… ä½¿ç”¨ç´¯ç©å¼ Chroma
    chroma_client = Client(Settings(persist_directory=VECTOR_INDEX_DIR))
    collection = chroma_client.get_or_create_collection("default")

    # æº–å‚™ Document å°è±¡ï¼ˆåŒ…å« metadataï¼‰
    documents = [
        Document(page_content=text, metadata=meta)
        for text, meta in zip(texts, metadatas)
    ]

    # å»ºç«‹ Chroma vector store ä¸¦æŒä¹…åŒ–
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embedding_model,
        persist_directory=VECTOR_INDEX_DIR,
        collection_name="default"
    )

    vectorstore.persist()

    with open(os.path.join(VECTOR_INDEX_DIR, "embedding_cache.pkl"), "wb") as f:
        pickle.dump((texts, metadatas), f)

    print(f"âœ… åµŒå…¥å®Œæˆï¼Œå…±æ–°å¢ {len(texts)} æ®µ")

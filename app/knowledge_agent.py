import pandas as pd
from rag_core import load_paper_vectorstore,build_proposal_prompt,build_detail_experimental_plan_prompt, load_experiment_vectorstore, preview_chunks, retrieve_chunks_multi_query, build_prompt, call_llm, build_inference_prompt, build_dual_inference_prompt, expand_query
from config import EXPERIMENT_DIR
import os


def agent_answer(question: str, mode:str ="defualt",  **kwargs):
    
    if mode == "ç´å…¥å¯¦é©—è³‡æ–™ï¼Œé€²è¡Œæ¨è«–èˆ‡å»ºè­°":
        print("ğŸ§ª å•Ÿç”¨æ¨¡å¼ï¼šç´å…¥å¯¦é©—è³‡æ–™ + æ¨è«–")
        # ğŸ§  Dual retriever å•Ÿç”¨
        paper_vectorstore = load_paper_vectorstore()  # æ–‡ç»å‘é‡åº«
        experiment_vectorstore = load_experiment_vectorstore()  # å¯¦é©—å‘é‡åº«
        print("ğŸ“¦ Paper å‘é‡åº«ï¼š", paper_vectorstore._collection.count())
        print("ğŸ“¦ Experiment å‘é‡åº«ï¼š", experiment_vectorstore._collection.count())

        # ğŸ” èªæ„æ‹“å±•
        query_list = expand_query(question) #çµ¦ chunks_paperçš„èªæ„æ‹“å±•ç”¨
        chunks_paper = retrieve_chunks_multi_query(paper_vectorstore, query_list, k=5)
        experiment_chunks = retrieve_chunks_multi_query(experiment_vectorstore, [question], k=5) #question å•ï¼Œèªæ„æœªæ‹“å±•
        preview_chunks(chunks_paper, title="æ–‡ç»å‘é‡åº«")
        preview_chunks(experiment_chunks, title="å¯¦é©—å‘é‡åº«")
        prompt, citations = build_dual_inference_prompt(
            chunks_paper, question, experiment_chunks
        )
    elif mode == "make proposal":
        paper_vectorstore = load_paper_vectorstore()
        print("ğŸ“¦ Paper å‘é‡åº«ï¼š", paper_vectorstore._collection.count())
        chunks = retrieve_chunks_multi_query(paper_vectorstore, [question], k=10)
        prompt, citations = build_proposal_prompt(chunks, question)


    elif mode == "å…è¨±å»¶ä¼¸èˆ‡æ¨è«–":
        # ä¸€èˆ¬æ¨¡å¼ä½¿ç”¨å–®ä¸€ retriever
        paper_vectorstore = load_paper_vectorstore()
        chunks = retrieve_chunks_multi_query(paper_vectorstore, [question])
        print("ğŸ“¦ Paper å‘é‡åº«ï¼š", paper_vectorstore._collection.count())
        print("ğŸ§  å•Ÿç”¨æ¨¡å¼ï¼šæ¨è«–æ¨¡å¼ï¼ˆä¸ç´å…¥å¯¦é©—è³‡æ–™ï¼‰")
        prompt, citations = build_inference_prompt(chunks, [question])
    
    elif mode == "åƒ…åš´è¬¹æ–‡ç»æº¯æº":
        print("ğŸ“š å•Ÿç”¨æ¨¡å¼ï¼šåš´è¬¹æ¨¡å¼ï¼ˆåƒ…æ–‡ç»ï¼Œç„¡æ¨è«–ï¼‰")
        paper_vectorstore = load_paper_vectorstore()
        chunks = retrieve_chunks_multi_query(paper_vectorstore, [question])
        print("ğŸ“¦ Paper å‘é‡åº«ï¼š", paper_vectorstore._collection.count())
        prompt, citations = build_prompt(chunks, [question])

    elif mode == "expand to experiment detail":
    # ä½¿ç”¨ proposal + paper_chunks ç”¢å‡º detail
        chunks = kwargs.get("chunks", [])
        proposal = kwargs.get("proposal", "")
        prompt = build_detail_experimental_plan_prompt(chunks, proposal)
        response = call_llm(prompt)
        return {
            "answer": response,
            "citations": [],  # ä½ å¯ä»¥ç•™ç©ºæˆ–ä¿ç•™ chunks è³‡è¨Š
        }

    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨¡å¼ï¼š{mode}")

    response = call_llm(prompt)

    return {
        "answer": response,
        "citations": citations,
        "chunks":chunks
    }

    
    



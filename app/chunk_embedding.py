from config import VECTOR_INDEX_DIR, EMBEDDING_MODEL_NAME
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pdf_read_and_chunk_page_get import load_and_parse_file, get_page_number_for_chunk
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from tqdm import tqdm
import os
from typing import List
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ðŸš€ åµŒå…¥æ¨¡åž‹ä½¿ç”¨è¨­å‚™ï¼š{device.upper()}")

def embed_documents_from_metadata(metadata_list, status_callback=None):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", ".", "ã€‚", " ", ""]
    )
    texts, metadatas = [], []

    for metadata in tqdm(metadata_list, desc="ðŸ“š Chunking & Metadata"):
        doc_type = metadata.get("type", "")
        title = metadata.get("title", "")
        tracing_number = metadata.get("tracing_number", "")
        filename = metadata["new_filename"]
        file_path = metadata["new_path"]

        doc_chunks = load_and_parse_file(file_path)
        for i, chunk in enumerate(splitter.split_text(doc_chunks)):
            if len(chunk.strip()) < 10:
                continue
            page_num = get_page_number_for_chunk(file_path, chunk)
            texts.append(chunk)
            metadatas.append({
                "title": title,
                "type": doc_type,
                "tracing_number": tracing_number,
                "filename": filename,
                "chunk_index": i,
                "page_number": page_num
            })

    print("ðŸ“¡ åµŒå…¥ä¸­...")
    

    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={
            "trust_remote_code": True,
            "device": device,
        }
    )
    
    experiment_vector_dir = os.path.join(VECTOR_INDEX_DIR, "paper_vector")
    vectorstore = Chroma(
        persist_directory=experiment_vector_dir,
        embedding_function=embedding_model, 
        collection_name="paper"
        )

    BATCH_SIZE = 500
    for i in range(0, len(texts), BATCH_SIZE):
        batch_texts = texts[i:i+BATCH_SIZE]
        batch_metadatas = metadatas[i:i+BATCH_SIZE]
        vectorstore.add_texts(texts=batch_texts, metadatas=batch_metadatas)
    
    vectorstore.persist()  # âœ… åŠ é€™è¡Œæ‰æœƒå¯«å…¥ collection
    doc_stats = vectorstore.get(include=["documents"])
    print("ðŸ“¦ å‘é‡åº«ç›®å‰å…±åŒ…å«ï¼š", len(doc_stats["documents"]), "æ®µ")

    if status_callback:
        status_callback(f"âœ… åµŒå…¥å®Œæˆï¼Œå…±æ–°å¢ž {len(texts)} æ®µ")

    print("ðŸ“Š åµŒå…¥æ®µè½çµ±è¨ˆï¼š")
    for i, text in enumerate(texts[:10]):
        preview = text[:80].replace("\n", " ")
        print(f"Chunk {i+1} | é•·åº¦: {len(text)} | é ­éƒ¨: {preview}")


def embed_experiment_txt_batch(txt_paths: List[str], status_callback=None):
    """
    å°‡æŒ‡å®šæ¸…å–®ä¸­çš„ txt æª”æ¡ˆåµŒå…¥ï¼ˆä¸éæ­·æ•´å€‹è³‡æ–™å¤¾ï¼‰
    æ¯å€‹ txt ç‚ºä¸€å€‹ chunkï¼Œexp_id ç‚ºæª”åï¼ˆç„¡å‰¯æª”åï¼‰
    """

    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={"trust_remote_code": True,
                      "device": device
        }
    
    )
    experiment_vector_dir = os.path.join(VECTOR_INDEX_DIR, "experiment_vector")
    vectorstore = Chroma(
        persist_directory=experiment_vector_dir, 
        embedding_function=embedding_model, 
        collection_name="experiment"
        )

    texts, metadatas = [], []

    for path in txt_paths:
        if not path.endswith(".txt"):
            continue
        exp_id = os.path.splitext(os.path.basename(path))[0]

        with open(path, "r", encoding="utf-8") as f:
            content = f.read().strip()
        if len(content) < 10:
            continue

        texts.append(content)
        metadatas.append({
            "type": "experiment",
            "exp_id": exp_id,
            "filename": os.path.basename(path),
        })

    if not texts:
        if status_callback:
            status_callback("âš ï¸ æ²’æœ‰æ–°çš„å¯¦é©—æ‘˜è¦å¯åµŒå…¥")
        return

    vectorstore.add_texts(texts=texts, metadatas=metadatas)
    vectorstore.persist()  # âœ… åŠ é€™è¡Œæ‰æœƒå¯«å…¥ collection

    docs = vectorstore.get(include=["documents"])
    print("ðŸ“¦ å‘é‡æ•¸é‡ï¼š", len(docs["documents"]))

    if status_callback:
        status_callback(f"âœ… åµŒå…¥å®Œæˆï¼Œå…± {len(texts)} ç­†å¯¦é©—æ‘˜è¦")

    print("ðŸ“Š æœ¬æ¬¡åµŒå…¥ previewï¼š")
    for i, t in enumerate(texts[:5]):
        print(f"#{i+1} | {metadatas[i]['exp_id']} | é ­ 80 å­—ï¼š{t[:80].replace(chr(10), ' ')}")


"""
AI ç ”ç©¶åŠ©ç† - æ–‡æª”åˆ†å¡Šå’Œå‘é‡åµŒå…¥æ¨¡å¡Š
================================

é€™å€‹æ¨¡å¡Šè² è²¬å°‡æ–‡æª”è½‰æ›ç‚ºå‘é‡è¡¨ç¤ºï¼Œç‚ºRAGç³»çµ±æä¾›æª¢ç´¢åŸºç¤ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. æ–‡æª”åˆ†å¡Šå’Œé è™•ç†
2. æ–‡æœ¬å‘é‡åŒ–
3. å‘é‡æ•¸æ“šåº«å­˜å„²
4. æ‰¹é‡è™•ç†æ”¯æŒ

æ¶æ§‹èªªæ˜ï¼š
- ä½¿ç”¨LangChainçš„æ–‡æœ¬åˆ†å‰²å™¨
- é›†æˆHuggingFaceåµŒå…¥æ¨¡å‹
- ä½¿ç”¨Chromaä½œç‚ºå‘é‡æ•¸æ“šåº«
- æ”¯æŒGPUåŠ é€Ÿè¨ˆç®—

âš ï¸ æ³¨æ„ï¼šæ­¤æ¨¡å¡Šæ˜¯RAGç³»çµ±çš„åŸºç¤ï¼Œæ‰€æœ‰æ–‡æª”æª¢ç´¢éƒ½ä¾è³´æ–¼æ­¤æ¨¡å¡Š
"""

import os
import sys
import time
import logging
from typing import List, Dict, Any, Optional
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import chromadb
from chromadb.config import Settings
from pdf_read_and_chunk_page_get import load_and_parse_file, get_page_number_for_chunk
import torch

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# ==================== å…¨å±€è®Šé‡ ====================
# é…ç½®è·¯å¾‘
VECTOR_INDEX_DIR = os.path.join(os.path.dirname(__file__), "..", "experiment_data", "vector_index")

# Add backend to path to import settings_manager
backend_path = os.path.join(os.path.dirname(__file__), "..", "backend")
if backend_path not in sys.path:
    sys.path.insert(0, backend_path)

try:
    from core.settings_manager import settings_manager
    EMBEDDING_MODEL_NAME = settings_manager.get_embedding_model()
except (ImportError, AttributeError):
    EMBEDDING_MODEL_NAME = "BAAI/bge-small-zh-v1.5"


# è¨­å‚™é…ç½®
device = "cuda" if torch.cuda.is_available() else "cpu"

# å…¨å±€ Chroma å¯¦ä¾‹ç·©å­˜ï¼Œé¿å…é‡è¤‡å‰µå»º
_chroma_instances = {}

def get_chroma_instance(vectorstore_type: str = "paper"):
    """
    ç²å–æˆ–å‰µå»º Chroma å¯¦ä¾‹ï¼ˆä½¿ç”¨æ–°çš„ ChromaDB æ¶æ§‹ï¼‰
    
    åƒæ•¸ï¼š
        vectorstore_type (str): å‘é‡æ•¸æ“šåº«é¡å‹ï¼ˆ"paper" æˆ– "experiment"ï¼‰
    
    è¿”å›ï¼š
        Chroma: å‘é‡æ•¸æ“šåº«å¯¦ä¾‹
    """
    if vectorstore_type not in _chroma_instances:
        try:
            embedding_model = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL_NAME,
                model_kwargs={"trust_remote_code": True, "device": device}
            )
            
            if vectorstore_type == "paper":
                vector_dir = os.path.join(VECTOR_INDEX_DIR, "paper_vector")
                collection_name = "paper"
            else:
                vector_dir = os.path.join(VECTOR_INDEX_DIR, "experiment_vector")
                collection_name = "experiment"
            
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            os.makedirs(vector_dir, exist_ok=True)
            
            # ä½¿ç”¨æ–°çš„ ChromaDB 1.0+ å®¢æˆ¶ç«¯é…ç½®
            client = chromadb.PersistentClient(
                path=vector_dir
            )
            
            _chroma_instances[vectorstore_type] = Chroma(
                client=client,
                collection_name=collection_name,
                embedding_function=embedding_model
            )
            
        except Exception as e:
            print(f"âŒ å‰µå»ºå‘é‡æ•¸æ“šåº«å¤±æ•—ï¼š{e}")
            raise
    
    return _chroma_instances[vectorstore_type]


# ==================== è¨­å‚™é…ç½® ====================
# è‡ªå‹•æª¢æ¸¬ä¸¦ä½¿ç”¨GPUæˆ–CPUé€²è¡Œå‘é‡è¨ˆç®—
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ åµŒå…¥æ¨¡å‹ä½¿ç”¨è¨­å‚™ï¼š{device.upper()}")


def embed_documents_from_metadata(metadata_list, status_callback=None):
    """
    æ ¹æ“šå…ƒæ•¸æ“šåˆ—è¡¨åµŒå…¥æ–‡æª”
    
    åŠŸèƒ½ï¼š
    1. è®€å–æ–‡æª”ä¸¦é€²è¡Œåˆ†å¡Šè™•ç†
    2. ç‚ºæ¯å€‹æ–‡æª”å¡Šæ·»åŠ å…ƒæ•¸æ“š
    3. å°‡æ–‡æª”å¡Šè½‰æ›ç‚ºå‘é‡ä¸¦å­˜å„²
    4. æä¾›é€²åº¦å›èª¿å’Œçµ±è¨ˆä¿¡æ¯
    
    åƒæ•¸ï¼š
        metadata_list: æ–‡æª”å…ƒæ•¸æ“šåˆ—è¡¨
        status_callback: é€²åº¦å›èª¿å‡½æ•¸
    
    è™•ç†æµç¨‹ï¼š
    1. æ–‡æœ¬åˆ†å‰²ï¼šå°‡æ–‡æª”åˆ†å‰²ç‚ºå°å¡Š
    2. å…ƒæ•¸æ“šæå–ï¼šç‚ºæ¯å€‹å¡Šæ·»åŠ æ¨™è­˜ä¿¡æ¯
    3. å‘é‡åŒ–ï¼šä½¿ç”¨åµŒå…¥æ¨¡å‹è½‰æ›ç‚ºå‘é‡
    4. å­˜å„²ï¼šä¿å­˜åˆ°Chromaå‘é‡æ•¸æ“šåº«
    
    æŠ€è¡“ç´°ç¯€ï¼š
    - ä½¿ç”¨éæ­¸å­—ç¬¦åˆ†å‰²å™¨
    - æ”¯æŒå¤šç¨®åˆ†éš”ç¬¦
    - æ‰¹é‡è™•ç†æé«˜æ•ˆç‡
    - è‡ªå‹•æŒä¹…åŒ–å­˜å„²
    """
    start_time = time.time()
    logger.info(f"ğŸš€ é–‹å§‹å‘é‡åµŒå…¥è™•ç†ï¼Œå…± {len(metadata_list)} å€‹æ–‡ä»¶")
    
    # ==================== æ–‡æœ¬åˆ†å‰²å™¨é…ç½® ====================
    # é…ç½®æ–‡æœ¬åˆ†å‰²åƒæ•¸
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,        # æ¯å€‹å¡Šçš„æœ€å¤§å­—ç¬¦æ•¸
        chunk_overlap=50,      # å¡Šä¹‹é–“çš„é‡ç–Šå­—ç¬¦æ•¸
        separators=["\n\n", "\n", ".", "ã€‚", " ", ""]  # åˆ†å‰²ç¬¦è™Ÿå„ªå…ˆç´š
    )
    
    texts, metadatas = [], []

    # ==================== éšæ®µ1: æ–‡æª”åˆ†å¡Šè™•ç† (40-70%) ====================
    if status_callback:
        status_callback("ğŸ“š é–‹å§‹æ–‡ä»¶åˆ†å¡Šè™•ç†...")
    
    logger.info("ğŸ“š é–‹å§‹æ–‡ä»¶åˆ†å¡Šè™•ç†...")
    chunking_start_time = time.time()
    
    # ä½¿ç”¨tqdmé¡¯ç¤ºé€²åº¦æ¢
    for i, metadata in enumerate(metadata_list):
        file_start_time = time.time()
        filename = metadata.get("new_filename", metadata.get("original_filename", "unknown"))
        title = metadata.get("title", "æœªçŸ¥æ¨™é¡Œ")
        doc_type = metadata.get("type", "unknown")
        tracing_number = metadata.get("tracing_number", "unknown")
        
        logger.info(f"ğŸ“„ è™•ç†ç¬¬ {i+1}/{len(metadata_list)} å€‹æ–‡ä»¶: {filename}")
        logger.info(f"   ğŸ“ æ¨™é¡Œ: {title}")
        logger.info(f"   ğŸ·ï¸ é¡å‹: {doc_type}")
        logger.info(f"   ğŸ”¢ è¿½è¹¤è™Ÿ: {tracing_number}")
        
        # ç²å–æ–‡ä»¶è·¯å¾‘
        file_path = metadata.get("new_path", metadata.get("original_path"))
        if not file_path:
            logger.error(f"âŒ ç„¡æ³•ç²å–æ–‡ä»¶è·¯å¾‘: {filename}")
            continue
        
        # è½‰æ›ç‚ºçµ•å°è·¯å¾‘
        if not os.path.isabs(file_path):
            # å¦‚æœæ˜¯ç›¸å°è·¯å¾‘ï¼Œè½‰æ›ç‚ºçµ•å°è·¯å¾‘
            current_dir = os.getcwd()
            logger.info(f"   ğŸ” ç•¶å‰å·¥ä½œç›®éŒ„: {current_dir}")
            logger.info(f"   ğŸ” ç›¸å°è·¯å¾‘: {file_path}")
            
            # æª¢æŸ¥æ˜¯å¦åœ¨backendç›®éŒ„
            if os.path.basename(current_dir) == "backend":
                # å¦‚æœåœ¨backendç›®éŒ„ï¼Œå‘ä¸Šå…©ç´šåˆ°é …ç›®æ ¹ç›®éŒ„
                project_root = os.path.dirname(os.path.dirname(current_dir))
                # æª¢æŸ¥é …ç›®æ ¹ç›®éŒ„æ˜¯å¦æ­£ç¢º
                if os.path.basename(project_root) == "AI_research_agent":
                    absolute_path = os.path.join(project_root, file_path)
                    logger.info(f"   ğŸ” backendç›®éŒ„ï¼Œé …ç›®æ ¹ç›®éŒ„: {project_root}")
                else:
                    # å¦‚æœä¸åœ¨æ­£ç¢ºçš„é …ç›®çµæ§‹ä¸­ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•
                    # æª¢æŸ¥ç•¶å‰ç›®éŒ„çš„çˆ¶ç›®éŒ„æ˜¯å¦åŒ…å« experiment_data
                    parent_dir = os.path.dirname(current_dir)
                    if os.path.exists(os.path.join(parent_dir, "experiment_data")):
                        absolute_path = os.path.join(parent_dir, file_path)
                        logger.info(f"   ğŸ” ä½¿ç”¨çˆ¶ç›®éŒ„: {parent_dir}")
                    else:
                        # æœ€å¾Œå˜—è©¦ä½¿ç”¨çµ•å°è·¯å¾‘
                        absolute_path = os.path.abspath(file_path)
                        logger.info(f"   ğŸ” ä½¿ç”¨çµ•å°è·¯å¾‘: {absolute_path}")
            else:
                # åœ¨å…¶ä»–ç›®éŒ„ï¼Œç›´æ¥ä½¿ç”¨çµ•å°è·¯å¾‘
                absolute_path = os.path.abspath(file_path)
                logger.info(f"   ğŸ” å…¶ä»–ç›®éŒ„ï¼Œçµ•å°è·¯å¾‘: {absolute_path}")
            
            file_path = absolute_path
        
        logger.info(f"   ğŸ” æœ€çµ‚æ–‡ä»¶è·¯å¾‘: {file_path}")
        
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            # å˜—è©¦åˆ—å‡ºç›®éŒ„å…§å®¹
            try:
                dir_path = os.path.dirname(file_path)
                if os.path.exists(dir_path):
                    files_in_dir = os.listdir(dir_path)
                    logger.info(f"   ğŸ“ ç›®éŒ„ {dir_path} ä¸­çš„æ–‡ä»¶: {files_in_dir}")
                else:
                    logger.error(f"   âŒ ç›®éŒ„ä¸å­˜åœ¨: {dir_path}")
            except Exception as e:
                logger.error(f"   âŒ ç„¡æ³•åˆ—å‡ºç›®éŒ„å…§å®¹: {e}")
            continue
        
        # è¨˜éŒ„æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(file_path)
        logger.info(f"   ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} bytes")
        
        # è®€å–æ–‡ä»¶å…§å®¹
        try:
            read_start_time = time.time()
            doc_chunks = load_and_parse_file(file_path)
            read_end_time = time.time()
            logger.info(f"   âœ… æ–‡ä»¶è®€å–å®Œæˆï¼Œè€—æ™‚: {read_end_time - read_start_time:.2f}ç§’")
            logger.info(f"   ğŸ“„ åŸå§‹æ–‡æœ¬é•·åº¦: {len(doc_chunks)} å­—ç¬¦")
        except FileNotFoundError:
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            continue
        except Exception as e:
            logger.error(f"âŒ è®€å–æ–‡ä»¶å¤±æ•— {file_path}: {e}")
            continue
        
        # å°æ–‡æª”é€²è¡Œåˆ†å¡Šè™•ç†
        try:
            chunk_start_time = time.time()
            chunks = splitter.split_text(doc_chunks)
            chunk_end_time = time.time()
            logger.info(f"   âœ… æ–‡æœ¬åˆ†å¡Šå®Œæˆï¼Œç”Ÿæˆ {len(chunks)} å€‹æ–‡æœ¬å¡Šï¼Œè€—æ™‚: {chunk_end_time - chunk_start_time:.2f}ç§’")
            
            # è¨˜éŒ„åˆ†å¡Šçµ±è¨ˆ
            chunk_sizes = [len(chunk.strip()) for chunk in chunks]
            avg_chunk_size = sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0
            min_chunk_size = min(chunk_sizes) if chunk_sizes else 0
            max_chunk_size = max(chunk_sizes) if chunk_sizes else 0
            logger.info(f"   ğŸ“Š åˆ†å¡Šçµ±è¨ˆ - å¹³å‡: {avg_chunk_size:.1f}, æœ€å°: {min_chunk_size}, æœ€å¤§: {max_chunk_size} å­—ç¬¦")
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬åˆ†å¡Šå¤±æ•— {file_path}: {e}")
            continue
        
        if status_callback:
            status_callback(f"ğŸ“š åˆ†å‰²æ–‡ä»¶ç‚º {len(chunks)} å€‹æ–‡æœ¬å¡Š...")
        
        valid_chunks = 0
        for j, chunk in enumerate(chunks):
            # éæ¿¾éçŸ­çš„æ–‡æœ¬å¡Š
            if len(chunk.strip()) < 10:
                logger.debug(f"   âš ï¸ è·³ééçŸ­æ–‡æœ¬å¡Š {j+1}: {len(chunk.strip())} å­—ç¬¦")
                continue
                
            # ç²å–é ç¢¼ä¿¡æ¯
            try:
                page_num = get_page_number_for_chunk(file_path, chunk)
            except Exception as e:
                logger.warning(f"âš ï¸ ç„¡æ³•ç²å–é ç¢¼ä¿¡æ¯ {file_path}: {e}")
                page_num = "?"
            
            # æ·»åŠ åˆ°è™•ç†åˆ—è¡¨
            texts.append(chunk)
            metadatas.append({
                "source": filename,
                "title": title,
                "type": doc_type,
                "tracing_number": tracing_number,
                "page": page_num,
                "chunk_index": j,
                "total_chunks": len(chunks)
            })
            valid_chunks += 1
        
        file_end_time = time.time()
        logger.info(f"   âœ… æ–‡ä»¶ {filename} è™•ç†å®Œæˆï¼Œæœ‰æ•ˆæ–‡æœ¬å¡Š: {valid_chunks}/{len(chunks)}ï¼Œè€—æ™‚: {file_end_time - file_start_time:.2f}ç§’")
        
        if status_callback:
            status_callback(f"âœ… å®Œæˆæ–‡ä»¶ {filename} çš„åˆ†å¡Šè™•ç†")
    
    chunking_end_time = time.time()
    logger.info(f"âœ… æ‰€æœ‰æ–‡ä»¶åˆ†å¡Šè™•ç†å®Œæˆï¼Œç¸½è€—æ™‚: {chunking_end_time - chunking_start_time:.2f}ç§’")
    logger.info(f"ğŸ“Š ç¸½å…±ç”Ÿæˆ {len(texts)} å€‹æœ‰æ•ˆæ–‡æœ¬å¡Š")
    
    # ==================== éšæ®µ2: å‘é‡åµŒå…¥è™•ç† (70-95%) ====================
    if not texts:
        logger.warning("âš ï¸ æ²’æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å¡Šé€²è¡ŒåµŒå…¥")
        return
    
    if status_callback:
        status_callback(f"ğŸ”¢ é–‹å§‹å‘é‡åµŒå…¥ï¼Œå…± {len(texts)} å€‹æ–‡æœ¬å¡Š...")
    
    logger.info(f"ğŸ”¢ é–‹å§‹å‘é‡åµŒå…¥ï¼Œå…± {len(texts)} å€‹æ–‡æœ¬å¡Š")
    embedding_start_time = time.time()
    
    # æ‰¹é‡åµŒå…¥æ–‡æœ¬å¡Š
    try:
        # ç²å–å‘é‡æ•¸æ“šåº«å¯¦ä¾‹
        logger.info("ğŸ”— ç²å–å‘é‡æ•¸æ“šåº«å¯¦ä¾‹...")
        vectorstore_start_time = time.time()
        vectorstore = get_chroma_instance()
        vectorstore_end_time = time.time()
        logger.info(f"âœ… å‘é‡æ•¸æ“šåº«å¯¦ä¾‹ç²å–å®Œæˆï¼Œè€—æ™‚: {vectorstore_end_time - vectorstore_start_time:.2f}ç§’")
        
        # åˆ†æ‰¹è™•ç†å‘é‡åµŒå…¥ï¼Œæ¯æ‰¹500å€‹æ–‡æœ¬å¡Š
        batch_size = 500
        total_batches = (len(texts) + batch_size - 1) // batch_size
        logger.info(f"ğŸ“¦ å°‡åˆ† {total_batches} æ‰¹é€²è¡Œå‘é‡åµŒå…¥ï¼Œæ¯æ‰¹ {batch_size} å€‹æ–‡æœ¬å¡Š")
        
        for batch_idx in range(total_batches):
            batch_start_time = time.time()
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(texts))
            
            batch_texts = texts[start_idx:end_idx]
            batch_metadatas = metadatas[start_idx:end_idx]
            
            logger.info(f"ğŸ”¢ è™•ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch_texts)} å€‹æ–‡æœ¬å¡Š)...")
            
            if status_callback:
                status_callback(f"ğŸ”¢ å‘é‡åµŒå…¥æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch_texts)} å€‹æ–‡æœ¬å¡Š)...")
            
            # æ‰¹é‡æ·»åŠ æ–‡æª”
            try:
                add_start_time = time.time()
                vectorstore.add_texts(texts=batch_texts, metadatas=batch_metadatas)
                add_end_time = time.time()
                logger.info(f"   âœ… æ‰¹æ¬¡ {batch_idx + 1} å‘é‡æ·»åŠ å®Œæˆï¼Œè€—æ™‚: {add_end_time - add_start_time:.2f}ç§’")
            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_idx + 1} å‘é‡æ·»åŠ å¤±æ•—: {e}")
                raise
            
            batch_end_time = time.time()
            logger.info(f"   âœ… æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} å®Œæˆï¼Œè€—æ™‚: {batch_end_time - batch_start_time:.2f}ç§’")
            
            if status_callback:
                progress_percent = 70 + int(((batch_idx + 1) / total_batches) * 25)  # 70-95%
                status_callback(f"âœ… å®Œæˆæ‰¹æ¬¡ {batch_idx + 1}/{total_batches} çš„å‘é‡åµŒå…¥")
        
        embedding_end_time = time.time()
        logger.info(f"âœ… æ‰€æœ‰æ‰¹æ¬¡å‘é‡åµŒå…¥å®Œæˆï¼Œç¸½è€—æ™‚: {embedding_end_time - embedding_start_time:.2f}ç§’")
        
        if status_callback:
            status_callback(f"âœ… å‘é‡åµŒå…¥å®Œæˆï¼Œå…±è™•ç† {len(texts)} å€‹æ–‡æœ¬å¡Š")
        
        print(f"âœ… å‘é‡åµŒå…¥å®Œæˆï¼Œå…±è™•ç† {len(texts)} å€‹æ–‡æœ¬å¡Š")
        
    except Exception as e:
        logger.error(f"âŒ å‘é‡åµŒå…¥å¤±æ•—: {e}")
        print(f"âŒ å‘é‡åµŒå…¥å¤±æ•—: {e}")
        if status_callback:
            status_callback(f"âŒ å‘é‡åµŒå…¥å¤±æ•—: {e}")
        raise
    
    end_time = time.time()
    total_time = end_time - start_time
    logger.info(f"ğŸ‰ å‘é‡åµŒå…¥è™•ç†å®Œæˆï¼Œç¸½è€—æ™‚: {total_time:.2f}ç§’")
    logger.info(f"ğŸ“Š è™•ç†çµ±è¨ˆ - æ–‡ä»¶æ•¸: {len(metadata_list)}, æ–‡æœ¬å¡Šæ•¸: {len(texts)}, å¹³å‡æ¯æ–‡ä»¶: {len(texts)/len(metadata_list):.1f} å¡Š")


def embed_experiment_txt_batch(txt_paths: List[str], status_callback=None):
    """
    æ‰¹é‡åµŒå…¥å¯¦é©—æ–‡æœ¬æ–‡ä»¶
    
    åŠŸèƒ½ï¼š
    1. è®€å–æŒ‡å®šçš„TXTæ–‡ä»¶åˆ—è¡¨
    2. å°‡æ¯å€‹æ–‡ä»¶ä½œç‚ºä¸€å€‹å®Œæ•´çš„æ–‡æª”å¡Š
    3. æå–å¯¦é©—IDä½œç‚ºæ¨™è­˜ç¬¦
    4. æ‰¹é‡å‘é‡åŒ–ä¸¦å­˜å„²
    
    åƒæ•¸ï¼š
        txt_paths (List[str]): TXTæ–‡ä»¶è·¯å¾‘åˆ—è¡¨
        status_callback: é€²åº¦å›èª¿å‡½æ•¸
    
    ç‰¹é»ï¼š
    - æ¯å€‹TXTæ–‡ä»¶ä½œç‚ºä¸€å€‹å®Œæ•´çš„å¯¦é©—è¨˜éŒ„
    - ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ“´å±•åï¼‰ä½œç‚ºå¯¦é©—ID
    - å°ˆé–€ç”¨æ–¼å¯¦é©—æ•¸æ“šçš„å‘é‡åŒ–
    - å­˜å„²åˆ°ç¨ç«‹çš„å¯¦é©—å‘é‡æ•¸æ“šåº«
    
    æŠ€è¡“ç´°ç¯€ï¼š
    - ä½¿ç”¨ç›¸åŒçš„åµŒå…¥æ¨¡å‹ç¢ºä¿ä¸€è‡´æ€§
    - å­˜å„²åˆ°experiment_vectorç›®éŒ„
    - é›†åˆåç¨±ç‚º"experiment"
    """
    
    # ==================== ç²å–å¯¦é©—å‘é‡æ•¸æ“šåº«å¯¦ä¾‹ ====================
    vectorstore = get_chroma_instance("experiment")

    texts, metadatas = [], []

    # ==================== æ–‡ä»¶è™•ç†å¾ªç’° ====================
    for path in txt_paths:
        # åªè™•ç†TXTæ–‡ä»¶
        if not path.endswith(".txt"):
            continue
            
        # æå–å¯¦é©—IDï¼ˆæ–‡ä»¶åä¸å«æ“´å±•åï¼‰
        exp_id = os.path.splitext(os.path.basename(path))[0]

        # è®€å–æ–‡ä»¶å…§å®¹
        try:
            # å°‡ç›¸å°è·¯å¾‘è½‰æ›ç‚ºçµ•å°è·¯å¾‘é€²è¡Œæ–‡ä»¶è®€å–
            if not os.path.isabs(path):
                current_dir = os.getcwd()
                if os.path.basename(current_dir) == "backend":
                    # å¦‚æœåœ¨ backend ç›®éŒ„ï¼Œå‘ä¸Šå…©ç´šåˆ°é …ç›®æ ¹ç›®éŒ„
                    project_root = os.path.dirname(os.path.dirname(current_dir))
                    if os.path.basename(project_root) == "AI_research_agent":
                        absolute_path = os.path.join(project_root, path)
                    else:
                        # å¦‚æœä¸åœ¨æ­£ç¢ºçš„é …ç›®çµæ§‹ä¸­ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•
                        parent_dir = os.path.dirname(current_dir)
                        if os.path.exists(os.path.join(parent_dir, "experiment_data")):
                            absolute_path = os.path.join(parent_dir, path)
                        else:
                            absolute_path = os.path.abspath(path)
                else:
                    absolute_path = os.path.abspath(path)
            else:
                absolute_path = path
            
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(absolute_path):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {absolute_path}")
                continue
            
            with open(absolute_path, "r", encoding="utf-8") as f:
                content = f.read().strip()
        except Exception as e:
            print(f"âŒ è®€å–æ–‡ä»¶å¤±æ•— {path}: {e}")
            continue
            
        # éæ¿¾éçŸ­çš„å…§å®¹
        if len(content) < 10:
            continue

        # æ·»åŠ åˆ°è™•ç†åˆ—è¡¨
        texts.append(content)
        metadatas.append({
            "type": "experiment",
            "exp_id": exp_id,
            "filename": os.path.basename(path),
        })

    # ==================== é©—è­‰è™•ç†çµæœ ====================
    if not texts:
        if status_callback:
            status_callback("âš ï¸ æ²’æœ‰æ–°çš„å¯¦é©—æ‘˜è¦å¯åµŒå…¥")
        return

    # ==================== æ‰¹é‡å‘é‡åŒ– ====================
    try:
        vectorstore.add_texts(texts=texts, metadatas=metadatas)
        # vectorstore.persist()  # å·²æ£„ç”¨ï¼Œè‡ªå‹•æŒä¹…åŒ–
    except Exception as e:
        print(f"âŒ å¯¦é©—æ•¸æ“šåµŒå…¥å¤±æ•—: {e}")
        if status_callback:
            status_callback(f"âŒ å¯¦é©—æ•¸æ“šåµŒå…¥å¤±æ•—: {e}")
        return

    # ==================== çµ±è¨ˆä¿¡æ¯ ====================
    try:
        docs = vectorstore.get(include=["documents"])
        print("ğŸ“¦ å‘é‡æ•¸é‡ï¼š", len(docs["documents"]))
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å–å¯¦é©—å‘é‡åº«çµ±è¨ˆä¿¡æ¯: {e}")
        docs = {"documents": []}

    # ==================== é€²åº¦å›èª¿ ====================
    if status_callback:
        status_callback(f"âœ… åµŒå…¥å®Œæˆï¼Œå…± {len(texts)} ç­†å¯¦é©—æ‘˜è¦")

    # ==================== è©³ç´°é è¦½ ====================
    print("ğŸ“Š æœ¬æ¬¡åµŒå…¥é è¦½ï¼š")
    for i, t in enumerate(texts[:5]):
        try:
            print(f"#{i+1} | {metadatas[i]['exp_id']} | é ­ 80 å­—ï¼š{t[:80].replace(chr(10), ' ')}")
        except Exception as e:
            print(f"#{i+1} | é è¦½é¡¯ç¤ºå¤±æ•—: {e}")


# ==================== è¼”åŠ©å‡½æ•¸ ====================

def get_vectorstore(vectorstore_type: str = "paper"):
    """
    ç²å–å‘é‡æ•¸æ“šåº«å¯¦ä¾‹ï¼ˆå·²æ£„ç”¨ï¼Œè«‹ä½¿ç”¨ get_chroma_instanceï¼‰
    
    åŠŸèƒ½ï¼š
    1. ç²å–æˆ–å‰µå»ºæŒ‡å®šçš„å‘é‡æ•¸æ“šåº«å¯¦ä¾‹
    2. ä½¿ç”¨ç·©å­˜é¿å…é‡è¤‡å‰µå»º
    
    åƒæ•¸ï¼š
        vectorstore_type (str): å‘é‡æ•¸æ“šåº«é¡å‹ï¼ˆ"paper" æˆ– "experiment"ï¼‰
    
    è¿”å›ï¼š
        Chroma: å‘é‡æ•¸æ“šåº«å¯¦ä¾‹
    """
    return get_chroma_instance(vectorstore_type)


def validate_embedding_model():
    """
    é©—è­‰åµŒå…¥æ¨¡å‹æ˜¯å¦å¯ç”¨
    
    åŠŸèƒ½ï¼š
    1. æª¢æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    2. æ¸¬è©¦æ¨¡å‹åŠ è¼‰
    3. é©—è­‰è¨­å‚™é…ç½®
    
    è¿”å›ï¼š
        bool: æ¨¡å‹æ˜¯å¦å¯ç”¨
    """
    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={"trust_remote_code": True, "device": device}
        )
        print(f"âœ… åµŒå…¥æ¨¡å‹é©—è­‰æˆåŠŸï¼š{EMBEDDING_MODEL_NAME}")
        return True
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹é©—è­‰å¤±æ•—ï¼š{e}")
        return False


def get_vectorstore_stats(vectorstore_type: str = "paper"):
    """
    ç²å–å‘é‡æ•¸æ“šåº«çµ±è¨ˆä¿¡æ¯
    
    åŠŸèƒ½ï¼š
    1. é€£æ¥æŒ‡å®šçš„å‘é‡æ•¸æ“šåº«
    2. ç²å–æ–‡æª”æ•¸é‡çµ±è¨ˆ
    3. è¿”å›è©³ç´°çš„çµ±è¨ˆä¿¡æ¯
    
    åƒæ•¸ï¼š
        vectorstore_type (str): å‘é‡æ•¸æ“šåº«é¡å‹ï¼ˆ"paper" æˆ– "experiment"ï¼‰
    
    è¿”å›ï¼š
        Dict: çµ±è¨ˆä¿¡æ¯å­—å…¸
    """
    try:
        vectorstore = get_chroma_instance(vectorstore_type)
        docs = vectorstore.get(include=["documents", "metadatas"])
        
        if vectorstore_type == "paper":
            vector_dir = os.path.join(VECTOR_INDEX_DIR, "paper_vector")
            collection_name = "paper"
        else:
            vector_dir = os.path.join(VECTOR_INDEX_DIR, "experiment_vector")
            collection_name = "experiment"
        
        return {
            "total_documents": len(docs["documents"]),
            "collection_name": collection_name,
            "vector_dir": vector_dir
        }
        
    except Exception as e:
        print(f"âŒ ç²å–çµ±è¨ˆä¿¡æ¯å¤±æ•—ï¼š{e}")
        return {"error": str(e)}


# ==================== æ¸¬è©¦ä»£ç¢¼ ====================
if __name__ == "__main__":
    """
    æ¸¬è©¦åµŒå…¥åŠŸèƒ½
    
    é€™å€‹æ¸¬è©¦ä»£ç¢¼ç”¨æ–¼é©—è­‰åµŒå…¥åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
    """
    print("ğŸ§ª é–‹å§‹æ¸¬è©¦åµŒå…¥åŠŸèƒ½...")
    
    # é©—è­‰åµŒå…¥æ¨¡å‹
    if validate_embedding_model():
        print("âœ… åµŒå…¥æ¨¡å‹é©—è­‰é€šé")
        
        # ç²å–çµ±è¨ˆä¿¡æ¯
        paper_stats = get_vectorstore_stats("paper")
        experiment_stats = get_vectorstore_stats("experiment")
        
        print("ğŸ“Š å‘é‡æ•¸æ“šåº«çµ±è¨ˆï¼š")
        print(f"  æ–‡ç»å‘é‡åº«ï¼š{paper_stats}")
        print(f"  å¯¦é©—å‘é‡åº«ï¼š{experiment_stats}")
    else:
        print("âŒ åµŒå…¥æ¨¡å‹é©—è­‰å¤±æ•—")


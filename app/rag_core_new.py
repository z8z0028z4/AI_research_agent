"""
RAGæ ¸å¿ƒæ¨¡çµ„ - ç°¡åŒ–ç‰ˆæœ¬
========

åŸºæ–¼æª¢ç´¢å¢å¼·ç”Ÿæˆçš„AIç ”ç©¶åŠ©æ‰‹æ ¸å¿ƒåŠŸèƒ½
åªæ”¯æ´ GPT-5 å’Œçµæ§‹åŒ–è¼¸å‡º
"""

import os
import json
import time
from typing import List, Dict, Any, Tuple
from pathlib import Path

# å°å…¥å¿…è¦çš„æ¨¡çµ„
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from chunk_embedding import get_chroma_instance
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
import openai

# å°å…¥é…ç½®å’Œæ©‹æ¥æ¨¡çµ„
try:
    from .config import (
        OPENAI_API_KEY, 
        VECTOR_INDEX_DIR, 
        EMBEDDING_MODEL_NAME,
        MAX_TOKENS,
        CHUNK_SIZE,
        CHUNK_OVERLAP
    )
    from .model_config_bridge import get_model_params, get_current_model
except ImportError:
    from config import (
        OPENAI_API_KEY, 
        VECTOR_INDEX_DIR, 
        EMBEDDING_MODEL_NAME,
        MAX_TOKENS,
        CHUNK_SIZE,
        CHUNK_OVERLAP
    )
    from model_config_bridge import get_model_params, get_current_model

# è¨­å®šOpenAI API Key
openai.api_key = OPENAI_API_KEY

def get_dynamic_schema_params():
    """
    å¾è¨­å®šç®¡ç†å™¨ç²å–å‹•æ…‹çš„ JSON Schema åƒæ•¸
    """
    try:
        import sys
        backend_path = os.path.join(os.path.dirname(__file__), "..", "backend")
        if backend_path not in sys.path:
            sys.path.insert(0, backend_path)
        
        try:
            from backend.core.settings_manager import settings_manager
        except ImportError:
            from core.settings_manager import settings_manager
        
        json_schema_params = settings_manager.get_json_schema_parameters()
        
        return {
            "min_length": json_schema_params.get("min_length", 5),
            "max_length": json_schema_params.get("max_length", 100)
        }
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å–å‹•æ…‹ schema åƒæ•¸ï¼Œä½¿ç”¨é è¨­å€¼: {e}")
        return {
            "min_length": 5,
            "max_length": 100
        }

def create_research_proposal_schema():
    """
    å‹•æ…‹å‰µå»ºç ”ç©¶ææ¡ˆçš„ JSON Schema
    """
    schema_params = get_dynamic_schema_params()
    
    return {
        "type": "object",
        "title": "ResearchProposal",
        "additionalProperties": False,
        "required": [
            "proposal_title",
            "need",
            "solution", 
            "differentiation",
            "benefit",
            "experimental_overview",
            "materials_list"
        ],
        "properties": {
            "proposal_title": {
                "type": "string",
                "description": "ç ”ç©¶ææ¡ˆçš„æ¨™é¡Œï¼Œç¸½çµç ”ç©¶ç›®æ¨™å’Œå‰µæ–°é»",
                "minLength": 10
            },
            "need": {
                "type": "string", 
                "description": "ç ”ç©¶éœ€æ±‚å’Œç¾æœ‰è§£æ±ºæ–¹æ¡ˆçš„å±€é™æ€§ï¼Œæ˜ç¢ºéœ€è¦è§£æ±ºçš„æŠ€è¡“ç“¶é ¸",
                "minLength": 10
            },
            "solution": {
                "type": "string",
                "description": "å…·é«”çš„è¨­è¨ˆå’Œé–‹ç™¼ç­–ç•¥ï¼ŒåŒ…æ‹¬æ–°çš„çµæ§‹ã€çµ„æˆæˆ–æ–¹æ³•",
                "minLength": 10
            },
            "differentiation": {
                "type": "string",
                "description": "èˆ‡ç¾æœ‰æ–‡ç»æˆ–æŠ€è¡“çš„æ¯”è¼ƒï¼Œå¼·èª¿çµæ§‹ã€æ€§èƒ½æˆ–å¯¦æ–½æ–¹é¢çš„çªç ´",
                "minLength": 10
            },
            "benefit": {
                "type": "string",
                "description": "é æœŸçš„æ€§èƒ½æ”¹é€²æˆ–æ‡‰ç”¨ç¯„åœæ“´å±•ï¼Œç›¡å¯èƒ½æä¾›å®šé‡ä¼°è¨ˆ",
                "minLength": 10
            },
            "experimental_overview": {
                "type": "string",
                "description": "å¯¦é©—æ¦‚è¿°ï¼ŒåŒ…æ‹¬èµ·å§‹ææ–™ã€æ¢ä»¶ã€å„€å™¨è¨­å‚™å’Œæ­¥é©Ÿæè¿°",
                "minLength": 10
            },
            "materials_list": {
                "type": "array",
                "items": {
                    "type": "string"
                },
                "description": "CRITICAL: åƒ…åˆ—å‡ºIUPACåŒ–å­¸å“åç¨±ï¼Œæ¯å€‹é …ç›®å¿…é ˆæ˜¯å–®ä¸€åŒ–å­¸å“åç¨±ï¼Œä¸åŒ…å«ä»»ä½•æè¿°ã€å‚™è¨»ã€æ‹¬è™Ÿèªªæ˜æˆ–å…¶ä»–æ–‡å­—ã€‚åš´ç¦åŒ…å«å¦‚'(dobpdc)'ã€'(representative...)'ã€'(trifluoromethyl-substituted...)'ç­‰æ‹¬è™Ÿå…§å®¹ã€‚æ¯å€‹åŒ–å­¸å“åç¨±å¿…é ˆæ˜¯æ¨™æº–çš„IUPACå‘½åï¼Œä¾‹å¦‚ï¼šmagnesium nitrate hexahydrate, 4,4'-dioxidobiphenyl-3,3'-dicarboxylic acid, 2-(2,2,2-trifluoroethylamino)ethan-1-amine, N,N-dimethylacetamide",
                "minItems": 1
            }
        }
    }

def create_experimental_detail_schema():
    """
    å‹•æ…‹å‰µå»ºå¯¦é©—ç´°ç¯€çš„ JSON Schema
    """
    schema_params = get_dynamic_schema_params()
    
    return {
        "type": "object",
        "title": "ExperimentalDetail",
        "additionalProperties": False,
        "required": [
            "synthesis_process",
            "materials_and_conditions",
            "analytical_methods",
            "precautions"
        ],
        "properties": {
            "synthesis_process": {
                "type": "string",
                "description": "è©³ç´°çš„åˆæˆéç¨‹ï¼ŒåŒ…æ‹¬æ­¥é©Ÿã€æ¢ä»¶ã€æ™‚é–“ç­‰",
                "minLength": 10
            },
            "materials_and_conditions": {
                "type": "string",
                "description": "ä½¿ç”¨çš„ææ–™å’Œåæ‡‰æ¢ä»¶ï¼ŒåŒ…æ‹¬æ¿ƒåº¦ã€æº«åº¦ã€å£“åŠ›ç­‰",
                "minLength": 10
            },
            "analytical_methods": {
                "type": "string",
                "description": "åˆ†ææ–¹æ³•å’Œè¡¨å¾µæŠ€è¡“ï¼Œå¦‚XRDã€SEMã€NMRç­‰",
                "minLength": 10
            },
            "precautions": {
                "type": "string",
                "description": "å¯¦é©—æ³¨æ„äº‹é …å’Œå®‰å…¨é é˜²æªæ–½",
                "minLength": 10
            }
        }
    }

def create_revision_explain_schema():
    """
    å‹•æ…‹å‰µå»ºä¿®è¨‚èªªæ˜çš„ JSON Schema
    """
    schema_params = get_dynamic_schema_params()
    
    return {
        "type": "object",
        "title": "RevisionExplain",
        "additionalProperties": False,
        "required": [
            "revision_explain"
        ],
        "properties": {
            "revision_explain": {
                "type": "string",
                "description": "è©³ç´°èªªæ˜ä¿®è¨‚çš„åŸå› ã€æ”¹é€²é»å’Œæ–°çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬æŠ€è¡“å‰µæ–°é»å’Œé æœŸæ•ˆæœ",
                "minLength": 10
            }
        }
    }

def create_revision_proposal_schema():
    """
    å‹•æ…‹å‰µå»ºä¿®è¨‚ææ¡ˆçš„ JSON Schema (åŒ…å«ä¿®è¨‚èªªæ˜)
    """
    schema_params = get_dynamic_schema_params()
    
    return {
        "type": "object",
        "title": "RevisionProposal",
        "additionalProperties": False,
        "required": [
            "revision_explanation",
            "proposal_title",
            "need",
            "solution", 
            "differentiation",
            "benefit",
            "experimental_overview",
            "materials_list"
        ],
        "properties": {
            "revision_explanation": {
                "type": "string",
                "description": "ç°¡è¦èªªæ˜ä¿®è¨‚çš„é‚è¼¯å’Œä¸»è¦æ”¹é€²é»ï¼ŒåŸºæ–¼ç”¨æˆ¶åé¥‹å°åŸå§‹ææ¡ˆçš„ä¿®æ”¹åŸå› ",
                "minLength": 10
            },
            "proposal_title": {
                "type": "string",
                "description": "ç ”ç©¶ææ¡ˆçš„æ¨™é¡Œï¼Œç¸½çµç ”ç©¶ç›®æ¨™å’Œå‰µæ–°é»",
                "minLength": 10
            },
            "need": {
                "type": "string", 
                "description": "ç ”ç©¶éœ€æ±‚å’Œç¾æœ‰è§£æ±ºæ–¹æ¡ˆçš„å±€é™æ€§ï¼Œæ˜ç¢ºéœ€è¦è§£æ±ºçš„æŠ€è¡“ç“¶é ¸",
                "minLength": 10
            },
            "solution": {
                "type": "string",
                "description": "å…·é«”çš„è¨­è¨ˆå’Œé–‹ç™¼ç­–ç•¥ï¼ŒåŒ…æ‹¬æ–°çš„çµæ§‹ã€çµ„æˆæˆ–æ–¹æ³•",
                "minLength": 10
            },
            "differentiation": {
                "type": "string",
                "description": "èˆ‡ç¾æœ‰æ–‡ç»æˆ–æŠ€è¡“çš„æ¯”è¼ƒï¼Œå¼·èª¿çµæ§‹ã€æ€§èƒ½æˆ–å¯¦æ–½æ–¹é¢çš„çªç ´",
                "minLength": 10
            },
            "benefit": {
                "type": "string",
                "description": "é æœŸçš„æ€§èƒ½æ”¹é€²æˆ–æ‡‰ç”¨ç¯„åœæ“´å±•ï¼Œç›¡å¯èƒ½æä¾›å®šé‡ä¼°è¨ˆ",
                "minLength": 10
            },
            "experimental_overview": {
                "type": "string",
                "description": "å¯¦é©—æ¦‚è¿°ï¼ŒåŒ…æ‹¬èµ·å§‹ææ–™ã€æ¢ä»¶ã€å„€å™¨è¨­å‚™å’Œæ­¥é©Ÿæè¿°",
                "minLength": 10
            },
            "materials_list": {
                "type": "array",
                "items": {
                    "type": "string"
                },
                "description": "CRITICAL: åƒ…åˆ—å‡ºIUPACåŒ–å­¸å“åç¨±ï¼Œæ¯å€‹é …ç›®å¿…é ˆæ˜¯å–®ä¸€åŒ–å­¸å“åç¨±ï¼Œä¸åŒ…å«ä»»ä½•æè¿°ã€å‚™è¨»ã€æ‹¬è™Ÿèªªæ˜æˆ–å…¶ä»–æ–‡å­—ã€‚åš´ç¦åŒ…å«å¦‚'(dobpdc)'ã€'(representative...)'ã€'(trifluoromethyl-substituted...)'ç­‰æ‹¬è™Ÿå…§å®¹ã€‚æ¯å€‹åŒ–å­¸å“åç¨±å¿…é ˆæ˜¯æ¨™æº–çš„IUPACå‘½åï¼Œä¾‹å¦‚ï¼šmagnesium nitrate hexahydrate, 4,4'-dioxidobiphenyl-3,3'-dicarboxylic acid, 2-(2,2,2-trifluoroethylamino)ethan-1-amine, N,N-dimethylacetamide",
                "minItems": 1
            }
        }
    }

# å‹•æ…‹ç”Ÿæˆ JSON Schema
RESEARCH_PROPOSAL_SCHEMA = create_research_proposal_schema()
EXPERIMENTAL_DETAIL_SCHEMA = create_experimental_detail_schema()
REVISION_EXPLAIN_SCHEMA = create_revision_explain_schema()
REVISION_PROPOSAL_SCHEMA = create_revision_proposal_schema()

# ==================== å‘é‡æ•¸æ“šåº«ç®¡ç† ====================

def load_paper_vectorstore():
    """
    è¼‰å…¥æ–‡ç»å‘é‡æ•¸æ“šåº«
    """
    return get_chroma_instance("paper")

def load_experiment_vectorstore():
    """
    è¼‰å…¥å¯¦é©—æ•¸æ“šå‘é‡æ•¸æ“šåº«
    """
    return get_chroma_instance("experiment")

# ==================== æ–‡æª”æª¢ç´¢åŠŸèƒ½ ====================

def retrieve_chunks_multi_query(
    vectorstore, query_list: List[str], k: int = 10, fetch_k: int = 20, score_threshold: float = 0.35
    ) -> List[Document]:
    """
    å¤šæŸ¥è©¢æ–‡æª”æª¢ç´¢åŠŸèƒ½
    """
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": fetch_k, "score_threshold": score_threshold}
    )

    # ä½¿ç”¨å­—å…¸é€²è¡Œå»é‡
    chunk_dict = {}
    print("ğŸ” æŸ¥è©¢åˆ—è¡¨ï¼š", query_list)
    
    # å°æ¯å€‹æŸ¥è©¢é€²è¡Œæª¢ç´¢
    for q in query_list:
        docs = retriever.get_relevant_documents(q)
        for doc in docs:
            # ä½¿ç”¨å”¯ä¸€æ¨™è­˜ç¬¦é€²è¡Œå»é‡
            key = doc.metadata.get("exp_id") or doc.metadata.get("source") or doc.page_content[:30]
            chunk_dict[key] = doc
    
    # é™åˆ¶è¿”å›çµæœæ•¸é‡ï¼Œä½¿ç”¨å‚³å…¥çš„ k åƒæ•¸
    result = list(chunk_dict.values())[:k]

    # æª¢ç´¢çµæœé©—è­‰
    if not result:
        print("âš ï¸ æ²’æœ‰æª¢ç´¢åˆ°ä»»ä½•æ–‡æª”ï¼Œå»ºè­°æª¢æŸ¥æª¢ç´¢å™¨æˆ–åµŒå…¥æ ¼å¼")
    else:
        print(f"ğŸ” å¤šæŸ¥è©¢æª¢ç´¢å…±æ‰¾åˆ° {len(result)} å€‹æ–‡æª”ï¼š")
        print("ğŸ” æª¢ç´¢åˆ°çš„æ–‡æª”é è¦½ï¼š")
        for i, doc in enumerate(result[:5], 1):
            meta = doc.metadata
            filename = meta.get("filename") or meta.get("source", "Unknown")
            page = meta.get("page_number") or meta.get("page", "?")
            preview = doc.page_content[:80].replace("\n", " ")
            print(f"--- æ–‡æª” {i} ---")
            print(f"ğŸ“„ {filename} | é ç¢¼ {page}")
            print(f"ğŸ“ å…§å®¹é è¦½ï¼š{preview}")
            print()

    return result

def preview_chunks(chunks: List[Document], title: str, max_preview: int = 5):
    """
    é è¦½æ–‡æª”å¡Šå…§å®¹
    """
    print(f"\nğŸ“¦ã€{title}ã€‘å…±æ‰¾åˆ° {len(chunks)} å€‹æ–‡æª”å¡Š")

    if not chunks:
        print("âš ï¸ æ²’æœ‰ä»»ä½•æ®µè½å¯é¡¯ç¤ºã€‚")
        return

    # é¡¯ç¤ºå‰å¹¾å€‹æ–‡æª”å¡Šçš„è©³ç´°ä¿¡æ¯
    for i, doc in enumerate(chunks[:max_preview], 1):
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        preview = doc.page_content[:100].replace("\n", " ")
        print(f"--- Chunk {i} ---")
        print(f"ğŸ“„ Filename: {filename} | Page: {page}")
        print(f"ğŸ“š Preview: {preview}")

# ==================== LLM èª¿ç”¨åŠŸèƒ½ ====================

def call_llm(prompt: str) -> str:
    """
    èª¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬ - åªæ”¯æ´ GPT-5
    """
    print(f"ğŸ” èª¿ç”¨ LLMï¼Œæç¤ºè©é•·åº¦ï¼š{len(prompt)} å­—ç¬¦")
    
    # ç²å–ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯å’Œåƒæ•¸
    try:
        from model_config_bridge import get_current_model, get_model_params
        current_model = get_current_model()
        llm_params = get_model_params()
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        print(f"ğŸ”§ æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        print(f"âŒ ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        raise Exception(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{str(e)}")
    
    try:
        # åªæ”¯æ´ GPT-5 ç³»åˆ—ä½¿ç”¨ Responses API
        if not current_model.startswith('gpt-5'):
            raise Exception(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
            
        from openai import OpenAI
        client = OpenAI()
        
        # æº–å‚™Responses APIçš„åƒæ•¸
        max_tokens = llm_params.get('max_output_tokens', llm_params.get('max_tokens', 2000))
        print(f"ğŸ”§ ä½¿ç”¨è¨­å®šçš„max_output_tokens: {max_tokens}")
        
        responses_params = {
            'model': current_model,
            'input': [{'role': 'user', 'content': prompt}],
            'max_output_tokens': max_tokens
        }
        
        # æ·»åŠ å…¶ä»–åƒæ•¸
        for key, value in llm_params.items():
            if key not in ['model', 'input', 'max_output_tokens', 'max_tokens', 'verbosity', 'reasoning_effort']:
                responses_params[key] = value
        
        # ç‰¹æ®Šè™•ç†verbosityå’Œreasoning_effort
        if 'text' in llm_params:
            responses_params['text'] = llm_params['text']
        if 'reasoning' in llm_params:
            responses_params['reasoning'] = llm_params['reasoning']
        
        print(f"ğŸ”§ ä½¿ç”¨Responses APIï¼Œåƒæ•¸ï¼š{responses_params}")
        
        # è™•ç†GPT-5çš„incompleteç‹€æ…‹
        max_retries = 3
        retry_count = 0
        current_max_tokens = max_tokens
        
        while retry_count < max_retries:
            if retry_count > 0:
                current_max_tokens += 1500
                responses_params['max_output_tokens'] = current_max_tokens
                print(f"ğŸ”„ é‡è©¦ {retry_count}ï¼šæé«˜max_output_tokensåˆ° {current_max_tokens}")
            
            response = client.responses.create(**responses_params)
            
            # æª¢æŸ¥æ•´é«”responseç‹€æ…‹
            if hasattr(response, 'status') and response.status == 'incomplete':
                print(f"âš ï¸ æª¢æ¸¬åˆ°incompleteç‹€æ…‹ï¼Œç­‰å¾…å¾Œé‡è©¦...")
                retry_count += 1
                if retry_count < max_retries:
                    time.sleep(2)
                    continue
                else:
                    print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
            
            # æå–æ–‡æœ¬å…§å®¹ï¼ˆåªä½¿ç”¨ output_textï¼‰
            if getattr(response, "output_text", None):
                txt = response.output_text.strip()
                if txt:
                    print(f"âœ… ä½¿ç”¨ output_text: {len(txt)} å­—ç¬¦")
                    print(f"âœ… LLM èª¿ç”¨æˆåŠŸï¼Œå›æ‡‰é•·åº¦ï¼š{len(txt)} å­—ç¬¦")
                    return txt
                else:
                    print(f"âŒ output_text ç‚ºç©º")
                    retry_count += 1
                    if retry_count < max_retries:
                        continue
                    else:
                        print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                        return ""
            else:
                print(f"âŒ ç„¡æ³•æå– output_text")
                retry_count += 1
                if retry_count < max_retries:
                    continue
                else:
                    print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                    return ""
        
        print(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
        return ""
        
    except Exception as e:
        print(f"âŒ LLM èª¿ç”¨å¤±æ•—ï¼š{e}")
        raise Exception(f"LLM èª¿ç”¨å¤±æ•—ï¼š{str(e)}")

def call_llm_structured_proposal(system_prompt: str, user_prompt: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–ç ”ç©¶ææ¡ˆ
    """
    print(f"ğŸ” èª¿ç”¨çµæ§‹åŒ–LLMï¼Œç³»çµ±æç¤ºè©é•·åº¦ï¼š{len(system_prompt)} å­—ç¬¦")
    print(f"ğŸ” ç”¨æˆ¶æç¤ºè©é•·åº¦ï¼š{len(user_prompt)} å­—ç¬¦")
    
    # ç²å–ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯å’Œåƒæ•¸
    try:
        from model_config_bridge import get_current_model, get_model_params
        current_model = get_current_model()
        llm_params = get_model_params()
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        print(f"ğŸ”§ æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        print(f"âŒ ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        raise Exception(f"ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{str(e)}")
    
    try:
        # åªæ”¯æ´ GPT-5 ç³»åˆ—ä½¿ç”¨ Responses API
        if not current_model.startswith('gpt-5'):
            raise Exception(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
            
        from openai import OpenAI
        client = OpenAI()
        
        # æº–å‚™Responses APIçš„åƒæ•¸
        max_tokens = llm_params.get('max_output_tokens', llm_params.get('max_tokens', 4000))
        
        # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
        current_schema = create_research_proposal_schema()
        
        # ä½¿ç”¨ Responses API + JSON Schema
        responses_params = {
            'model': current_model,
            'input': [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            'text': {
                'format': {
                    'type': 'json_schema',
                    'name': 'ResearchProposal',
                    'strict': True,
                    'schema': current_schema,
                },
                'verbosity': 'low'
            },
            'reasoning': {'effort': 'medium'},
            'max_output_tokens': max_tokens
        }
        
        print(f"ğŸ”§ ä½¿ç”¨Responses API with JSON Schemaï¼Œåƒæ•¸ï¼š{responses_params}")
        
        # è™•ç†GPT-5çš„incompleteç‹€æ…‹
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            response = client.responses.create(**responses_params)
            
            print(f"ğŸ” DEBUG: APIèª¿ç”¨å®Œæˆ (å˜—è©¦ {retry_count + 1}/{max_retries})")
            print(f"ğŸ” DEBUG: response.status: {getattr(response, 'status', 'N/A')}")
            
            # æª¢æŸ¥æ•´é«”responseç‹€æ…‹
            if hasattr(response, 'status') and response.status == 'incomplete':
                print(f"âš ï¸ æª¢æ¸¬åˆ°incompleteç‹€æ…‹ï¼Œç­‰å¾…å¾Œé‡è©¦...")
                retry_count += 1
                if retry_count < max_retries:
                    time.sleep(2)
                    continue
                else:
                    print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                    return {}
            
            # æå–JSONå…§å®¹
            try:
                output_text = getattr(response, 'output_text', None)
                if output_text:
                    print(f"âœ… ä½¿ç”¨ resp.output_text æå–å…§å®¹: {len(output_text)} å­—ç¬¦")
                    try:
                        proposal_data = json.loads(output_text)
                        print(f"âœ… æˆåŠŸè§£æJSONçµæ§‹åŒ–ææ¡ˆ")
                        
                        # æœ¬åœ° JSON Schema é©—è­‰
                        try:
                            from jsonschema import Draft202012Validator
                            from jsonschema.exceptions import ValidationError
                            Draft202012Validator(current_schema).validate(proposal_data)
                            print("âœ… æœ¬åœ° Schema é©—è­‰é€šé")
                        except ImportError:
                            print("âš ï¸ jsonschema æœªå®‰è£ï¼Œè·³éæœ¬åœ°é©—è­‰")
                        except ValidationError as e:
                            print(f"âš ï¸ æœ¬åœ° Schema é©—è­‰å¤±æ•—: {e}")
                        
                        return proposal_data
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ JSONè§£æå¤±æ•—: {e}")
                        print(f"âš ï¸ å˜—è©¦çš„æ–‡æœ¬: {output_text[:200]}...")
                        return {}
                
                # å¦‚æœæ²’æœ‰æ‰¾åˆ°JSONå…§å®¹
                print(f"âš ï¸ ç„¡æ³•å¾Responses APIæå–JSONå…§å®¹")
                return {}
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æå¤±æ•—: {e}")
                retry_count += 1
                if retry_count < max_retries:
                    time.sleep(2)
                    continue
                else:
                    print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                    return {}
            except Exception as e:
                print(f"âŒ æå–JSONå…§å®¹å¤±æ•—: {e}")
                retry_count += 1
                if retry_count < max_retries:
                    time.sleep(2)
                    continue
                else:
                    print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                    return {}
        
        print(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œè¿”å›ç©ºå­—å…¸")
        return {}
        
    except Exception as e:
        print(f"âŒ çµæ§‹åŒ–LLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        raise Exception(f"çµæ§‹åŒ–LLMèª¿ç”¨å¤±æ•—ï¼š{str(e)}")

# ==================== æç¤ºè©æ§‹å»ºåŠŸèƒ½ ====================

def build_prompt(chunks: List[Document], question: str) -> Tuple[str, List[Dict]]:
    """
    æ§‹å»ºæç¤ºè©
    """
    context_text = ""
    citations = []
    citation_map = {}
    
    for i, doc in enumerate(chunks):
        metadata = doc.metadata
        title = metadata.get("title", "Untitled")
        filename = metadata.get("filename") or metadata.get("source", "Unknown")
        page = metadata.get("page_number") or metadata.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")

        citation_key = f"{filename}_p{page}"
        if citation_key not in citation_map:
            label = f"[{len(citations) + 1}]"
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
            citation_map[citation_key] = label
        else:
            label = citation_map[citation_key]

        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
    You are a research literature search assistant. Please answer questions based only on the provided literature excerpts.
    Please use [1], [2], etc. to cite paragraph sources in your answers, and do not repeat the sources at the end.
    If the paragraphs mention specific experimental conditions (temperature, time, etc.), please be sure to include them in your answer.
    Important: You can only cite the provided literature excerpts. The current literature excerpt numbers are [1] to [{len(chunks)}] (total {len(chunks)} excerpts)

    --- Literature Summary ---
    {context_text}

    --- Question ---
    {question}
    """
    return system_prompt.strip(), citations

def expand_query(user_prompt: str) -> List[str]:
    """
    å°‡ç”¨æˆ¶è¼¸å…¥çš„è‡ªç„¶èªè¨€å•é¡Œè½‰æ›ç‚ºå¤šå€‹èªç¾©æœç´¢æŸ¥è©¢èªå¥
    """
    try:
        from model_config_bridge import get_current_model, get_model_params
        current_model = get_current_model()
        llm_params = get_model_params()
    except Exception as e:
        print(f"âŒ ç„¡æ³•ç²å–æ¨¡å‹åƒæ•¸ï¼š{e}")
        raise Exception(f"ç„¡æ³•ç²å–æ¨¡å‹åƒæ•¸ï¼š{str(e)}")

    system_prompt = """You are a scientific assistant helping expand a user's synthesis question into multiple semantic search queries. 
    Each query should be precise, relevant, and useful for retrieving related technical documents. 
    Only return a list of 3 to 6 search queries in English. Do not explain, do not include numbering if not needed."""

    full_prompt = f"{system_prompt}\n\nUser question:\n{user_prompt}"

    try:
        # åªæ”¯æ´ GPT-5 ç³»åˆ—ä½¿ç”¨ Responses API
        if not current_model.startswith('gpt-5'):
            raise Exception(f"ä¸æ”¯æ´çš„æ¨¡å‹ï¼š{current_model}ï¼Œåªæ”¯æ´ GPT-5 ç³»åˆ—")
            
        from openai import OpenAI
        client = OpenAI()
        
        # æº–å‚™Responses APIçš„åƒæ•¸
        responses_params = {
            'model': current_model,
            'input': [{'role': 'user', 'content': full_prompt}]
        }
        
        # æ·»åŠ å…¶ä»–åƒæ•¸ï¼ˆæ’é™¤modelå’Œinputï¼‰
        for key, value in llm_params.items():
            if key not in ['model', 'input']:
                responses_params[key] = value
        
        # ç§»é™¤reasoningåƒæ•¸ï¼Œé¿å…è¿”å›ResponseReasoningItem
        if 'reasoning' in responses_params:
            del responses_params['reasoning']
        
        response = client.responses.create(**responses_params)
        
        # æå–è¼¸å‡º
        output = ""
        if hasattr(response, 'output') and response.output:
            for item in response.output:
                # è·³éResponseReasoningItemå°è±¡
                if hasattr(item, 'type') and item.type == 'reasoning':
                    continue
                
                if hasattr(item, "content"):
                    for content in item.content:
                        if hasattr(content, "text"):
                            output += content.text
                elif hasattr(item, "text"):
                    output += item.text
                elif hasattr(item, "message"):
                    if hasattr(item.message, "content"):
                        output += item.message.content
                    else:
                        output += str(item.message)
                else:
                    item_str = str(item)
                    if not item_str.startswith('ResponseReasoningItem'):
                        output += item_str
        
        output = output.strip()
        
        # å˜—è©¦è§£æç‚ºæŸ¥è©¢åˆ—è¡¨
        if output.startswith("[") and output.endswith("]"):
            try:
                import ast
                return ast.literal_eval(output)
            except Exception:
                pass

        queries = [line.strip("-â€¢ ").strip() for line in output.split("\n") if line.strip()]
        return [q for q in queries if len(q) > 4]
        
    except Exception as e:
        print(f"âŒ æŸ¥è©¢æ“´å±•å¤±æ•—ï¼š{e}")
        raise Exception(f"æŸ¥è©¢æ“´å±•å¤±æ•—ï¼š{str(e)}")

def build_proposal_prompt(question: str, chunks: List[Document]) -> Tuple[str, List[Dict]]:
    """
    æ§‹å»ºææ¡ˆç”Ÿæˆæç¤ºè©
    """
    paper_context_text = ""
    citations = []
    citation_map = {}

    for i, doc in enumerate(chunks):
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        
        citation_key = f"{filename}_p{page}"
        if citation_key not in citation_map:
            label = f"[{len(citations) + 1}]"
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
            citation_map[citation_key] = label
        else:
            label = citation_map[citation_key]

        paper_context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
    You are a scientific research expert who excels at proposing innovative and feasible research proposals based on literature summaries and research objectives.
    Your expertise covers materials science, chemistry, physics, and engineering, and you are capable of deriving new ideas grounded in experimental evidence and theoretical principles.

    Your task is to generate a structured research proposal based on the provided literature excerpts and research objectives. The proposal should be innovative, scientifically rigorous, and feasible.

    IMPORTANT: You must respond in valid JSON format only. Do not include any text before or after the JSON object.

    The JSON must have the following structure:
    {{
        "proposal_title": "Title of the research proposal",
        "need": "Research need and current limitations",
        "solution": "Proposed design and development strategies",
        "differentiation": "Comparison with existing technologies",
        "benefit": "Expected improvements and benefits",
        "experimental_overview": "Experimental approach and methodology",
        "materials_list": ["material1", "material2", "material3"]
    }}

    Key requirements:
    1. Propose new components, structures, or mechanisms (e.g., new ligands, frameworks, catalysts, processing techniques) based on the literature
    2. Clearly explain structural or functional advantages and potential reactivity/performance
    3. All proposed designs must have a logical basis â€” avoid inventing unreasonable structures without justification
    4. Maintain scientific rigor, clarity, and avoid vague descriptions
    5. Use only the provided literature labels ([1], [2], etc.) for citations, and do not fabricate sources
    6. Ensure every claim is supported by a cited source or reasonable extension from the literature
    7. For materials_list, include ONLY IUPAC chemical names without any descriptions, notes, or parenthetical explanations. Each item must be a single chemical name only.

    Literature excerpts are provided below with labels from [1] to [{len(chunks)}] (total {len(chunks)} excerpts).
    """
    
    return system_prompt.strip(), citations

# ==================== ææ¡ˆç”ŸæˆåŠŸèƒ½ ====================

def generate_structured_proposal(chunks: List[Document], question: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆçµæ§‹åŒ–ç ”ç©¶ææ¡ˆ
    """
    print(f"ğŸ” DEBUG: generate_structured_proposal é–‹å§‹")
    print(f"ğŸ” DEBUG: chunks é•·åº¦: {len(chunks) if chunks else 0}")
    print(f"ğŸ” DEBUG: question: {question}")
    
    system_prompt, citations = build_proposal_prompt(question, chunks)
    
    # æ§‹å»ºç”¨æˆ¶æç¤ºè©ï¼ˆåŒ…å«æ–‡ç»æ‘˜è¦ï¼‰
    paper_context_text = ""
    for i, doc in enumerate(chunks):
        metadata = doc.metadata
        title = metadata.get("title", "Untitled")
        filename = metadata.get("filename") or metadata.get("source", "Unknown")
        page = metadata.get("page_number") or metadata.get("page", "?")
        
        paper_context_text += f"[{i+1}] {title} | Page {page}\n{doc.page_content}\n\n"
    
    user_prompt = f"""
    --- Literature Excerpts ---
    {paper_context_text}

    --- Research Objectives ---
    {question}
    """
    
    # èª¿ç”¨çµæ§‹åŒ–LLM
    proposal_data = call_llm_structured_proposal(system_prompt, user_prompt)
    
    # æ·»åŠ å¼•ç”¨ä¿¡æ¯åˆ°è¿”å›çµæœ
    if proposal_data:
        proposal_data['citations'] = citations
    
    return proposal_data

def structured_proposal_to_text(proposal_data: Dict[str, Any]) -> str:
    """
    å°‡çµæ§‹åŒ–ææ¡ˆè½‰æ›ç‚ºå‚³çµ±æ–‡æœ¬æ ¼å¼
    """
    if not proposal_data:
        return ""
    
    text_parts = []
    
    # æ¨™é¡Œ
    if proposal_data.get('proposal_title'):
        text_parts.append(f"Proposal: {proposal_data['proposal_title']}\n")
    
    # Need
    if proposal_data.get('need'):
        text_parts.append("Need:\n")
        text_parts.append(f"{proposal_data['need']}\n")
    
    # Solution
    if proposal_data.get('solution'):
        text_parts.append("Solution:\n")
        text_parts.append(f"{proposal_data['solution']}\n")
    
    # Differentiation
    if proposal_data.get('differentiation'):
        text_parts.append("Differentiation:\n")
        text_parts.append(f"{proposal_data['differentiation']}\n")
    
    # Benefit
    if proposal_data.get('benefit'):
        text_parts.append("Benefit:\n")
        text_parts.append(f"{proposal_data['benefit']}\n")
    
    # Experimental overview
    if proposal_data.get('experimental_overview'):
        text_parts.append("Experimental overview:\n")
        text_parts.append(f"{proposal_data['experimental_overview']}\n")
    
    # Materials list
    if proposal_data.get('materials_list'):
        materials_json = json.dumps(proposal_data['materials_list'], ensure_ascii=False, indent=2)
        text_parts.append(f"```json\n{materials_json}\n```\n")
    
    return "\n".join(text_parts)

# ==================== ä¸»è¦æ¥å£å‡½æ•¸ ====================

def generate_proposal(chunks: List[Document], question: str) -> Tuple[str, Dict[str, Any]]:
    """
    ç”Ÿæˆç ”ç©¶ææ¡ˆ - åªæ”¯æ´çµæ§‹åŒ–è¼¸å‡º
    """
    print(f"ğŸ” DEBUG: generate_proposal é–‹å§‹")
    print(f"ğŸ” DEBUG: chunks é•·åº¦: {len(chunks) if chunks else 0}")
    print(f"ğŸ” DEBUG: question: {question}")
    
    try:
        print("ğŸ”§ ä½¿ç”¨çµæ§‹åŒ–è¼¸å‡ºç”Ÿæˆææ¡ˆ...")
        structured_proposal = generate_structured_proposal(chunks, question)
        print(f"ğŸ” DEBUG: generate_structured_proposal è¿”å›: {type(structured_proposal)}")
        
        if structured_proposal and all(key in structured_proposal for key in ['proposal_title', 'need', 'solution']):
            print("âœ… çµæ§‹åŒ–ææ¡ˆç”ŸæˆæˆåŠŸ")
            text_proposal = structured_proposal_to_text(structured_proposal)
            print(f"ğŸ” DEBUG: ç”Ÿæˆçš„æ–‡æœ¬ææ¡ˆé•·åº¦: {len(text_proposal)}")
            return text_proposal, structured_proposal
        else:
            print("âŒ çµæ§‹åŒ–ææ¡ˆç”Ÿæˆå¤±æ•—æˆ–æ ¼å¼ä¸å®Œæ•´")
            if structured_proposal:
                print(f"ğŸ” DEBUG: ç¼ºå°‘çš„éµ: {[key for key in ['proposal_title', 'need', 'solution'] if key not in structured_proposal]}")
            raise Exception("çµæ§‹åŒ–ææ¡ˆç”Ÿæˆå¤±æ•—")
            
    except Exception as e:
        print(f"âŒ ææ¡ˆç”Ÿæˆå¤±æ•—: {e}")
        raise Exception(f"ææ¡ˆç”Ÿæˆå¤±æ•—: {str(e)}")

# ==================== å…¶ä»–åŠŸèƒ½å‡½æ•¸ ====================

def build_inference_prompt(chunks: List[Document], question: str) -> Tuple[str, List[Dict]]:
    """
    æ§‹å»ºæ¨ç†æç¤ºè©
    """
    context_text = ""
    citations = []
    citation_map = {}
    
    for i, doc in enumerate(chunks):
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        
        citation_key = f"{filename}_p{page}"
        if citation_key not in citation_map:
            label = f"[{len(citations) + 1}]"
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
            citation_map[citation_key] = label
        else:
            label = citation_map[citation_key]

        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
    You are a materials synthesis consultant who understands and excels at comparing the chemical and physical properties of materials. You can propose innovative suggestions based on known experimental conditions for situations where there is no clear literature.

    Please conduct extended thinking based on the following literature and experimental data:
    - You can propose new combinations, temperatures, times, or pathways.
    - Even combinations not yet documented in the literature can be suggested, but you must provide reasonable reasoning.
    - When making inferences and extended thinking, please try to mention "what literature clues this idea originates from" to support your explanation. The current literature excerpt numbers are [1] to [{len(chunks)}] (total {len(chunks)} excerpts)

    --- Literature Summary ---
    {context_text}

    --- Question ---
    {question}
    """
    return system_prompt.strip(), citations

def build_dual_inference_prompt(
    chunks_paper: List[Document],
    question: str,
    experiment_chunks: List[Document]
    ) -> Tuple[str, List[Dict]]:
    """
    æ§‹å»ºé›™é‡æ¨ç†æç¤ºè©
    """
    paper_context_text = ""
    exp_context_text = ""
    citations = []
    label_index = 1

    # --- Literature Summary ---
    paper_context_text += "--- Literature Summary ---\n"
    for doc in chunks_paper:
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{label_index}]"

        citations.append({
            "label": label,
            "title": title,
            "source": filename,
            "page": page,
            "snippet": snippet,
            "type": "paper"
        })

        paper_context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"
        label_index += 1

    # --- Experiment Summary ---
    exp_context_text += "--- Similar Experiment Summary ---\n"
    for doc in experiment_chunks:
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        exp_id = meta.get("exp_id", "unknown_exp")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{label_index}]"

        citations.append({
            "label": label,
            "title": exp_id,
            "source": filename,
            "page": "-",
            "snippet": snippet,
            "type": "experiment"
        })

        exp_context_text += f"{label} Experiment {exp_id}\n{doc.page_content}\n\n"
        label_index += 1
        
    system_prompt = f"""
    You are a materials synthesis consultant who understands and excels at comparing the chemical and physical properties of materials.

    You will see three parts of information. Please conduct comprehensive analysis and provide specific inferences and innovative suggestions for experiments:
    1. Literature summary (with source annotations [1], [2])
    2. Similar experiment summary (from vector database)
    3. Experiment records (tables)

    Please propose new suggestions for the research question, including:
    - Adjusted synthesis pathways and conditions (such as temperature, time, ratios)
    - Factors that may affect synthesis success rate
    - Reasoning behind the causes, citing literature ([1], [2]...) or similar experiment results when necessary
    Important: You can only cite the provided literature excerpts. The current literature excerpt numbers are [1] to [{len(chunks_paper) + len(experiment_chunks)}] (total {len(chunks_paper) + len(experiment_chunks)} excerpts)

    --- Literature Knowledge Sources ---
    {paper_context_text}

    --- Experiment Records ---
    {exp_context_text}

    --- Research Question ---
    {question}
    """
    return system_prompt.strip(), citations

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain.chat_models import ChatOpenAI
from config import VECTOR_INDEX_DIR, EMBEDDING_MODEL_NAME, LLM_MODEL_NAME
import pandas as pd
from typing import List, Tuple, Dict
import os
from collections import defaultdict


def load_paper_vectorstore():
    embedding_model = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={"trust_remote_code": True}
    )
    paper_vector_dir = os.path.join(VECTOR_INDEX_DIR, "paper_vector")
    vectorstore = Chroma(
        persist_directory=paper_vector_dir,
        embedding_function=embedding_model, 
        collection_name="paper"
        )
    return vectorstore

def load_experiment_vectorstore():
    embedding_model = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={"trust_remote_code": True}
    )
    experiment_vector_dir = os.path.join(VECTOR_INDEX_DIR, "experiment_vector")
    vectorstore = Chroma(
        persist_directory=experiment_vector_dir, 
        embedding_function=embedding_model, 
        collection_name="experiment"
        )
    return vectorstore

def retrieve_chunks_multi_query(
    vectorstore, query_list: List[str], k: int = 10, fetch_k: int = 20, score_threshold: float = 0.35) -> List[Document]:
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": fetch_k, "score_threshold": score_threshold}
    )

    chunk_dict = defaultdict(float)
    print("query_listç‚º: ", query_list)
    for q in query_list:
        docs = retriever.get_relevant_documents(q)
        for doc in docs:
            key = doc.metadata.get("exp_id") or doc.metadata.get("source") or doc.page_content[:30]
            chunk_dict[key] = doc  # åŽ»é‡ by id or chunk
    result = list(chunk_dict.values())[:10]

    if not result:
        print("âš ï¸ æ²’æœ‰æŠ“åˆ°ä»»ä½•æ®µè½ï¼Œå»ºè­°æª¢æŸ¥ retriever æˆ–åµŒå…¥æ ¼å¼")
    print(f"ðŸ” Multi-query æŠ“åˆ°å…± {len(result)} æ®µè½ï¼š")
    print("ðŸ” Retriever æŠ“åˆ°çš„æ®µè½ï¼š")
    for i, doc in enumerate(result[:5], 1):
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        preview = doc.page_content[:80].replace("\n", " ")
        print(f"--- Chunk {i} ---")
        print(f"ðŸ“„ {filename} | Page {page}")
        print(preview)
        print()

    return result


def preview_chunks(chunks: list[Document], title: str, max_preview: int = 5):
    print(f"\nðŸ“¦ã€{title}ã€‘å…±æŠ“åˆ° {len(chunks)} æ®µè½")

    if not chunks:
        print("âš ï¸ æ²’æœ‰ä»»ä½•æ®µè½å¯é¡¯ç¤ºã€‚")
        return

    for i, doc in enumerate(chunks[:max_preview], 1):
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        preview = doc.page_content[:100].replace("\n", " ")
        print(f"--- Chunk {i} ---")
        print(f"ðŸ“„ {filename} | Page {page}")
        print(f"ðŸ“š å…§å®¹ï¼š{preview}")

# def retrieve_chunks(vectorstore, query: str, k: int = 10, fetch_k: int = 30, score_threshold: float = 0.35) -> List[Document]:
#     retriever = vectorstore.as_retriever(
#         search_type="mmr",
#         search_kwargs={"k": k, "fetch_k": fetch_k, "score_threshold": score_threshold}
#     )
#     context_docs = retriever.get_relevant_documents(query)

#     if not context_docs:
#         print("âš ï¸ æ²’æœ‰æŠ“åˆ°ä»»ä½•æ®µè½ï¼Œå»ºè­°æª¢æŸ¥ retriever æˆ–åµŒå…¥æ ¼å¼")
#     else:
#         print("ðŸ” Retriever æŠ“åˆ°çš„æ®µè½ï¼š")
#         for i, doc in enumerate(context_docs, 1):
#             meta = doc.metadata
#             filename = meta.get("filename") or meta.get("source", "Unknown")
#             page = meta.get("page_number") or meta.get("page", "?")
#             preview = doc.page_content[:100].replace("\n", " ")
#             print(f"--- Chunk {i} ---")
#             print(f"ðŸ“„ {filename} | Page {page}")
#             print(preview)
#             print()

#     return context_docs


def build_prompt(chunks: List[Document], df: pd.DataFrame, question: str) -> Tuple[str, List[Dict]]:
    context_text = ""
    citations = []
    citation_map = {}
    
    for i, doc in enumerate(chunks):
        metadata = doc.metadata
        title = metadata.get("title", "Untitled")
        filename = metadata.get("filename") or metadata.get("source", "Unknown")
        page = metadata.get("page_number") or metadata.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")

        label = f"[{i+1}]"
 
        if label not in citation_map.values():
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
        citation_map[f"{filename}_p{page}"] = label

        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
ä½ æ˜¯ä¸€ä½ç ”ç©¶æ–‡ç»æœå°‹åŠ©ç†ï¼Œåƒ…æ ¹æ“šæä¾›çš„æ–‡ç»æ®µè½ä¾†ä¸¦å›žç­”å•é¡Œã€‚
è«‹åœ¨å›žç­”ä¸­ä½¿ç”¨ [1], [2] ç­‰æ¨™è¨»æ®µè½å‡ºè™•ï¼Œä¸è¦åœ¨çµå°¾é‡è¤‡åˆ—å‡ºä¾†æºã€‚
å¦‚æ®µè½ä¸­æœ‰æåˆ°å…·é«”çš„å¯¦é©—æ¢ä»¶ï¼ˆæº«åº¦ã€æ™‚é–“ç­‰ï¼‰ï¼Œè«‹å‹™å¿…åŒ…å«åœ¨å›žç­”ä¸­ã€‚
--- æ–‡ç»æ‘˜è¦ ---
{context_text}


--- å•é¡Œ ---
{question}
"""
    return system_prompt.strip(), citations


def call_llm(prompt: str) -> str:
    llm = ChatOpenAI(model_name=LLM_MODEL_NAME, temperature=0)
    return llm.predict(prompt)

def build_inference_prompt(chunks: List[Document], df: pd.DataFrame, question: str) -> Tuple[str, List[Dict]]:
    context_text = ""
    citations = []
    for i, doc in enumerate(chunks):
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{i+1}]"

        citations.append({
            "label": label,
            "title": title,
            "source": filename,
            "page": page,
            "snippet": snippet
        })

        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    past_exp = df.to_string(index=False) if not df.empty else "ç„¡ç´€éŒ„"

    system_prompt = f"""
ä½ æ˜¯ä¸€ä½ææ–™åˆæˆé¡§å•ï¼Œä½ äº†è§£ä¸¦å–„æ–¼æ¯”è¼ƒææ–™é–“æ–¼åŒ–å­¸ã€ç‰©ç†æ€§è³ªä¸Šçš„å·®ç•°ï¼Œèƒ½å¤ é‡å°å°šæœªæœ‰æ˜Žç¢ºæ–‡ç»çš„æƒ…å¢ƒï¼Œæ ¹æ“šå·²çŸ¥ä¹‹å¯¦é©—æ¢ä»¶æŽ¨è«–å‡ºå‰µæ–°å»ºè­°ã€‚

è«‹æ ¹æ“šä»¥ä¸‹æ–‡ç»èˆ‡å¯¦é©—è³‡æ–™ï¼Œé€²è¡Œå»¶ä¼¸æ€è€ƒï¼š
- ä½ å¯ä»¥æå‡ºæ–°çš„çµ„åˆã€æº«åº¦ã€æ™‚é–“æˆ–è·¯å¾‘ã€‚
- å³ä½¿æ–‡ç»ä¸­å°šæœªè¨˜è¼‰çš„çµ„åˆä¹Ÿå¯ä»¥å»ºè­°ï¼Œä½†è¦æå‡ºåˆç†æŽ¨è«–ã€‚
- æŽ¨è«–ã€å»¶ä¼¸æ€è€ƒçš„åŒæ™‚ï¼Œè«‹å„˜å¯èƒ½æåˆ°ã€Œé€™æ¨£çš„æƒ³æ³•æºæ–¼å“ªç¨®æ–‡ç»ç·šç´¢ã€ä¾†è¼”åŠ©è§£é‡‹ï¼Œè«‹åœ¨å›žç­”ä¸­ä½¿ç”¨ [1], [2] ç­‰æ¨™è¨»æ®µè½å‡ºè™•ï¼Œä¸è¦åœ¨çµå°¾é‡è¤‡åˆ—å‡ºä¾†æºã€‚


--- æ–‡ç»æ‘˜è¦ ---
{context_text}

--- å¯¦é©—ç´€éŒ„ ---
{past_exp}

--- å•é¡Œ ---
{question}
"""
    return system_prompt.strip(), citations

def build_dual_inference_prompt(
    chunks_paper: List[Document],
    df: pd.DataFrame,
    question: str,
    experiment_chunks: List[Document]
    ) -> Tuple[str, List[Dict]]:

    context_text = ""
    citations = []
    label_index = 1

    # --- æ–‡ç»æ‘˜è¦ ---
    context_text += "--- æ–‡ç»æ‘˜è¦ ---\n"
    for doc in chunks_paper:
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{label_index}]"

        citations.append({
            "label": label,
            "title": title,
            "source": filename,
            "page": page,
            "snippet": snippet,
            "type": "paper"
        })

        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"
        label_index += 1

    # --- å¯¦é©—æ‘˜è¦ ---
    context_text += "--- é¡žä¼¼å¯¦é©—æ‘˜è¦ ---\n"
    for doc in experiment_chunks:
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        exp_id = meta.get("exp_id", "unknown_exp")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{label_index}]"

        citations.append({
            "label": label,
            "title": exp_id,
            "source": filename,
            "page": "-",  # æ²’æœ‰é æ•¸
            "snippet": snippet,
            "type": "experiment"
        })

        context_text += f"{label} å¯¦é©— {exp_id}\n{doc.page_content}\n\n"
        label_index += 1

    # --- å¯¦é©—ç´€éŒ„è¡¨æ ¼ï¼ˆDataFrameï¼‰ ---
    past_exp = df.to_string(index=False) if not df.empty else "ç„¡ç´€éŒ„"

    # --- Prompt æ³¨å…¥ ---
    system_prompt = f"""
ä½ æ˜¯ä¸€ä½ææ–™åˆæˆé¡§å•ï¼Œä½ äº†è§£ä¸¦å–„æ–¼æ¯”è¼ƒææ–™åœ¨åŒ–å­¸èˆ‡ç‰©ç†æ€§è³ªä¸Šçš„å·®ç•°ã€‚

ä½ å°‡çœ‹åˆ°ä¸‰å€‹éƒ¨åˆ†è³‡è¨Šï¼Œè«‹ç¶œåˆåˆ†æžï¼Œå°å¯¦é©—é€²è¡Œå…·é«”æŽ¨è«–èˆ‡å‰µæ–°å»ºè­°ï¼š
1. æ–‡ç»æ‘˜è¦ï¼ˆæœ‰æ¨™è¨»ä¾†æº [1]ã€[2]ï¼‰
2. é¡žä¼¼å¯¦é©—æ‘˜è¦ï¼ˆä¾†è‡ªå‘é‡è³‡æ–™åº«ï¼‰
3. å¯¦é©—ç´€éŒ„ï¼ˆè¡¨æ ¼ï¼‰

è«‹é‡å°ç ”ç©¶å•é¡Œæå‡ºæ–°çš„å»ºè­°ï¼ŒåŒ…å«ï¼š
- èª¿æ•´çš„åˆæˆè·¯å¾‘ã€æ¢ä»¶ï¼ˆå¦‚æº«åº¦ã€æ™‚é–“ã€é…æ¯”ï¼‰
- å¯èƒ½å½±éŸ¿åˆæˆæˆåŠŸçŽ‡çš„è®Šå› 
- æŽ¨è«–å…¶èƒŒå¾Œçš„åŽŸå› ï¼Œå¿…è¦æ™‚å¼•ç”¨æ–‡ç»ï¼ˆ[1]ã€[2]...ï¼‰æˆ–é¡žä¼¼å¯¦é©—çµæžœ

--- çŸ¥è­˜ä¾†æºèˆ‡æ‘˜è¦ ---
{context_text}

--- å¯¦é©—ç´€éŒ„è¡¨æ ¼ ---
{past_exp}

--- ç ”ç©¶å•é¡Œ ---
{question}
"""
    return system_prompt.strip(), citations



def expand_query(user_prompt: str) -> List[str]:
    """
    å°‡ä½¿ç”¨è€…è¼¸å…¥çš„è‡ªç„¶èªžè¨€å•é¡Œè½‰ç‚ºå¤šå€‹ semantic search æŸ¥è©¢èªžå¥ã€‚
    å›žå‚³çš„è‹±æ–‡èªžå¥å¯ç”¨æ–¼æ–‡ç»å‘é‡æª¢ç´¢ã€‚
    """
    llm = ChatOpenAI(model_name=LLM_MODEL_NAME, temperature=0.3)

    system_prompt = """You are a scientific assistant helping expand a user's synthesis question into multiple semantic search queries. 
Each query should be precise, relevant, and useful for retrieving related technical documents. 
Only return a list of 3 to 6 search queries in English. Do not explain, do not include numbering if not needed.

ä½ æ˜¯ä¸€ä½ç§‘å­¸åŠ©ç†ï¼Œå”åŠ©å°‡ä½¿ç”¨è€…çš„åˆæˆç›¸é—œå•é¡Œæ“´å±•ç‚ºå¤šå€‹èªžæ„æœå°‹æŸ¥è©¢ã€‚
æ¯å€‹æŸ¥è©¢éƒ½æ‡‰è©²æº–ç¢ºã€ç›¸é—œï¼Œä¸¦æœ‰åŠ©æ–¼æª¢ç´¢ç›¸é—œçš„æŠ€è¡“æ–‡ä»¶ã€‚
åªéœ€å›žå‚³ 3 åˆ° 6 å€‹è‹±æ–‡æŸ¥è©¢çš„åˆ—è¡¨ã€‚ä¸éœ€è¦è§£é‡‹ï¼Œä¹Ÿä¸éœ€åŠ ä¸Šç·¨è™Ÿï¼ˆé™¤éžå¿…è¦ï¼‰ã€‚"""

    full_prompt = f"{system_prompt}\n\nUser question:\n{user_prompt}"

    output = llm.predict(full_prompt).strip()

    # å˜—è©¦è§£æžæˆ query list
    if output.startswith("[") and output.endswith("]"):
        try:
            return eval(output)
        except Exception:
            pass  # fall back

    queries = [line.strip("-â€¢ ").strip() for line in output.split("\n") if line.strip()]
    return [q for q in queries if len(q) > 4]
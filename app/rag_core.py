from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain.chat_models import ChatOpenAI
from config import VECTOR_INDEX_DIR, EMBEDDING_MODEL_NAME, LLM_MODEL_NAME
import pandas as pd
from typing import List, Tuple, Dict
import os
from collections import defaultdict


def load_paper_vectorstore():
    embedding_model = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={"trust_remote_code": True}
    )
    paper_vector_dir = os.path.join(VECTOR_INDEX_DIR, "paper_vector")
    vectorstore = Chroma(
        persist_directory=paper_vector_dir,
        embedding_function=embedding_model, 
        collection_name="paper"
        )
    return vectorstore

def load_experiment_vectorstore():
    embedding_model = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={"trust_remote_code": True}
    )
    experiment_vector_dir = os.path.join(VECTOR_INDEX_DIR, "experiment_vector")
    vectorstore = Chroma(
        persist_directory=experiment_vector_dir, 
        embedding_function=embedding_model, 
        collection_name="experiment"
        )
    return vectorstore

def retrieve_chunks_multi_query(
    vectorstore, query_list: List[str], k: int = 10, fetch_k: int = 20, score_threshold: float = 0.35) -> List[Document]:
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": fetch_k, "score_threshold": score_threshold}
    )

    chunk_dict = defaultdict(float)
    print("query_listç‚º: ", query_list)
    for q in query_list:
        docs = retriever.get_relevant_documents(q)
        for doc in docs:
            key = doc.metadata.get("exp_id") or doc.metadata.get("source") or doc.page_content[:30]
            chunk_dict[key] = doc  # å»é‡ by id or chunk
    result = list(chunk_dict.values())[:10]

    if not result:
        print("âš ï¸ æ²’æœ‰æŠ“åˆ°ä»»ä½•æ®µè½ï¼Œå»ºè­°æª¢æŸ¥ retriever æˆ–åµŒå…¥æ ¼å¼")
    print(f"ğŸ” Multi-query æŠ“åˆ°å…± {len(result)} æ®µè½ï¼š")
    print("ğŸ” Retriever æŠ“åˆ°çš„æ®µè½ï¼š")
    for i, doc in enumerate(result[:5], 1):
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        preview = doc.page_content[:80].replace("\n", " ")
        print(f"--- Chunk {i} ---")
        print(f"ğŸ“„ {filename} | Page {page}")
        print(preview)
        print()

    return result


def preview_chunks(chunks: list[Document], title: str, max_preview: int = 5):
    print(f"\nğŸ“¦ã€{title}ã€‘å…±æŠ“åˆ° {len(chunks)} æ®µè½")

    if not chunks:
        print("âš ï¸ æ²’æœ‰ä»»ä½•æ®µè½å¯é¡¯ç¤ºã€‚")
        return

    for i, doc in enumerate(chunks[:max_preview], 1):
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        preview = doc.page_content[:100].replace("\n", " ")
        print(f"--- Chunk {i} ---")
        print(f"ğŸ“„ {filename} | Page {page}")
        print(f"ğŸ“š å…§å®¹ï¼š{preview}")

# def retrieve_chunks(vectorstore, query: str, k: int = 10, fetch_k: int = 30, score_threshold: float = 0.35) -> List[Document]:
#     retriever = vectorstore.as_retriever(
#         search_type="mmr",
#         search_kwargs={"k": k, "fetch_k": fetch_k, "score_threshold": score_threshold}
#     )
#     context_docs = retriever.get_relevant_documents(query)

#     if not context_docs:
#         print("âš ï¸ æ²’æœ‰æŠ“åˆ°ä»»ä½•æ®µè½ï¼Œå»ºè­°æª¢æŸ¥ retriever æˆ–åµŒå…¥æ ¼å¼")
#     else:
#         print("ğŸ” Retriever æŠ“åˆ°çš„æ®µè½ï¼š")
#         for i, doc in enumerate(context_docs, 1):
#             meta = doc.metadata
#             filename = meta.get("filename") or meta.get("source", "Unknown")
#             page = meta.get("page_number") or meta.get("page", "?")
#             preview = doc.page_content[:100].replace("\n", " ")
#             print(f"--- Chunk {i} ---")
#             print(f"ğŸ“„ {filename} | Page {page}")
#             print(preview)
#             print()

#     return context_docs


def build_prompt(chunks: List[Document], question: str) -> Tuple[str, List[Dict]]:
    context_text = ""
    citations = []
    citation_map = {}
    
    for i, doc in enumerate(chunks):
        metadata = doc.metadata
        title = metadata.get("title", "Untitled")
        filename = metadata.get("filename") or metadata.get("source", "Unknown")
        page = metadata.get("page_number") or metadata.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")

        label = f"[{i+1}]"
 
        if label not in citation_map.values():
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
        citation_map[f"{filename}_p{page}"] = label

        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
ä½ æ˜¯ä¸€ä½ç ”ç©¶æ–‡ç»æœå°‹åŠ©ç†ï¼Œåƒ…æ ¹æ“šæä¾›çš„æ–‡ç»æ®µè½ä¾†ä¸¦å›ç­”å•é¡Œã€‚
è«‹åœ¨å›ç­”ä¸­ä½¿ç”¨ [1], [2] ç­‰æ¨™è¨»æ®µè½å‡ºè™•ï¼Œä¸è¦åœ¨çµå°¾é‡è¤‡åˆ—å‡ºä¾†æºã€‚
å¦‚æ®µè½ä¸­æœ‰æåˆ°å…·é«”çš„å¯¦é©—æ¢ä»¶ï¼ˆæº«åº¦ã€æ™‚é–“ç­‰ï¼‰ï¼Œè«‹å‹™å¿…åŒ…å«åœ¨å›ç­”ä¸­ã€‚
--- æ–‡ç»æ‘˜è¦ ---
{context_text}


--- å•é¡Œ ---
{question}
"""
    return system_prompt.strip(), citations


def call_llm(prompt: str) -> str:
    llm = ChatOpenAI(model_name=LLM_MODEL_NAME, temperature=0)
    return llm.predict(prompt)

def build_inference_prompt(chunks: List[Document], question: str) -> Tuple[str, List[Dict]]:
    context_text = ""
    citations = []
    for i, doc in enumerate(chunks):
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{i+1}]"

        citations.append({
            "label": label,
            "title": title,
            "source": filename,
            "page": page,
            "snippet": snippet
        })

        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
ä½ æ˜¯ä¸€ä½ææ–™åˆæˆé¡§å•ï¼Œä½ äº†è§£ä¸¦å–„æ–¼æ¯”è¼ƒææ–™é–“æ–¼åŒ–å­¸ã€ç‰©ç†æ€§è³ªä¸Šçš„å·®ç•°ï¼Œèƒ½å¤ é‡å°å°šæœªæœ‰æ˜ç¢ºæ–‡ç»çš„æƒ…å¢ƒï¼Œæ ¹æ“šå·²çŸ¥ä¹‹å¯¦é©—æ¢ä»¶æ¨è«–å‡ºå‰µæ–°å»ºè­°ã€‚

è«‹æ ¹æ“šä»¥ä¸‹æ–‡ç»èˆ‡å¯¦é©—è³‡æ–™ï¼Œé€²è¡Œå»¶ä¼¸æ€è€ƒï¼š
- ä½ å¯ä»¥æå‡ºæ–°çš„çµ„åˆã€æº«åº¦ã€æ™‚é–“æˆ–è·¯å¾‘ã€‚
- å³ä½¿æ–‡ç»ä¸­å°šæœªè¨˜è¼‰çš„çµ„åˆä¹Ÿå¯ä»¥å»ºè­°ï¼Œä½†è¦æå‡ºåˆç†æ¨è«–ã€‚
- æ¨è«–ã€å»¶ä¼¸æ€è€ƒçš„åŒæ™‚ï¼Œè«‹å„˜å¯èƒ½æåˆ°ã€Œé€™æ¨£çš„æƒ³æ³•æºæ–¼å“ªç¨®æ–‡ç»ç·šç´¢ã€ä¾†è¼”åŠ©è§£é‡‹ï¼Œè«‹åœ¨å›ç­”ä¸­ä½¿ç”¨ [1], [2] ç­‰æ¨™è¨»æ®µè½å‡ºè™•ï¼Œä¸è¦åœ¨çµå°¾é‡è¤‡åˆ—å‡ºä¾†æºã€‚


--- æ–‡ç»æ‘˜è¦ ---
{context_text}

--- å•é¡Œ ---
{question}
"""
    return system_prompt.strip(), citations

def build_dual_inference_prompt(
    chunks_paper: List[Document],
    question: str,
    experiment_chunks: List[Document]
    ) -> Tuple[str, List[Dict]]:

    paper_context_text = ""
    exp_context_text = ""
    citations = []
    label_index = 1

    # --- æ–‡ç»æ‘˜è¦ ---
    paper_context_text += "--- æ–‡ç»æ‘˜è¦ ---\n"
    for doc in chunks_paper:
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{label_index}]"

        citations.append({
            "label": label,
            "title": title,
            "source": filename,
            "page": page,
            "snippet": snippet,
            "type": "paper"
        })

        paper_context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"
        label_index += 1

    # --- å¯¦é©—æ‘˜è¦ ---
    exp_context_text += "--- é¡ä¼¼å¯¦é©—æ‘˜è¦ ---\n"
    for doc in experiment_chunks:
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        exp_id = meta.get("exp_id", "unknown_exp")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{label_index}]"

        citations.append({
            "label": label,
            "title": exp_id,
            "source": filename,
            "page": "-",  # æ²’æœ‰é æ•¸
            "snippet": snippet,
            "type": "experiment"
        })

        exp_context_text += f"{label} å¯¦é©— {exp_id}\n{doc.page_content}\n\n"
        label_index += 1
        
    # --- Prompt æ³¨å…¥ ---
    system_prompt = f"""
ä½ æ˜¯ä¸€ä½ææ–™åˆæˆé¡§å•ï¼Œä½ äº†è§£ä¸¦å–„æ–¼æ¯”è¼ƒææ–™åœ¨åŒ–å­¸èˆ‡ç‰©ç†æ€§è³ªä¸Šçš„å·®ç•°ã€‚

ä½ å°‡çœ‹åˆ°ä¸‰å€‹éƒ¨åˆ†è³‡è¨Šï¼Œè«‹ç¶œåˆåˆ†æï¼Œå°å¯¦é©—é€²è¡Œå…·é«”æ¨è«–èˆ‡å‰µæ–°å»ºè­°ï¼š
1. æ–‡ç»æ‘˜è¦ï¼ˆæœ‰æ¨™è¨»ä¾†æº [1]ã€[2]ï¼‰
2. é¡ä¼¼å¯¦é©—æ‘˜è¦ï¼ˆä¾†è‡ªå‘é‡è³‡æ–™åº«ï¼‰
3. å¯¦é©—ç´€éŒ„ï¼ˆè¡¨æ ¼ï¼‰

è«‹é‡å°ç ”ç©¶å•é¡Œæå‡ºæ–°çš„å»ºè­°ï¼ŒåŒ…å«ï¼š
- èª¿æ•´çš„åˆæˆè·¯å¾‘ã€æ¢ä»¶ï¼ˆå¦‚æº«åº¦ã€æ™‚é–“ã€é…æ¯”ï¼‰
- å¯èƒ½å½±éŸ¿åˆæˆæˆåŠŸç‡çš„è®Šå› 
- æ¨è«–å…¶èƒŒå¾Œçš„åŸå› ï¼Œå¿…è¦æ™‚å¼•ç”¨æ–‡ç»ï¼ˆ[1]ã€[2]...ï¼‰æˆ–é¡ä¼¼å¯¦é©—çµæœ

--- æ–‡ç»çŸ¥è­˜ä¾†æº ---
{paper_context_text}

--- å¯¦é©—ç´€éŒ„ ---
{exp_context_text}

--- ç ”ç©¶å•é¡Œ ---
{question}
"""
    return system_prompt.strip(), citations



def expand_query(user_prompt: str) -> List[str]:
    """
    å°‡ä½¿ç”¨è€…è¼¸å…¥çš„è‡ªç„¶èªè¨€å•é¡Œè½‰ç‚ºå¤šå€‹ semantic search æŸ¥è©¢èªå¥ã€‚
    å›å‚³çš„è‹±æ–‡èªå¥å¯ç”¨æ–¼æ–‡ç»å‘é‡æª¢ç´¢ã€‚
    """
    llm = ChatOpenAI(model_name=LLM_MODEL_NAME, temperature=0.3)

    system_prompt = """You are a scientific assistant helping expand a user's synthesis question into multiple semantic search queries. 
Each query should be precise, relevant, and useful for retrieving related technical documents. 
Only return a list of 3 to 6 search queries in English. Do not explain, do not include numbering if not needed.

ä½ æ˜¯ä¸€ä½ç§‘å­¸åŠ©ç†ï¼Œå”åŠ©å°‡ä½¿ç”¨è€…çš„åˆæˆç›¸é—œå•é¡Œæ“´å±•ç‚ºå¤šå€‹èªæ„æœå°‹æŸ¥è©¢ã€‚
æ¯å€‹æŸ¥è©¢éƒ½æ‡‰è©²æº–ç¢ºã€ç›¸é—œï¼Œä¸¦æœ‰åŠ©æ–¼æª¢ç´¢ç›¸é—œçš„æŠ€è¡“æ–‡ä»¶ã€‚
åªéœ€å›å‚³ 3 åˆ° 6 å€‹è‹±æ–‡æŸ¥è©¢çš„åˆ—è¡¨ã€‚ä¸éœ€è¦è§£é‡‹ï¼Œä¹Ÿä¸éœ€åŠ ä¸Šç·¨è™Ÿï¼ˆé™¤éå¿…è¦ï¼‰ã€‚"""

    full_prompt = f"{system_prompt}\n\nUser question:\n{user_prompt}"

    output = llm.predict(full_prompt).strip()

    # å˜—è©¦è§£ææˆ query list
    if output.startswith("[") and output.endswith("]"):
        try:
            return eval(output)
        except Exception:
            pass  # fall back

    queries = [line.strip("-â€¢ ").strip() for line in output.split("\n") if line.strip()]
    return [q for q in queries if len(q) > 4]


def build_proposal_prompt(chunks: List[Document], question: str) -> Tuple[str, List[Dict]]:
    paper_context_text = ""
    citations = []

    for i, doc in enumerate(chunks):
        
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{i+1}]"

        citations.append({
            "label": label,
            "title": title,
            "source": filename,
            "page": page,
            "snippet": snippet
        })

        paper_context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
ä½ æ˜¯ä¸€ä½ææ–™åŒ–å­¸å°ˆå®¶ï¼Œæ“…é•·æ ¹æ“šæ–‡ç»æ‘˜è¦èˆ‡ç ”ç©¶ç›®æ¨™ï¼Œæå‡ºå…·æœ‰å‰µæ–°æ€§èˆ‡å¯è¡Œæ€§çš„ææ–™åˆæˆç ”ç©¶ææ¡ˆã€‚
å¦‚èƒ½æ ¹æ“šæ–‡ç»æå‡ºæ–°çš„é…é«”ç¨®é¡ï¼ˆå¦‚ pyridyl, biphenyl, sulfonateï¼‰æˆ–é‡‘å±¬ç¯€é»ï¼ˆå¦‚ Zn, Mg, Zrï¼‰ï¼Œè«‹å…·é«”åˆ—å‡ºä¸¦èªªæ˜å…¶çµæ§‹å„ªå‹¢èˆ‡åæ‡‰æ€§ã€‚

è«‹ä½ æ ¹æ“šä¸‹æ–¹æ–‡ç»å…§å®¹èˆ‡ç ”ç©¶ç›®æ¨™ï¼Œæ’°å¯«ä¸€ä»½çµæ§‹åŒ–ææ¡ˆï¼Œæ ¼å¼å¦‚ä¸‹ï¼ˆè«‹å®Œæ•´ç”¢å‡ºæ¯ä¸€æ®µï¼‰ï¼š

---
Proposal: ï¼ˆè«‹è‡ªå‹•ç”¢ç”Ÿææ¡ˆæ¨™é¡Œï¼Œæ¿ƒç¸®ç ”ç©¶ç›®æ¨™èˆ‡å‰µæ–°é»ï¼‰

Need:
- ç°¡è¿°ç ”ç©¶ç›®æ¨™èˆ‡ç›®å‰ææ–™åœ¨æ‡‰ç”¨ä¸Šçš„é™åˆ¶
- æ˜ç¢ºæŒ‡å‡ºç”¢æ¥­ã€ä¸–ç•Œã€æ”¿åºœç­‰å¾…è§£æ±ºçš„ç—›é»æˆ–æŠ€è¡“ç“¶é ¸

Solution:
- æå‡ºå…·é«”çš„ææ–™è¨­è¨ˆèˆ‡åˆæˆç­–ç•¥
- å»ºè­°æ–°çš„åŒ–å­¸çµæ§‹ï¼ˆå¦‚é‡‘å±¬ã€æœ‰æ©Ÿé…é«”ã€åŠŸèƒ½å®˜èƒ½åŸº)ï¼Œæ˜ç¢ºæŒ‡å‡ºå»ºè­°çš„åŒ–å­¸çµæ§‹åç¨±(å¦‚é…é«”åç¨±)
- è«‹èªªæ˜æ–°è¨­è¨ˆçš„åŒ–å­¸é‚è¼¯ï¼ˆå¦‚é…ä½ç’°å¢ƒã€å­”å¾‘ã€å®˜èƒ½åŸºä½œç”¨ï¼‰

Differentiation:
- èˆ‡ç¾æœ‰æ–‡ç»ææ–™çš„å·®ç•°ï¼ˆå¯åˆ—è¡¨æ¯”è¼ƒï¼‰
- å¼·èª¿çµæ§‹ã€æ¢ä»¶æˆ–æ€§èƒ½ä¸Šçš„çªç ´

Benefit:
- é æœŸæ”¹å–„çš„æ€§èƒ½æˆ–æ‡‰ç”¨é¢å‘
- è‹¥èƒ½é‡åŒ–ï¼ˆå¦‚æå‡ COâ‚‚ å¸é™„é‡ã€ç©©å®šæ€§ã€é¸æ“‡æ€§ï¼‰è«‹ç›¡é‡åˆ—å‡º

Based Literature:
- ä½¿ç”¨æ®µè½æ¨™ç±¤åˆ—å‡ºå¼•ç”¨çš„ä¾æ“šï¼Œä¾‹å¦‚ï¼š[1]ã€[2]...
- è«‹ç¢ºä¿æ¯å€‹æ¨è«–éƒ½æœ‰æ–‡ç»å°ç…§ä¾æ“šæˆ–åˆç†å»¶ä¼¸ç†ç”±

Experimental overview:
1. ä½¿ç”¨çš„èµ·å§‹ææ–™èˆ‡åæ‡‰æ¢ä»¶ï¼ˆå¦‚æº«åº¦ã€æ™‚é–“ï¼‰
2. åˆæˆä½¿ç”¨å„€å™¨ã€è¨­å‚™æè¿°ä¸¦åˆ—è¡¨
3. åˆæˆæ­¥é©Ÿæ•˜è¿°ï¼ˆæè¿°é—œéµé‚è¼¯ï¼‰



æœ€å¾Œï¼Œè«‹åœ¨å›ç­”æœ€æœ«æ®µï¼Œä»¥ JSON æ ¼å¼åˆ—å‡ºæ­¤ææ¡ˆæ‰€ä½¿ç”¨åˆ°çš„æ‰€æœ‰åŒ–å­¸å“ï¼ˆåŒ…å«é‡‘å±¬é¹½ã€æœ‰æ©Ÿé…é«”ã€æº¶åŠ‘ç­‰ï¼‰ã€‚ä»¥IUPAC Nameå›ç­”ï¼Œæ ¼å¼èˆ‡ç¯„ä¾‹å¦‚ä¸‹ï¼š

```markdown

```json
["formic acid", "N,N-dimethylformamide", "copper;dinitrate;trihydrate"]
```
---

è«‹æ³¨æ„ï¼š
- çµæ§‹å»ºè­°è¦æœ‰é‚è¼¯ä¾æ“šï¼Œé¿å…éš¨æ„ç·¨é€ ä¸åˆç†çµæ§‹
- è«‹ä¿æŒç§‘å­¸æ€§ã€å¯è®€æ€§ï¼Œä¸¦é¿å…ç©ºæ³›æ•˜è¿°
- å¼•ç”¨æ¨™è¨»è«‹ä½¿ç”¨æä¾›çš„æ–‡ç»æ®µè½æ¨™ç±¤ [1]ã€[2]ï¼Œä¸è¦ä½¿ç”¨è™›æ§‹ä¾†æº

--- æ–‡ç»æ®µè½ ---
{paper_context_text}

--- ç ”ç©¶ç›®æ¨™ ---
{question}

"""
    
    return system_prompt.strip(), citations

def build_detail_experimental_plan_prompt(chunks: List[Document], proposal_text: str) -> str:
    citations=[]
    paper_context_text = "\n\n".join(doc.page_content for doc in chunks)

    system_prompt = f"""
You are an experienced consultant in materials experiment design. Based on the following research proposal and related literature excerpts, please provide the researcher with a detailed set of recommended experimental procedures:

Please include:

Synthesis Process: A step-by-step description of each experimental operation, including sequence, logic, and purpose.

Materials and Conditions: The required raw materials for each step (including proportions), and the reaction conditions (temperature, time, containers).

Analytical Methods: Suggested characterization tools (such as XRD, BET, TGA) and the purpose for each.

Precautions: Key points or parameter limitations mentioned in the literature.

Please clearly list the information in bullet points or paragraphs.

--- literature chunks ---
{paper_context_text}

--- User's Proposal ---
{proposal_text}
"""
    return system_prompt.strip(), citations



def build_iterative_proposal_prompt(
    question: str,
    new_chunks: List[Document],
    old_chunks: List[Document],
    past_proposal: str
) -> Tuple[str, List[Dict]]:
    """
    å»ºç«‹æ–°çš„ç ”ç©¶ææ¡ˆ promptï¼Œçµåˆä½¿ç”¨è€…çš„åé¥‹ã€æ–°æª¢ç´¢æ–‡ç»ã€èˆŠæ–‡ç»èˆ‡åŸå§‹ææ¡ˆã€‚
    åŒæ™‚å›å‚³ citation listã€‚
    """
    citations = []

    def format_chunks(chunks: List[Document], label_offset=0) -> Tuple[str, List[Dict]]:
        text = ""
        local_citations = []
        for i, doc in enumerate(chunks):
            meta = doc.metadata
            title = meta.get("title", "Untitled")
            filename = meta.get("filename") or meta.get("source", "Unknown")
            page = meta.get("page_number") or meta.get("page", "?")
            snippet = doc.page_content[:80].replace("\n", " ")
            label = f"[{label_offset + i + 1}]"

            local_citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })

            text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

        return text, local_citations

    old_text, old_citations = format_chunks(old_chunks)
    new_text, new_citations = format_chunks(new_chunks, label_offset=len(old_citations))
    citations.extend(old_citations + new_citations)

    system_prompt = f"""
ä½ æ˜¯ä¸€ä½ç†Ÿç·´çš„ææ–™å¯¦é©—è¨­è¨ˆé¡§å•ï¼Œè«‹æ ¹æ“šä½¿ç”¨è€…æä¾›çš„åé¥‹ã€åŸå§‹ææ¡ˆã€èˆ‡æ–‡ç»å…§å®¹ï¼Œå”åŠ©ä¿®æ”¹éƒ¨åˆ†çš„ç ”ç©¶ææ¡ˆã€‚

è«‹ä¾å¾ªä»¥ä¸‹æŒ‡å¼•ï¼š
1. å„ªå…ˆè™•ç†ä½¿ç”¨è€…æå‡ºæ¬²ä¿®æ”¹ä¹‹è™•ï¼Œä¸¦å¾æ–‡ç»ä¸­å°‹æ‰¾å¯èƒ½çš„æ”¹é€²æ–¹å‘ã€‚
2. é™¤äº†ä½¿ç”¨è€…æå‡ºä¹‹ä¸æ»¿æ„è™•é€²è¡Œä¿®æ”¹å¤–ï¼Œå…¶ä»–éƒ¨åˆ†(NSDB)è«‹ä¿æŒåŸææ¡ˆå…§å®¹ï¼Œä¸é ˆæ›´å‹•ï¼Œç›´æ¥è¼¸å‡º
3. ä¿æŒææ¡ˆæ ¼å¼èˆ‡åŸå§‹ææ¡ˆå…§å®¹ä¸€è‡´ï¼Œä¸¦è«‹æ–¼ç¬¬ä¸€æ®µç°¡è¦èªªæ˜æ›´æ”¹çš„æ–¹å‘ï¼Œç¸½å…±åŒ…å«ä¸‹åˆ—å€å¡Šï¼š
   - revision explanation: ç°¡è¦èªªæ˜æœ¬æ¬¡ææ¡ˆèˆ‡å‰æ¬¡ææ¡ˆçš„å·®ç•°
   - Need
   - Solution
   - Differentiation
   - Benefit
   - Based Literature
   - Experimental overview

æœ€å¾Œï¼Œè«‹åœ¨å›ç­”æœ€æœ«æ®µï¼Œä»¥ JSON æ ¼å¼åˆ—å‡ºæ­¤ææ¡ˆæ‰€ä½¿ç”¨åˆ°çš„æ‰€æœ‰åŒ–å­¸å“ï¼ˆåŒ…å«é‡‘å±¬é¹½ã€æœ‰æ©Ÿé…é«”ã€æº¶åŠ‘ç­‰ï¼‰ã€‚ä»¥ IUPAC Name å›ç­”ï¼Œå›ç­”æ ¼å¼èˆ‡ç¯„ä¾‹å¦‚ä¸‹ï¼š

```json
["formic acid", "N,N-dimethylformamide", "copper;dinitrate;trihydrate"]
--- ä½¿ç”¨è€…çš„åé¥‹ ---
{question}

--- åŸå§‹ææ¡ˆå…§å®¹ ---
{past_proposal}

--- åŸå§‹ææ¡ˆæ‰€åŸºæ–¼çš„æ–‡ç»æ®µè½ ---
{old_text}

--- æ ¹æ“šåé¥‹è£œå……çš„æ–°æª¢ç´¢æ®µè½ ---
{new_text}
"""
    return system_prompt.strip(), citations


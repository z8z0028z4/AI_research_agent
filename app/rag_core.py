"""
AI ç ”ç©¶åŠ©ç† - RAGæ ¸å¿ƒæ¨¡å¡Š
========================

é€™å€‹æ¨¡å¡Šæ˜¯æ•´å€‹ç³»çµ±çš„RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰æ ¸å¿ƒï¼Œè² è²¬ï¼š
1. å‘é‡æ•¸æ“šåº«ç®¡ç†
2. æ–‡æª”æª¢ç´¢å’Œç›¸ä¼¼åº¦æœç´¢
3. æç¤ºè©æ§‹å»ºå’Œå„ªåŒ–
4. LLMèª¿ç”¨å’Œå›ç­”ç”Ÿæˆ

æ¶æ§‹èªªæ˜ï¼š
- ä½¿ç”¨Chromaä½œç‚ºå‘é‡æ•¸æ“šåº«
- æ”¯æŒå¤šæŸ¥è©¢æª¢ç´¢
- æä¾›å¤šç¨®æç¤ºè©æ¨¡æ¿
- é›†æˆOpenAI GPTæ¨¡å‹

âš ï¸ æ³¨æ„ï¼šæ­¤æ¨¡å¡Šæ˜¯ç³»çµ±çš„æ ¸å¿ƒçµ„ä»¶ï¼Œæ‰€æœ‰çŸ¥è­˜è™•ç†éƒ½ä¾è³´æ–¼æ­¤æ¨¡å¡Š
"""

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain.chat_models import ChatOpenAI
from config import VECTOR_INDEX_DIR, EMBEDDING_MODEL_NAME, LLM_MODEL_NAME, LLM_PARAMS
import pandas as pd
from typing import List, Tuple, Dict
import os
from collections import defaultdict

# Embedding model configuration

# ==================== å‘é‡æ•¸æ“šåº«ç®¡ç† ====================

def load_paper_vectorstore():
    """
    è¼‰å…¥æ–‡ç»å‘é‡æ•¸æ“šåº«
    
    åŠŸèƒ½ï¼š
    1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    2. é€£æ¥æ–‡ç»å‘é‡å­˜å„²
    3. è¿”å›å¯ç”¨çš„å‘é‡æ•¸æ“šåº«å°è±¡
    
    è¿”å›ï¼š
        Chroma: æ–‡ç»å‘é‡æ•¸æ“šåº«å°è±¡
    
    æŠ€è¡“ç´°ç¯€ï¼š
    - ä½¿ç”¨HuggingFaceåµŒå…¥æ¨¡å‹
    - æŒä¹…åŒ–å­˜å„²åœ¨paper_vectorç›®éŒ„
    - é›†åˆåç¨±ç‚º"paper"
    """
    # Load Nomic embedding model
    try:
        print(f"ğŸ”§ Loading Nomic embedding model: {EMBEDDING_MODEL_NAME}")
        embedding_model = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={"trust_remote_code": True},
            encode_kwargs={"normalize_embeddings": True}
        )
        print("âœ… Nomic embedding model loaded successfully")
    except Exception as e:
        print(f"âŒ Nomic embedding failed: {e}")
        raise e
    
    paper_vector_dir = os.path.join(VECTOR_INDEX_DIR, "paper_vector")
    vectorstore = Chroma(
        persist_directory=paper_vector_dir,
        embedding_function=embedding_model, 
        collection_name="paper"
        )
    return vectorstore


def load_experiment_vectorstore():
    """
    è¼‰å…¥å¯¦é©—æ•¸æ“šå‘é‡æ•¸æ“šåº«
    
    åŠŸèƒ½ï¼š
    1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
    2. é€£æ¥å¯¦é©—æ•¸æ“šå‘é‡å­˜å„²
    3. è¿”å›å¯ç”¨çš„å‘é‡æ•¸æ“šåº«å°è±¡
    
    è¿”å›ï¼š
        Chroma: å¯¦é©—æ•¸æ“šå‘é‡æ•¸æ“šåº«å°è±¡
    
    æŠ€è¡“ç´°ç¯€ï¼š
    - ä½¿ç”¨ç›¸åŒçš„åµŒå…¥æ¨¡å‹ç¢ºä¿ä¸€è‡´æ€§
    - æŒä¹…åŒ–å­˜å„²åœ¨experiment_vectorç›®éŒ„
    - é›†åˆåç¨±ç‚º"experiment"
    """
    # Load Nomic embedding model
    try:
        print(f"ğŸ”§ Loading Nomic embedding model: {EMBEDDING_MODEL_NAME}")
        embedding_model = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={"trust_remote_code": True},
            encode_kwargs={"normalize_embeddings": True}
        )
        print("âœ… Nomic embedding model loaded successfully")
    except Exception as e:
        print(f"âŒ Nomic embedding failed: {e}")
        raise e
    experiment_vector_dir = os.path.join(VECTOR_INDEX_DIR, "experiment_vector")
    vectorstore = Chroma(
        persist_directory=experiment_vector_dir, 
        embedding_function=embedding_model, 
        collection_name="experiment"
        )
    return vectorstore


# ==================== æ–‡æª”æª¢ç´¢åŠŸèƒ½ ====================

def retrieve_chunks_multi_query(
    vectorstore, query_list: List[str], k: int = 10, fetch_k: int = 20, score_threshold: float = 0.35
    ) -> List[Document]:
    """
    å¤šæŸ¥è©¢æ–‡æª”æª¢ç´¢åŠŸèƒ½
    
    åŠŸèƒ½ï¼š
    1. å°å¤šå€‹æŸ¥è©¢é€²è¡Œä¸¦è¡Œæª¢ç´¢
    2. å»é‡å’Œæ’åºæª¢ç´¢çµæœ
    3. æä¾›è©³ç´°çš„æª¢ç´¢çµ±è¨ˆä¿¡æ¯
    
    åƒæ•¸ï¼š
        vectorstore: å‘é‡æ•¸æ“šåº«å°è±¡
        query_list (List[str]): æŸ¥è©¢åˆ—è¡¨
        k (int): è¿”å›çš„æ–‡æª”æ•¸é‡
        fetch_k (int): åˆå§‹æª¢ç´¢çš„æ–‡æª”æ•¸é‡
        score_threshold (float): ç›¸ä¼¼åº¦é–¾å€¼
    
    è¿”å›ï¼š
        List[Document]: æª¢ç´¢åˆ°çš„æ–‡æª”åˆ—è¡¨
    
    æŠ€è¡“ç´°ç¯€ï¼š
    - ä½¿ç”¨MMRï¼ˆæœ€å¤§é‚Šéš›ç›¸é—œæ€§ï¼‰æœç´¢
    - è‡ªå‹•å»é‡é¿å…é‡è¤‡å…§å®¹
    - æä¾›è©³ç´°çš„æª¢ç´¢æ—¥èªŒ
    """
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": fetch_k, "score_threshold": score_threshold}
    )

    # ä½¿ç”¨å­—å…¸é€²è¡Œå»é‡
    chunk_dict = defaultdict(float)
    print("ğŸ” æŸ¥è©¢åˆ—è¡¨ï¼š", query_list)
    
    # å°æ¯å€‹æŸ¥è©¢é€²è¡Œæª¢ç´¢
    for q in query_list:
        docs = retriever.get_relevant_documents(q)
        for doc in docs:
            # ä½¿ç”¨å”¯ä¸€æ¨™è­˜ç¬¦é€²è¡Œå»é‡
            key = doc.metadata.get("exp_id") or doc.metadata.get("source") or doc.page_content[:30]
            chunk_dict[key] = doc
    
    # é™åˆ¶è¿”å›çµæœæ•¸é‡ï¼Œä½¿ç”¨å‚³å…¥çš„ k åƒæ•¸
    result = list(chunk_dict.values())[:k]

    # æª¢ç´¢çµæœé©—è­‰
    if not result:
        print("âš ï¸ æ²’æœ‰æª¢ç´¢åˆ°ä»»ä½•æ–‡æª”ï¼Œå»ºè­°æª¢æŸ¥æª¢ç´¢å™¨æˆ–åµŒå…¥æ ¼å¼")
    else:
        print(f"ğŸ” å¤šæŸ¥è©¢æª¢ç´¢å…±æ‰¾åˆ° {len(result)} å€‹æ–‡æª”ï¼š")
        print("ğŸ” æª¢ç´¢åˆ°çš„æ–‡æª”é è¦½ï¼š")
        for i, doc in enumerate(result[:5], 1):
            meta = doc.metadata
            filename = meta.get("filename") or meta.get("source", "Unknown")
            page = meta.get("page_number") or meta.get("page", "?")
            preview = doc.page_content[:80].replace("\n", " ")
            print(f"--- æ–‡æª” {i} ---")
            print(f"ğŸ“„ {filename} | é ç¢¼ {page}")
            print(f"ğŸ“ å…§å®¹é è¦½ï¼š{preview}")
            print()

    return result


def preview_chunks(chunks: List[Document], title: str, max_preview: int = 5):
    """
    é è¦½æ–‡æª”å¡Šå…§å®¹
    
    åŠŸèƒ½ï¼š
    1. é¡¯ç¤ºæ–‡æª”å¡Šçš„åŸºæœ¬ä¿¡æ¯
    2. æä¾›å…§å®¹é è¦½
    3. å¹«åŠ©èª¿è©¦å’Œé©—è­‰æª¢ç´¢çµæœ
    
    åƒæ•¸ï¼š
        chunks (List[Document]): æ–‡æª”å¡Šåˆ—è¡¨
        title (str): é è¦½æ¨™é¡Œ
        max_preview (int): æœ€å¤§é è¦½æ•¸é‡
    """
    print(f"\nğŸ“¦ã€{title}ã€‘å…±æ‰¾åˆ° {len(chunks)} å€‹æ–‡æª”å¡Š")

    if not chunks:
        print("âš ï¸ æ²’æœ‰ä»»ä½•æ®µè½å¯é¡¯ç¤ºã€‚")
        return

    # é¡¯ç¤ºå‰å¹¾å€‹æ–‡æª”å¡Šçš„è©³ç´°ä¿¡æ¯
    for i, doc in enumerate(chunks[:max_preview], 1):
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        preview = doc.page_content[:100].replace("\n", " ")
        print(f"--- Chunk {i} ---")
        print(f"ğŸ“„ Filename: {filename} | Page: {page}")
        print(f"ğŸ“š Preview: {preview}")


# ==================== æç¤ºè©æ§‹å»ºåŠŸèƒ½ ====================

def build_prompt(chunks: List[Document], question: str) -> Tuple[str, List[Dict]]:
    # æª¢æŸ¥ï¼šchunks å¿…é ˆæ˜¯ List[Document]ï¼Œquestion æ‡‰ç‚º str
    context_text = ""
    citations = []
    citation_map = {}
    
    for i, doc in enumerate(chunks):
        # æª¢æŸ¥ï¼šdoc æ‡‰æœ‰ metadata å±¬æ€§ï¼Œä¸”ç‚º dict
        metadata = doc.metadata
        title = metadata.get("title", "Untitled")
        # æª¢æŸ¥ï¼šfilename ä¾†æºæ–¼ "filename" æˆ– "source"ï¼Œè‹¥éƒ½ç„¡å‰‡ç‚º "Unknown"
        filename = metadata.get("filename") or metadata.get("source", "Unknown")
        # æª¢æŸ¥ï¼špage ä¾†æºæ–¼ "page_number" æˆ– "page"ï¼Œè‹¥éƒ½ç„¡å‰‡ç‚º "?"
        page = metadata.get("page_number") or metadata.get("page", "?")
        # é è¦½ç‰‡æ®µï¼Œå–å‰ 80 å­—å…ƒï¼Œä¸¦å°‡æ›è¡Œæ›¿æ›ç‚ºç©ºæ ¼
        snippet = doc.page_content[:80].replace("\n", " ")

        # æª¢æŸ¥ï¼šé¿å…é‡è¤‡çš„ (filename, page) çµ„åˆ
        citation_key = f"{filename}_p{page}"
        if citation_key not in citation_map:
            label = f"[{len(citations) + 1}]"
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
            citation_map[citation_key] = label
        else:
            label = citation_map[citation_key]

        # context_text ç´¯åŠ æ¯å€‹ chunk çš„å…§å®¹ï¼Œæ ¼å¼ç‚º [n] title | Page n
        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    # system_prompt ç‚ºæœ€çµ‚æç¤ºè©ï¼ŒåŒ…å« context_text åŠ question
    system_prompt = f"""

    ä½ æ˜¯ä¸€ä½ç ”ç©¶æ–‡ç»æœå°‹åŠ©ç†ï¼Œåƒ…æ ¹æ“šæä¾›çš„æ–‡ç»æ®µè½ä¾†ä¸¦å›ç­”å•é¡Œã€‚
    è«‹åœ¨å›ç­”ä¸­ä½¿ç”¨ [1], [2] ç­‰æ¨™è¨»æ®µè½å‡ºè™•ï¼Œä¸è¦åœ¨çµå°¾é‡è¤‡åˆ—å‡ºä¾†æºã€‚
    å¦‚æ®µè½ä¸­æœ‰æåˆ°å…·é«”çš„å¯¦é©—æ¢ä»¶ï¼ˆæº«åº¦ã€æ™‚é–“ç­‰ï¼‰ï¼Œè«‹å‹™å¿…åŒ…å«åœ¨å›ç­”ä¸­ã€‚
    é‡è¦ï¼šåªèƒ½å¼•ç”¨æä¾›çš„æ–‡ç»æ®µè½ï¼Œç•¶å‰æä¾›çš„æ–‡ç»æ®µè½ç·¨è™Ÿç‚º [1] åˆ° [{len(chunks)}]ï¼ˆå…± {len(chunks)} å€‹æ®µè½ï¼‰

    --- æ–‡ç»æ‘˜è¦ ---
    {context_text}


    --- å•é¡Œ ---
    {question}
    """
    # æª¢æŸ¥ï¼šå›å‚³ system_prompt å»é™¤é¦–å°¾ç©ºç™½ï¼Œä»¥åŠ citations åˆ—è¡¨
    return system_prompt.strip(), citations


def call_llm(prompt: str) -> str:
    print(f"ğŸ” èª¿ç”¨ LLMï¼Œæç¤ºè©é•·åº¦ï¼š{len(prompt)} å­—ç¬¦")
    try:
        llm = ChatOpenAI(**LLM_PARAMS)
        response = llm.invoke(prompt)
        print(f"âœ… LLM èª¿ç”¨æˆåŠŸï¼Œå›æ‡‰é•·åº¦ï¼š{len(response.content)} å­—ç¬¦")
        print(f"ğŸ“ LLM å›æ‡‰é è¦½ï¼š{response.content[:200]}...")
        return response.content
    except Exception as e:
        print(f"âŒ LLM èª¿ç”¨å¤±æ•—ï¼š{e}")
        return ""

def build_inference_prompt(chunks: List[Document], question: str) -> Tuple[str, List[Dict]]:
    context_text = ""
    citations = []
    citation_map = {}
    
    for i, doc in enumerate(chunks):
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        
        # æª¢æŸ¥ï¼šé¿å…é‡è¤‡çš„ (filename, page) çµ„åˆ
        citation_key = f"{filename}_p{page}"
        if citation_key not in citation_map:
            label = f"[{len(citations) + 1}]"
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
            citation_map[citation_key] = label
        else:
            label = citation_map[citation_key]

        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½ææ–™åˆæˆé¡§å•ï¼Œä½ äº†è§£ä¸¦å–„æ–¼æ¯”è¼ƒææ–™é–“æ–¼åŒ–å­¸ã€ç‰©ç†æ€§è³ªä¸Šçš„å·®ç•°ï¼Œèƒ½å¤ é‡å°å°šæœªæœ‰æ˜ç¢ºæ–‡ç»çš„æƒ…å¢ƒï¼Œæ ¹æ“šå·²çŸ¥ä¹‹å¯¦é©—æ¢ä»¶æ¨è«–å‡ºå‰µæ–°å»ºè­°ã€‚

    è«‹æ ¹æ“šä»¥ä¸‹æ–‡ç»èˆ‡å¯¦é©—è³‡æ–™ï¼Œé€²è¡Œå»¶ä¼¸æ€è€ƒï¼š
    - ä½ å¯ä»¥æå‡ºæ–°çš„çµ„åˆã€æº«åº¦ã€æ™‚é–“æˆ–è·¯å¾‘ã€‚
    - å³ä½¿æ–‡ç»ä¸­å°šæœªè¨˜è¼‰çš„çµ„åˆä¹Ÿå¯ä»¥å»ºè­°ï¼Œä½†è¦æå‡ºåˆç†æ¨è«–ã€‚
    - æ¨è«–ã€å»¶ä¼¸æ€è€ƒçš„åŒæ™‚ï¼Œè«‹å„˜å¯èƒ½æåˆ°ã€Œé€™æ¨£çš„æƒ³æ³•æºæ–¼å“ªç¨®æ–‡ç»ç·šç´¢ã€ä¾†è¼”åŠ©è§£é‡‹ï¼Œç•¶å‰æä¾›çš„æ–‡ç»æ®µè½ç·¨è™Ÿç‚º [1] åˆ° [{len(chunks)}]ï¼ˆå…± {len(chunks)} å€‹æ®µè½ï¼‰

    --- æ–‡ç»æ‘˜è¦ ---
    {context_text}

    --- å•é¡Œ ---
    {question}
    """
    return system_prompt.strip(), citations

def build_dual_inference_prompt(
    chunks_paper: List[Document],
    question: str,
    experiment_chunks: List[Document]
    ) -> Tuple[str, List[Dict]]:

    paper_context_text = ""
    exp_context_text = ""
    citations = []
    label_index = 1

    # --- æ–‡ç»æ‘˜è¦ ---
    paper_context_text += "--- æ–‡ç»æ‘˜è¦ ---\n"
    for doc in chunks_paper:
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{label_index}]"

        citations.append({
            "label": label,
            "title": title,
            "source": filename,
            "page": page,
            "snippet": snippet,
            "type": "paper"
        })

        paper_context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"
        label_index += 1

    # --- å¯¦é©—æ‘˜è¦ ---
    exp_context_text += "--- é¡ä¼¼å¯¦é©—æ‘˜è¦ ---\n"
    for doc in experiment_chunks:
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        exp_id = meta.get("exp_id", "unknown_exp")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{label_index}]"

        citations.append({
            "label": label,
            "title": exp_id,
            "source": filename,
            "page": "-",  # æ²’æœ‰é æ•¸
            "snippet": snippet,
            "type": "experiment"
        })

        exp_context_text += f"{label} å¯¦é©— {exp_id}\n{doc.page_content}\n\n"
        label_index += 1
        
    # --- Prompt æ³¨å…¥ ---
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½ææ–™åˆæˆé¡§å•ï¼Œä½ äº†è§£ä¸¦å–„æ–¼æ¯”è¼ƒææ–™åœ¨åŒ–å­¸èˆ‡ç‰©ç†æ€§è³ªä¸Šçš„å·®ç•°ã€‚

    ä½ å°‡çœ‹åˆ°ä¸‰å€‹éƒ¨åˆ†è³‡è¨Šï¼Œè«‹ç¶œåˆåˆ†æï¼Œå°å¯¦é©—é€²è¡Œå…·é«”æ¨è«–èˆ‡å‰µæ–°å»ºè­°ï¼š
    1. æ–‡ç»æ‘˜è¦ï¼ˆæœ‰æ¨™è¨»ä¾†æº [1]ã€[2]ï¼‰
    2. é¡ä¼¼å¯¦é©—æ‘˜è¦ï¼ˆä¾†è‡ªå‘é‡è³‡æ–™åº«ï¼‰
    3. å¯¦é©—ç´€éŒ„ï¼ˆè¡¨æ ¼ï¼‰

    è«‹é‡å°ç ”ç©¶å•é¡Œæå‡ºæ–°çš„å»ºè­°ï¼ŒåŒ…å«ï¼š
    - èª¿æ•´çš„åˆæˆè·¯å¾‘ã€æ¢ä»¶ï¼ˆå¦‚æº«åº¦ã€æ™‚é–“ã€é…æ¯”ï¼‰
    - å¯èƒ½å½±éŸ¿åˆæˆæˆåŠŸç‡çš„è®Šå› 
    - æ¨è«–å…¶èƒŒå¾Œçš„åŸå› ï¼Œå¿…è¦æ™‚å¼•ç”¨æ–‡ç»ï¼ˆ[1]ã€[2]...ï¼‰æˆ–é¡ä¼¼å¯¦é©—çµæœ
    é‡è¦ï¼šåªèƒ½å¼•ç”¨æä¾›çš„æ–‡ç»æ®µè½ï¼Œç•¶å‰æä¾›çš„æ–‡ç»æ®µè½ç·¨è™Ÿç‚º [1] åˆ° [{len(chunks_paper) + len(experiment_chunks)}]ï¼ˆå…± {len(chunks_paper) + len(experiment_chunks)} å€‹æ®µè½ï¼‰

    --- æ–‡ç»çŸ¥è­˜ä¾†æº ---
    {paper_context_text}

    --- å¯¦é©—ç´€éŒ„ ---
    {exp_context_text}

    --- ç ”ç©¶å•é¡Œ ---
    {question}
    """
    return system_prompt.strip(), citations



def expand_query(user_prompt: str) -> List[str]:
    """
    å°‡ä½¿ç”¨è€…è¼¸å…¥çš„è‡ªç„¶èªè¨€å•é¡Œè½‰ç‚ºå¤šå€‹ semantic search æŸ¥è©¢èªå¥ã€‚
    å›å‚³çš„è‹±æ–‡èªå¥å¯ç”¨æ–¼æ–‡ç»å‘é‡æª¢ç´¢ã€‚
    """
    llm = ChatOpenAI(**LLM_PARAMS)

    system_prompt = """You are a scientific assistant helping expand a user's synthesis question into multiple semantic search queries. 
    Each query should be precise, relevant, and useful for retrieving related technical documents. 
    Only return a list of 3 to 6 search queries in English. Do not explain, do not include numbering if not needed.

    ä½ æ˜¯ä¸€ä½ç§‘å­¸åŠ©ç†ï¼Œå”åŠ©å°‡ä½¿ç”¨è€…çš„åˆæˆç›¸é—œå•é¡Œæ“´å±•ç‚ºå¤šå€‹èªæ„æœå°‹æŸ¥è©¢ã€‚
    æ¯å€‹æŸ¥è©¢éƒ½æ‡‰è©²æº–ç¢ºã€ç›¸é—œï¼Œä¸¦æœ‰åŠ©æ–¼æª¢ç´¢ç›¸é—œçš„æŠ€è¡“æ–‡ä»¶ã€‚
    åªéœ€å›å‚³ 3 åˆ° 6 å€‹è‹±æ–‡æŸ¥è©¢çš„åˆ—è¡¨ã€‚ä¸éœ€è¦è§£é‡‹ï¼Œä¹Ÿä¸éœ€åŠ ä¸Šç·¨è™Ÿï¼ˆé™¤éå¿…è¦ï¼‰ã€‚"""

    full_prompt = f"{system_prompt}\n\nUser question:\n{user_prompt}"

    output = llm.predict(full_prompt).strip()

    # å˜—è©¦è§£ææˆ query list
    if output.startswith("[") and output.endswith("]"):
        try:
            return eval(output)
        except Exception:
            pass  # fall back

    queries = [line.strip("-â€¢ ").strip() for line in output.split("\n") if line.strip()]
    return [q for q in queries if len(q) > 4]


def build_proposal_prompt(chunks: List[Document], question: str) -> Tuple[str, List[Dict]]:
    paper_context_text = ""
    citations = []
    citation_map = {}

    for i, doc in enumerate(chunks):
        
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        
        # æª¢æŸ¥ï¼šé¿å…é‡è¤‡çš„ (filename, page) çµ„åˆ
        citation_key = f"{filename}_p{page}"
        if citation_key not in citation_map:
            label = f"[{len(citations) + 1}]"
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
            citation_map[citation_key] = label
        else:
            label = citation_map[citation_key]

        paper_context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½ææ–™åŒ–å­¸å°ˆå®¶ï¼Œæ“…é•·æ ¹æ“šæ–‡ç»æ‘˜è¦èˆ‡ç ”ç©¶ç›®æ¨™ï¼Œæå‡ºå…·æœ‰å‰µæ–°æ€§èˆ‡å¯è¡Œæ€§çš„ææ–™åˆæˆç ”ç©¶ææ¡ˆã€‚
    å¦‚èƒ½æ ¹æ“šæ–‡ç»æå‡ºæ–°çš„é…é«”ç¨®é¡ï¼ˆå¦‚ pyridyl, biphenyl, sulfonateï¼‰æˆ–é‡‘å±¬ç¯€é»ï¼ˆå¦‚ Zn, Mg, Zrï¼‰ï¼Œè«‹å…·é«”åˆ—å‡ºä¸¦èªªæ˜å…¶çµæ§‹å„ªå‹¢èˆ‡åæ‡‰æ€§ã€‚

    è«‹ä½ æ ¹æ“šä¸‹æ–¹æ–‡ç»å…§å®¹èˆ‡ç ”ç©¶ç›®æ¨™ï¼Œæ’°å¯«ä¸€ä»½çµæ§‹åŒ–ææ¡ˆï¼Œæ ¼å¼å¦‚ä¸‹ï¼ˆè«‹å®Œæ•´ç”¢å‡ºæ¯ä¸€æ®µï¼‰ï¼š

    ---
    Proposal: ï¼ˆè«‹è‡ªå‹•ç”¢ç”Ÿææ¡ˆæ¨™é¡Œï¼Œæ¿ƒç¸®ç ”ç©¶ç›®æ¨™èˆ‡å‰µæ–°é»ï¼‰

    Need:
    - ç°¡è¿°ç ”ç©¶ç›®æ¨™èˆ‡ç›®å‰ææ–™åœ¨æ‡‰ç”¨ä¸Šçš„é™åˆ¶
    - æ˜ç¢ºæŒ‡å‡ºç”¢æ¥­ã€ä¸–ç•Œã€æ”¿åºœç­‰å¾…è§£æ±ºçš„ç—›é»æˆ–æŠ€è¡“ç“¶é ¸

    Solution:
    - æå‡ºå…·é«”çš„ææ–™è¨­è¨ˆèˆ‡åˆæˆç­–ç•¥
    - å»ºè­°æ–°çš„åŒ–å­¸çµæ§‹ï¼ˆå¦‚é‡‘å±¬ã€æœ‰æ©Ÿé…é«”ã€åŠŸèƒ½å®˜èƒ½åŸº)ï¼Œæ˜ç¢ºæŒ‡å‡ºåŒ–å­¸çµæ§‹åç¨±(ä¾‹å¦‚é…é«”åç¨±)
    - è«‹èªªæ˜æ–°è¨­è¨ˆçš„åŒ–å­¸é‚è¼¯ï¼ˆå¦‚é…ä½ç’°å¢ƒã€å­”å¾‘ã€å®˜èƒ½åŸºä½œç”¨ï¼‰

    Differentiation:
    - åˆ—è¡¨æ¯”è¼ƒèˆ‡ç¾æœ‰æ–‡ç»ææ–™çš„å·®ç•°
    - å¼·èª¿çµæ§‹ã€æ¢ä»¶æˆ–æ€§èƒ½ä¸Šçš„çªç ´

    Benefit:
    - é æœŸæ”¹å–„çš„æ€§èƒ½æˆ–æ‡‰ç”¨é¢å‘
    - åˆ—å‡ºé‡åŒ–çš„é ä¼°å€¼ï¼ˆå¦‚æå‡ COâ‚‚ å¸é™„é‡ã€ç©©å®šæ€§ã€é¸æ“‡æ€§ï¼‰

    Based Literature:
    - ä½¿ç”¨æ®µè½æ¨™ç±¤åˆ—å‡ºå¼•ç”¨çš„ä¾æ“šï¼Œæ¯æ®µè©±éƒ½éœ€è¦æä¾›æ–‡ç»æ®µè½ç·¨è™Ÿï¼Œæ–‡ç»æ®µè½ç·¨è™Ÿç‚º [1] åˆ° [{len(chunks)}]ï¼ˆå…± {len(chunks)} å€‹æ®µè½ï¼‰
    - è«‹ç¢ºä¿æ¯å€‹æ¨è«–éƒ½æœ‰æ–‡ç»å°ç…§ä¾æ“šæˆ–åˆç†å»¶ä¼¸ç†ç”±

    Experimental overview:
    1. ä½¿ç”¨çš„èµ·å§‹ææ–™èˆ‡åæ‡‰æ¢ä»¶ï¼ˆå¦‚æº«åº¦ã€æ™‚é–“ï¼‰
    2. åˆæˆä½¿ç”¨å„€å™¨ã€è¨­å‚™æè¿°ä¸¦åˆ—è¡¨
    3. åˆæˆæ­¥é©Ÿæ•˜è¿°ï¼ˆæè¿°é—œéµé‚è¼¯ï¼‰



    æœ€å¾Œï¼Œè«‹åœ¨å›ç­”æœ€æœ«æ®µï¼Œä»¥ JSON æ ¼å¼åˆ—å‡ºæ­¤ææ¡ˆæ‰€ä½¿ç”¨åˆ°çš„æ‰€æœ‰åŒ–å­¸å“ï¼ˆåŒ…å«é‡‘å±¬é¹½ã€æœ‰æ©Ÿé…é«”ã€æº¶åŠ‘ç­‰ï¼‰ã€‚ä»¥IUPAC Nameå›ç­”ï¼Œæ ¼å¼èˆ‡ç¯„ä¾‹å¦‚ä¸‹ï¼š

    ```markdown

    ```json
    ["formic acid", "N,N-dimethylformamide", "copper;dinitrate;trihydrate"]
    ```
    ---

    è«‹æ³¨æ„ï¼š
    - çµæ§‹å»ºè­°è¦æœ‰é‚è¼¯ä¾æ“šï¼Œé¿å…éš¨æ„ç·¨é€ ä¸åˆç†çµæ§‹
    - è«‹ä¿æŒç§‘å­¸æ€§ã€å¯è®€æ€§ï¼Œä¸¦é¿å…ç©ºæ³›æ•˜è¿°
    - å¼•ç”¨æ¨™è¨»è«‹ä½¿ç”¨æä¾›çš„æ–‡ç»æ®µè½æ¨™ç±¤ [1]ã€[2]ï¼Œä¸è¦ä½¿ç”¨è™›æ§‹ä¾†æº

    --- æ–‡ç»æ®µè½ ---
    {paper_context_text}

    --- ç ”ç©¶ç›®æ¨™ ---
    {question}

    """
    
    return system_prompt.strip(), citations

def build_detail_experimental_plan_prompt(chunks: List[Document], proposal_text: str) -> Tuple[str, List[Dict]]:
    context_text = ""
    citations = []
    citation_map = {}
    
    for i, doc in enumerate(chunks):
        # æª¢æŸ¥ï¼šdoc æ‡‰æœ‰ metadata å±¬æ€§ï¼Œä¸”ç‚º dict
        metadata = doc.metadata
        title = metadata.get("title", "Untitled")
        # æª¢æŸ¥ï¼šfilename ä¾†æºæ–¼ "filename" æˆ– "source"ï¼Œè‹¥éƒ½ç„¡å‰‡ç‚º "Unknown"
        filename = metadata.get("filename") or metadata.get("source", "Unknown")
        # æª¢æŸ¥ï¼špage ä¾†æºæ–¼ "page_number" æˆ– "page"ï¼Œè‹¥éƒ½ç„¡å‰‡ç‚º "?"
        page = metadata.get("page_number") or metadata.get("page", "?")
        # é è¦½ç‰‡æ®µï¼Œå–å‰ 80 å­—å…ƒï¼Œä¸¦å°‡æ›è¡Œæ›¿æ›ç‚ºç©ºæ ¼
        snippet = doc.page_content[:80].replace("\n", " ")

        # æª¢æŸ¥ï¼šé¿å…é‡è¤‡çš„ (filename, page) çµ„åˆ
        citation_key = f"{filename}_p{page}"
        if citation_key not in citation_map:
            label = f"[{len(citations) + 1}]"
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
            citation_map[citation_key] = label
        else:
            label = citation_map[citation_key]

        # context_text ç´¯åŠ æ¯å€‹ chunk çš„å…§å®¹ï¼Œæ ¼å¼ç‚º [n] title | Page n
        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
    You are an experienced consultant in materials experiment design. Based on the following research proposal and related literature excerpts, please provide the researcher with a detailed set of recommended experimental procedures:

    Please include:

    Synthesis Process: A step-by-step description of each experimental operation, including sequence, logic, and purpose. 
    guidline for synthesis process:
    - Suggest a specific range of experimental conditions(temperature, time, pressure, etc.)
    - For each reaction conditons and steps that have been mentioned in the literature, make sure to cite ([1],[2], ...).
    - For suggested, not literature-based conditions, explain the your logic to convince the user.

    Materials and Conditions: The required raw materials for each step (including proportions), and the reaction conditions (temperature, time, containers).

    Analytical Methods: Suggested characterization tools (such as XRD, BET, TGA) and the purpose for each.

    Precautions: Key points or parameter limitations mentioned in the literature.

    Please clearly list the information in bullet points or paragraphs.
    Please use [1], [2], etc. to cite the literature sources in your response, åªèƒ½å¼•ç”¨æä¾›çš„æ–‡ç»æ®µè½ï¼Œç•¶å‰æä¾›çš„æ–‡ç»æ®µè½ç·¨è™Ÿç‚º [1] åˆ° [{len(chunks)}]ï¼ˆå…± {len(chunks)} å€‹æ®µè½ï¼‰

    --- literature chunks ---
    {context_text}

    --- User's Proposal ---
    {proposal_text}
    """
    return system_prompt.strip(), citations



def build_iterative_proposal_prompt(
    question: str,
    new_chunks: List[Document],
    old_chunks: List[Document],
    past_proposal: str
) -> Tuple[str, List[Dict]]:
    """
    å»ºç«‹æ–°çš„ç ”ç©¶ææ¡ˆ promptï¼Œçµåˆä½¿ç”¨è€…çš„åé¥‹ã€æ–°æª¢ç´¢æ–‡ç»ã€èˆŠæ–‡ç»èˆ‡åŸå§‹ææ¡ˆã€‚
    åŒæ™‚å›å‚³ citation listã€‚
    """
    citations = []

    def format_chunks(chunks: List[Document], label_offset=0) -> Tuple[str, List[Dict]]:
        text = ""
        local_citations = []
        for i, doc in enumerate(chunks):
            meta = doc.metadata
            title = meta.get("title", "Untitled")
            filename = meta.get("filename") or meta.get("source", "Unknown")
            page = meta.get("page_number") or meta.get("page", "?")
            snippet = doc.page_content[:80].replace("\n", " ")
            label = f"[{label_offset + i + 1}]"

            local_citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })

            text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

        return text, local_citations

    old_text, old_citations = format_chunks(old_chunks)
    new_text, new_citations = format_chunks(new_chunks, label_offset=len(old_citations))
    citations.extend(old_citations + new_citations)

    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½ç†Ÿç·´çš„ææ–™å¯¦é©—è¨­è¨ˆé¡§å•ï¼Œè«‹æ ¹æ“šä½¿ç”¨è€…æä¾›çš„åé¥‹ã€åŸå§‹ææ¡ˆã€èˆ‡æ–‡ç»å…§å®¹ï¼Œå”åŠ©ä¿®æ”¹éƒ¨åˆ†çš„ç ”ç©¶ææ¡ˆã€‚

    è«‹ä¾å¾ªä»¥ä¸‹æŒ‡å¼•ï¼š
    1. å„ªå…ˆè™•ç†ä½¿ç”¨è€…æå‡ºæ¬²ä¿®æ”¹ä¹‹è™•ï¼Œä¸¦å¾æ–‡ç»ä¸­å°‹æ‰¾å¯èƒ½çš„æ”¹é€²æ–¹å‘ã€‚
    2. é™¤äº†ä½¿ç”¨è€…æå‡ºä¹‹ä¸æ»¿æ„è™•é€²è¡Œä¿®æ”¹å¤–ï¼Œå…¶ä»–éƒ¨åˆ†(NSDB)è«‹ä¿æŒåŸææ¡ˆå…§å®¹ï¼Œä¸é ˆæ›´å‹•ï¼Œç›´æ¥è¼¸å‡º
    3. ä¿æŒææ¡ˆæ ¼å¼èˆ‡åŸå§‹ææ¡ˆå…§å®¹ä¸€è‡´ï¼Œä¸¦è«‹æ–¼ç¬¬ä¸€æ®µç°¡è¦èªªæ˜æ›´æ”¹çš„æ–¹å‘ï¼Œç¸½å…±åŒ…å«ä¸‹åˆ—å€å¡Šï¼š
    - revision explanation: ç°¡è¦èªªæ˜æœ¬æ¬¡ææ¡ˆèˆ‡å‰æ¬¡ææ¡ˆçš„å·®ç•°
    - Need
    - Solution
    - Differentiation
    - Benefit
    - Based Literature
    - Experimental overview
    é‡è¦ï¼šåªèƒ½å¼•ç”¨æä¾›çš„æ–‡ç»æ®µè½ï¼Œç•¶å‰æä¾›çš„æ–‡ç»æ®µè½ç·¨è™Ÿç‚º [1] åˆ° [{len(old_chunks) + len(new_chunks)}]ï¼ˆå…± {len(old_chunks) + len(new_chunks)} å€‹æ®µè½ï¼‰

    æœ€å¾Œï¼Œè«‹åœ¨å›ç­”æœ€æœ«æ®µï¼Œä»¥ JSON æ ¼å¼åˆ—å‡ºæ­¤ææ¡ˆæ‰€ä½¿ç”¨åˆ°çš„æ‰€æœ‰åŒ–å­¸å“ï¼ˆåŒ…å«é‡‘å±¬é¹½ã€æœ‰æ©Ÿé…é«”ã€æº¶åŠ‘ç­‰ï¼‰ã€‚ä»¥ IUPAC Name å›ç­”ï¼Œå›ç­”æ ¼å¼èˆ‡ç¯„ä¾‹å¦‚ä¸‹ï¼š

    ```json
    ["formic acid", "N,N-dimethylformamide", "copper;dinitrate;trihydrate"]
    --- ä½¿ç”¨è€…çš„åé¥‹ ---
    {question}

    --- åŸå§‹ææ¡ˆå…§å®¹ ---
    {past_proposal}

    --- åŸå§‹ææ¡ˆæ‰€åŸºæ–¼çš„æ–‡ç»æ®µè½ ---
    {old_text}

    --- æ ¹æ“šåé¥‹è£œå……çš„æ–°æª¢ç´¢æ®µè½ ---
    {new_text}
    """
    return system_prompt.strip(), citations


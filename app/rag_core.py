from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain.chat_models import ChatOpenAI
from config import VECTOR_INDEX_DIR, EMBEDDING_MODEL_NAME
import pandas as pd
from typing import List, Tuple, Dict
import re


def load_vectorstore():
    embedding_model = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={"trust_remote_code": True}
    )
    vectorstore = Chroma(persist_directory=VECTOR_INDEX_DIR, embedding_function=embedding_model)
    return vectorstore


def retrieve_chunks(vectorstore, query: str, k: int = 10, fetch_k: int = 30, score_threshold: float = 0.35) -> List[Document]:
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": fetch_k, "score_threshold": score_threshold}
    )
    context_docs = retriever.get_relevant_documents(query)

    if not context_docs:
        print("âš ï¸ æ²’æœ‰æŠ“åˆ°ä»»ä½•æ®µè½ï¼Œå»ºè­°æª¢æŸ¥ retriever æˆ–åµŒå…¥æ ¼å¼")
    else:
        print("ðŸ” Retriever æŠ“åˆ°çš„æ®µè½ï¼š")
        for i, doc in enumerate(context_docs, 1):
            meta = doc.metadata
            filename = meta.get("filename") or meta.get("source", "Unknown")
            page = meta.get("page_number") or meta.get("page", "?")
            preview = doc.page_content[:100].replace("\n", " ")
            print(f"--- Chunk {i} ---")
            print(f"ðŸ“„ {filename} | Page {page}")
            print(preview)
            print()

    return context_docs


def build_prompt(chunks: List[Document], df: pd.DataFrame, question: str) -> Tuple[str, List[Dict]]:
    context_text = ""
    citations = []
    citation_map = {}
    
    for i, doc in enumerate(chunks):
        metadata = doc.metadata
        title = metadata.get("title", "Untitled")
        filename = metadata.get("filename") or metadata.get("source", "Unknown")
        page = metadata.get("page_number") or metadata.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")

        label = f"[{i+1}]"
 
        if label not in citation_map.values():
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
        citation_map[f"{filename}_p{page}"] = label

        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    past_exp = df.to_string(index=False) if not df.empty else "ç„¡ç´€éŒ„"

    system_prompt = f"""
ä½ æ˜¯ä¸€ä½ç ”ç©¶åŠ©ç†ï¼Œåƒ…æ ¹æ“šä¸‹åˆ—æ–‡ç»æ®µè½èˆ‡å¯¦é©—ç´€éŒ„å›žç­”å•é¡Œã€‚
è«‹åœ¨å›žç­”ä¸­ä½¿ç”¨ [1], [2] ç­‰æ¨™è¨»æ®µè½å‡ºè™•ï¼Œä¸è¦åœ¨çµå°¾é‡è¤‡åˆ—å‡ºä¾†æºã€‚
å¦‚æ®µè½ä¸­æœ‰æåˆ°å…·é«”çš„å¯¦é©—æ¢ä»¶ï¼ˆæº«åº¦ã€æ™‚é–“ç­‰ï¼‰ï¼Œè«‹å‹™å¿…åŒ…å«åœ¨å›žç­”ä¸­ã€‚
--- æ–‡ç»æ‘˜è¦ ---
{context_text}

--- å¯¦é©—ç´€éŒ„ ---
{past_exp}

--- å•é¡Œ ---
{question}
"""
    return system_prompt.strip(), citations


def call_llm(prompt: str) -> str:
    llm = ChatOpenAI(model_name="gpt-4-1106-preview", temperature=0)
    return llm.predict(prompt)

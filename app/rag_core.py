"""
RAGæ ¸å¿ƒæ¨¡çµ„
==========

åŸºæ–¼æª¢ç´¢å¢å¼·ç”Ÿæˆçš„AIç ”ç©¶åŠ©æ‰‹æ ¸å¿ƒåŠŸèƒ½
æ•´åˆæ–‡ç»æª¢ç´¢ã€çŸ¥è­˜æå–å’Œæ™ºèƒ½å•ç­”
"""

import os
import json
import time
import re
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from collections import defaultdict


# å°å…¥å¿…è¦çš„æ¨¡çµ„
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from chunk_embedding import get_chroma_instance
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
import openai

# å°å…¥é…ç½®å’Œæ©‹æ¥æ¨¡çµ„
from config import (
    OPENAI_API_KEY, 
    VECTOR_INDEX_DIR, 
    EMBEDDING_MODEL_NAME,
    MAX_TOKENS,
    CHUNK_SIZE,
    CHUNK_OVERLAP
)
from model_config_bridge import get_model_params, get_current_model

# è¨­å®šOpenAI API Key
openai.api_key = OPENAI_API_KEY

def get_dynamic_schema_params():
    """
    å¾è¨­å®šç®¡ç†å™¨ç²å–å‹•æ…‹çš„ JSON Schema åƒæ•¸
    
    Returns:
        Dict: åŒ…å« min_length å’Œ max_length çš„å­—å…¸
    """
    try:
        # å°å…¥è¨­å®šç®¡ç†å™¨
        import sys
        backend_path = os.path.join(os.path.dirname(__file__), "..", "backend")
        if backend_path not in sys.path:
            sys.path.insert(0, backend_path)
        
        from core.settings_manager import settings_manager
        json_schema_params = settings_manager.get_json_schema_parameters()
        
        return {
            "min_length": json_schema_params.get("min_length", 5),
            "max_length": json_schema_params.get("max_length", 100)
        }
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å–å‹•æ…‹ schema åƒæ•¸ï¼Œä½¿ç”¨é è¨­å€¼: {e}")
        return {
            "min_length": 5,
            "max_length": 100
        }

def create_research_proposal_schema():
    """
    å‹•æ…‹å‰µå»ºç ”ç©¶ææ¡ˆçš„ JSON Schema
    
    Returns:
        Dict: ç ”ç©¶ææ¡ˆçš„ JSON Schema
    """
    schema_params = get_dynamic_schema_params()
    
    return {
        "type": "object",
        "title": "ResearchProposal",
        "additionalProperties": False,
        "required": [
            "proposal_title",
            "need",
            "solution", 
            "differentiation",
            "benefit",
            "experimental_overview",
            "materials_list"
        ],
        "properties": {
            "proposal_title": {
                "type": "string",
                "description": "ç ”ç©¶ææ¡ˆçš„æ¨™é¡Œï¼Œç¸½çµç ”ç©¶ç›®æ¨™å’Œå‰µæ–°é»",
                "minLength": 10
                #"maxLength": 1000
            },
            "need": {
                "type": "string", 
                "description": "ç ”ç©¶éœ€æ±‚å’Œç¾æœ‰è§£æ±ºæ–¹æ¡ˆçš„å±€é™æ€§ï¼Œæ˜ç¢ºéœ€è¦è§£æ±ºçš„æŠ€è¡“ç“¶é ¸",
                "minLength": 10
                #"maxLength": 600  # å…è¨±æ›´é•·çš„æè¿°
            },
            "solution": {
                "type": "string",
                "description": "å…·é«”çš„è¨­è¨ˆå’Œé–‹ç™¼ç­–ç•¥ï¼ŒåŒ…æ‹¬æ–°çš„çµæ§‹ã€çµ„æˆæˆ–æ–¹æ³•",
                "minLength": 10
                #"maxLength": 1000
            },
            "differentiation": {
                "type": "string",
                "description": "èˆ‡ç¾æœ‰æ–‡ç»æˆ–æŠ€è¡“çš„æ¯”è¼ƒï¼Œå¼·èª¿çµæ§‹ã€æ€§èƒ½æˆ–å¯¦æ–½æ–¹é¢çš„çªç ´",
                "minLength": 10
                #"maxLength": 800
            },
            "benefit": {
                "type": "string",
                "description": "é æœŸçš„æ€§èƒ½æ”¹é€²æˆ–æ‡‰ç”¨ç¯„åœæ“´å±•ï¼Œç›¡å¯èƒ½æä¾›å®šé‡ä¼°è¨ˆ",
                "minLength": 10
                #"maxLength": 600
            },
            "experimental_overview": {
                "type": "string",
                "description": "å¯¦é©—æ¦‚è¿°ï¼ŒåŒ…æ‹¬èµ·å§‹ææ–™ã€æ¢ä»¶ã€å„€å™¨è¨­å‚™å’Œæ­¥é©Ÿæè¿°",
                "minLength": 10
                #"maxLength": 600
            },
            "materials_list": {
                "type": "array",
                "items": {
                    "type": "string"
                },
                "description": "CRITICAL: åƒ…åˆ—å‡ºIUPACåŒ–å­¸å“åç¨±ï¼Œæ¯å€‹é …ç›®å¿…é ˆæ˜¯å–®ä¸€åŒ–å­¸å“åç¨±ï¼Œä¸åŒ…å«ä»»ä½•æè¿°ã€å‚™è¨»ã€æ‹¬è™Ÿèªªæ˜æˆ–å…¶ä»–æ–‡å­—ã€‚åš´ç¦åŒ…å«å¦‚'(dobpdc)'ã€'(representative...)'ã€'(trifluoromethyl-substituted...)'ç­‰æ‹¬è™Ÿå…§å®¹ã€‚æ¯å€‹åŒ–å­¸å“åç¨±å¿…é ˆæ˜¯æ¨™æº–çš„IUPACå‘½åï¼Œä¾‹å¦‚ï¼šmagnesium nitrate hexahydrate, 4,4'-dioxidobiphenyl-3,3'-dicarboxylic acid, 2-(2,2,2-trifluoroethylamino)ethan-1-amine, N,N-dimethylacetamide",
                "minItems": 1
            }
        }
    }

def create_experimental_detail_schema():
    """
    å‹•æ…‹å‰µå»ºå¯¦é©—ç´°ç¯€çš„ JSON Schema
    
    Returns:
        Dict: å¯¦é©—ç´°ç¯€çš„ JSON Schema
    """
    schema_params = get_dynamic_schema_params()
    
    # æ ¹æ“šæ¸¬è©¦å ±å‘Šï¼Œç°¡åŒ– schema ä»¥é¿å…éé•·è¼¸å‡º
    return {
        "type": "object",
        "title": "ExperimentalDetail",
        "additionalProperties": False,
        "required": [
            "synthesis_process",
            "materials_and_conditions",
            "analytical_methods",
            "precautions"
        ],
        "properties": {
            "synthesis_process": {
                "type": "string",
                "description": "è©³ç´°çš„åˆæˆéç¨‹ï¼ŒåŒ…æ‹¬æ­¥é©Ÿã€æ¢ä»¶ã€æ™‚é–“ç­‰",
                "minLength": 10
                #"maxLength": 1000  # é™åˆ¶é•·åº¦é¿å…éé•·è¼¸å‡º
            },
            "materials_and_conditions": {
                "type": "string",
                "description": "ä½¿ç”¨çš„ææ–™å’Œåæ‡‰æ¢ä»¶ï¼ŒåŒ…æ‹¬æ¿ƒåº¦ã€æº«åº¦ã€å£“åŠ›ç­‰",
                "minLength": 10
                #"maxLength": 600  # é™åˆ¶é•·åº¦
            },
            "analytical_methods": {
                "type": "string",
                "description": "åˆ†ææ–¹æ³•å’Œè¡¨å¾µæŠ€è¡“ï¼Œå¦‚XRDã€SEMã€NMRç­‰",
                "minLength": 10
                #"maxLength": 400  # é™åˆ¶é•·åº¦
            },
            "precautions": {
                "type": "string",
                "description": "å¯¦é©—æ³¨æ„äº‹é …å’Œå®‰å…¨é é˜²æªæ–½",
                "minLength": 10
                #"maxLength": 400  # é™åˆ¶é•·åº¦
            }
        }
    }

def create_revision_explain_schema():
    """
    å‹•æ…‹å‰µå»ºä¿®è¨‚èªªæ˜çš„ JSON Schema
    
    Returns:
        Dict: ä¿®è¨‚èªªæ˜çš„ JSON Schema
    """
    schema_params = get_dynamic_schema_params()
    
    return {
        "type": "object",
        "title": "RevisionExplain",
        "additionalProperties": False,
        "required": [
            "revision_explain"
        ],
        "properties": {
            "revision_explain": {
                "type": "string",
                "description": "è©³ç´°èªªæ˜ä¿®è¨‚çš„åŸå› ã€æ”¹é€²é»å’Œæ–°çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬æŠ€è¡“å‰µæ–°é»å’Œé æœŸæ•ˆæœ",
                "minLength": 10
                #"maxLength": 1000  # å…è¨±è¼ƒé•·çš„èªªæ˜
            }
        }
    }

def create_revision_proposal_schema():
    """
    å‹•æ…‹å‰µå»ºä¿®è¨‚ææ¡ˆçš„ JSON Schema (åŒ…å«ä¿®è¨‚èªªæ˜)
    
    Returns:
        Dict: ä¿®è¨‚ææ¡ˆçš„ JSON Schema
    """
    schema_params = get_dynamic_schema_params()
    
    return {
        "type": "object",
        "title": "RevisionProposal",
        "additionalProperties": False,
        "required": [
            "revision_explanation",
            "proposal_title",
            "need",
            "solution", 
            "differentiation",
            "benefit",
            "experimental_overview",
            "materials_list"
        ],
        "properties": {
            "revision_explanation": {
                "type": "string",
                "description": "ç°¡è¦èªªæ˜ä¿®è¨‚çš„é‚è¼¯å’Œä¸»è¦æ”¹é€²é»ï¼ŒåŸºæ–¼ç”¨æˆ¶åé¥‹å°åŸå§‹ææ¡ˆçš„ä¿®æ”¹åŸå› ",
                "minLength": 10
            },
            "proposal_title": {
                "type": "string",
                "description": "ç ”ç©¶ææ¡ˆçš„æ¨™é¡Œï¼Œç¸½çµç ”ç©¶ç›®æ¨™å’Œå‰µæ–°é»",
                "minLength": 10
            },
            "need": {
                "type": "string", 
                "description": "ç ”ç©¶éœ€æ±‚å’Œç¾æœ‰è§£æ±ºæ–¹æ¡ˆçš„å±€é™æ€§ï¼Œæ˜ç¢ºéœ€è¦è§£æ±ºçš„æŠ€è¡“ç“¶é ¸",
                "minLength": 10
            },
            "solution": {
                "type": "string",
                "description": "å…·é«”çš„è¨­è¨ˆå’Œé–‹ç™¼ç­–ç•¥ï¼ŒåŒ…æ‹¬æ–°çš„çµæ§‹ã€çµ„æˆæˆ–æ–¹æ³•",
                "minLength": 10
            },
            "differentiation": {
                "type": "string",
                "description": "èˆ‡ç¾æœ‰æ–‡ç»æˆ–æŠ€è¡“çš„æ¯”è¼ƒï¼Œå¼·èª¿çµæ§‹ã€æ€§èƒ½æˆ–å¯¦æ–½æ–¹é¢çš„çªç ´",
                "minLength": 10
            },
            "benefit": {
                "type": "string",
                "description": "é æœŸçš„æ€§èƒ½æ”¹é€²æˆ–æ‡‰ç”¨ç¯„åœæ“´å±•ï¼Œç›¡å¯èƒ½æä¾›å®šé‡ä¼°è¨ˆ",
                "minLength": 10
            },
            "experimental_overview": {
                "type": "string",
                "description": "å¯¦é©—æ¦‚è¿°ï¼ŒåŒ…æ‹¬èµ·å§‹ææ–™ã€æ¢ä»¶ã€å„€å™¨è¨­å‚™å’Œæ­¥é©Ÿæè¿°",
                "minLength": 10
            },
            "materials_list": {
                "type": "array",
                "items": {
                    "type": "string"
                },
                "description": "CRITICAL: åƒ…åˆ—å‡ºIUPACåŒ–å­¸å“åç¨±ï¼Œæ¯å€‹é …ç›®å¿…é ˆæ˜¯å–®ä¸€åŒ–å­¸å“åç¨±ï¼Œä¸åŒ…å«ä»»ä½•æè¿°ã€å‚™è¨»ã€æ‹¬è™Ÿèªªæ˜æˆ–å…¶ä»–æ–‡å­—ã€‚åš´ç¦åŒ…å«å¦‚'(dobpdc)'ã€'(representative...)'ã€'(trifluoromethyl-substituted...)'ç­‰æ‹¬è™Ÿå…§å®¹ã€‚æ¯å€‹åŒ–å­¸å“åç¨±å¿…é ˆæ˜¯æ¨™æº–çš„IUPACå‘½åï¼Œä¾‹å¦‚ï¼šmagnesium nitrate hexahydrate, 4,4'-dioxidobiphenyl-3,3'-dicarboxylic acid, 2-(2,2,2-trifluoroethylamino)ethan-1-amine, N,N-dimethylacetamide",
                "minItems": 1
            }
        }
    }

# å‹•æ…‹ç”Ÿæˆ JSON Schema
RESEARCH_PROPOSAL_SCHEMA = create_research_proposal_schema()
EXPERIMENTAL_DETAIL_SCHEMA = create_experimental_detail_schema()
REVISION_EXPLAIN_SCHEMA = create_revision_explain_schema()
REVISION_PROPOSAL_SCHEMA = create_revision_proposal_schema()

# Embedding model configuration

# ==================== å‘é‡æ•¸æ“šåº«ç®¡ç† ====================

def load_paper_vectorstore():
    """
    è¼‰å…¥æ–‡ç»å‘é‡æ•¸æ“šåº«
    
    åŠŸèƒ½ï¼š
    1. ç²å–æˆ–å‰µå»ºæ–‡ç»å‘é‡æ•¸æ“šåº«å¯¦ä¾‹
    2. ä½¿ç”¨ç·©å­˜é¿å…é‡è¤‡å‰µå»º
    
    è¿”å›ï¼š
        Chroma: æ–‡ç»å‘é‡æ•¸æ“šåº«å°è±¡
    
    æŠ€è¡“ç´°ç¯€ï¼š
    - ä½¿ç”¨ç·©å­˜çš„ Chroma å¯¦ä¾‹
    - æŒä¹…åŒ–å­˜å„²åœ¨paper_vectorç›®éŒ„
    - é›†åˆåç¨±ç‚º"paper"
    """
    return get_chroma_instance("paper")


def load_experiment_vectorstore():
    """
    è¼‰å…¥å¯¦é©—æ•¸æ“šå‘é‡æ•¸æ“šåº«
    
    åŠŸèƒ½ï¼š
    1. ç²å–æˆ–å‰µå»ºå¯¦é©—æ•¸æ“šå‘é‡æ•¸æ“šåº«å¯¦ä¾‹
    2. ä½¿ç”¨ç·©å­˜é¿å…é‡è¤‡å‰µå»º
    
    è¿”å›ï¼š
        Chroma: å¯¦é©—æ•¸æ“šå‘é‡æ•¸æ“šåº«å°è±¡
    
    æŠ€è¡“ç´°ç¯€ï¼š
    - ä½¿ç”¨ç·©å­˜çš„ Chroma å¯¦ä¾‹
    - æŒä¹…åŒ–å­˜å„²åœ¨experiment_vectorç›®éŒ„
    - é›†åˆåç¨±ç‚º"experiment"
    """
    return get_chroma_instance("experiment")


# ==================== æ–‡æª”æª¢ç´¢åŠŸèƒ½ ====================

def retrieve_chunks_multi_query(
    vectorstore, query_list: List[str], k: int = 10, fetch_k: int = 20, score_threshold: float = 0.35
    ) -> List[Document]:
    """
    å¤šæŸ¥è©¢æ–‡æª”æª¢ç´¢åŠŸèƒ½
    
    åŠŸèƒ½ï¼š
    1. å°å¤šå€‹æŸ¥è©¢é€²è¡Œä¸¦è¡Œæª¢ç´¢
    2. å»é‡å’Œæ’åºæª¢ç´¢çµæœ
    3. æä¾›è©³ç´°çš„æª¢ç´¢çµ±è¨ˆä¿¡æ¯
    
    åƒæ•¸ï¼š
        vectorstore: å‘é‡æ•¸æ“šåº«å°è±¡
        query_list (List[str]): æŸ¥è©¢åˆ—è¡¨
        k (int): è¿”å›çš„æ–‡æª”æ•¸é‡
        fetch_k (int): åˆå§‹æª¢ç´¢çš„æ–‡æª”æ•¸é‡
        score_threshold (float): ç›¸ä¼¼åº¦é–¾å€¼
    
    è¿”å›ï¼š
        List[Document]: æª¢ç´¢åˆ°çš„æ–‡æª”åˆ—è¡¨
    
    æŠ€è¡“ç´°ç¯€ï¼š
    - ä½¿ç”¨MMRï¼ˆæœ€å¤§é‚Šéš›ç›¸é—œæ€§ï¼‰æœç´¢
    - è‡ªå‹•å»é‡é¿å…é‡è¤‡å…§å®¹
    - æä¾›è©³ç´°çš„æª¢ç´¢æ—¥èªŒ
    """
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": k, "fetch_k": fetch_k, "score_threshold": score_threshold}
    )

    # ä½¿ç”¨å­—å…¸é€²è¡Œå»é‡
    chunk_dict = {}
    print("ğŸ” æŸ¥è©¢åˆ—è¡¨ï¼š", query_list)
    
    # å°æ¯å€‹æŸ¥è©¢é€²è¡Œæª¢ç´¢
    for q in query_list:
        docs = retriever.get_relevant_documents(q)
        for doc in docs:
            # ä½¿ç”¨å”¯ä¸€æ¨™è­˜ç¬¦é€²è¡Œå»é‡
            key = doc.metadata.get("exp_id") or doc.metadata.get("source") or doc.page_content[:30]
            chunk_dict[key] = doc
    
    # é™åˆ¶è¿”å›çµæœæ•¸é‡ï¼Œä½¿ç”¨å‚³å…¥çš„ k åƒæ•¸
    result = list(chunk_dict.values())[:k]

    # æª¢ç´¢çµæœé©—è­‰
    if not result:
        print("âš ï¸ æ²’æœ‰æª¢ç´¢åˆ°ä»»ä½•æ–‡æª”ï¼Œå»ºè­°æª¢æŸ¥æª¢ç´¢å™¨æˆ–åµŒå…¥æ ¼å¼")
    else:
        print(f"ğŸ” å¤šæŸ¥è©¢æª¢ç´¢å…±æ‰¾åˆ° {len(result)} å€‹æ–‡æª”ï¼š")
        print("ğŸ” æª¢ç´¢åˆ°çš„æ–‡æª”é è¦½ï¼š")
        for i, doc in enumerate(result[:5], 1):
            meta = doc.metadata
            filename = meta.get("filename") or meta.get("source", "Unknown")
            page = meta.get("page_number") or meta.get("page", "?")
            preview = doc.page_content[:80].replace("\n", " ")
            print(f"--- æ–‡æª” {i} ---")
            print(f"ğŸ“„ {filename} | é ç¢¼ {page}")
            print(f"ğŸ“ å…§å®¹é è¦½ï¼š{preview}")
            print()

    return result


def preview_chunks(chunks: List[Document], title: str, max_preview: int = 5):
    """
    é è¦½æ–‡æª”å¡Šå…§å®¹
    
    åŠŸèƒ½ï¼š
    1. é¡¯ç¤ºæ–‡æª”å¡Šçš„åŸºæœ¬ä¿¡æ¯
    2. æä¾›å…§å®¹é è¦½
    3. å¹«åŠ©èª¿è©¦å’Œé©—è­‰æª¢ç´¢çµæœ
    
    åƒæ•¸ï¼š
        chunks (List[Document]): æ–‡æª”å¡Šåˆ—è¡¨
        title (str): é è¦½æ¨™é¡Œ
        max_preview (int): æœ€å¤§é è¦½æ•¸é‡
    """
    print(f"\nğŸ“¦ã€{title}ã€‘å…±æ‰¾åˆ° {len(chunks)} å€‹æ–‡æª”å¡Š")

    if not chunks:
        print("âš ï¸ æ²’æœ‰ä»»ä½•æ®µè½å¯é¡¯ç¤ºã€‚")
        return

    # é¡¯ç¤ºå‰å¹¾å€‹æ–‡æª”å¡Šçš„è©³ç´°ä¿¡æ¯
    for i, doc in enumerate(chunks[:max_preview], 1):
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        preview = doc.page_content[:100].replace("\n", " ")
        print(f"--- Chunk {i} ---")
        print(f"ğŸ“„ Filename: {filename} | Page: {page}")
        print(f"ğŸ“š Preview: {preview}")


# ==================== æç¤ºè©æ§‹å»ºåŠŸèƒ½ ====================

def build_prompt(chunks: List[Document], question: str) -> Tuple[str, List[Dict]]: #åš´è¬¹å›ç­”æ¨¡å¼ï¼Œä¸å…è¨±ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è­˜
    # æª¢æŸ¥ï¼šchunks å¿…é ˆæ˜¯ List[Document]ï¼Œquestion æ‡‰ç‚º str
    context_text = ""
    citations = []
    citation_map = {}
    
    for i, doc in enumerate(chunks):
        # æª¢æŸ¥ï¼šdoc æ‡‰æœ‰ metadata å±¬æ€§ï¼Œä¸”ç‚º dict
        metadata = doc.metadata
        title = metadata.get("title", "Untitled")
        # æª¢æŸ¥ï¼šfilename ä¾†æºæ–¼ "filename" æˆ– "source"ï¼Œè‹¥éƒ½ç„¡å‰‡ç‚º "Unknown"
        filename = metadata.get("filename") or metadata.get("source", "Unknown")
        # æª¢æŸ¥ï¼špage ä¾†æºæ–¼ "page_number" æˆ– "page"ï¼Œè‹¥éƒ½ç„¡å‰‡ç‚º "?"
        page = metadata.get("page_number") or metadata.get("page", "?")
        # é è¦½ç‰‡æ®µï¼Œå–å‰ 80 å­—å…ƒï¼Œä¸¦å°‡æ›è¡Œæ›¿æ›ç‚ºç©ºæ ¼
        snippet = doc.page_content[:80].replace("\n", " ")

        # æª¢æŸ¥ï¼šé¿å…é‡è¤‡çš„ (filename, page) çµ„åˆ
        citation_key = f"{filename}_p{page}"
        if citation_key not in citation_map:
            label = f"[{len(citations) + 1}]"
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
            citation_map[citation_key] = label
        else:
            label = citation_map[citation_key]

        # context_text ç´¯åŠ æ¯å€‹ chunk çš„å…§å®¹ï¼Œæ ¼å¼ç‚º [n] title | Page n
        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    # system_prompt is the final prompt containing context_text and question
    system_prompt = f"""

    You are a research literature search assistant. Please answer questions based only on the provided literature excerpts.
    Please use [1], [2], etc. to cite paragraph sources in your answers, and do not repeat the sources at the end.
    If the paragraphs mention specific experimental conditions (temperature, time, etc.), please be sure to include them in your answer.
    Important: You can only cite the provided literature excerpts. The current literature excerpt numbers are [1] to [{len(chunks)}] (total {len(chunks)} excerpts)

    --- Literature Summary ---
    {context_text}


    --- Question ---
    {question}
    """
    # Check: return system_prompt with trimmed whitespace and citations list
    return system_prompt.strip(), citations


def call_llm(prompt: str) -> str:
    print(f"ğŸ” èª¿ç”¨ LLMï¼Œæç¤ºè©é•·åº¦ï¼š{len(prompt)} å­—ç¬¦")
    print(f"ğŸ” DEBUG: prompt é¡å‹: {type(prompt)}")
    print(f"ğŸ” DEBUG: prompt å‰100å­—ç¬¦: {prompt[:100]}...")
    
    # ç²å–ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯å’Œåƒæ•¸
    try:
        from model_config_bridge import get_current_model, get_model_params
        current_model = get_current_model()
        llm_params = get_model_params()
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        print(f"ğŸ”§ æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
        print(f"ğŸ” DEBUG: current_model é¡å‹: {type(current_model)}")
        print(f"ğŸ” DEBUG: current_model.startswith('gpt-5'): {current_model.startswith('gpt-5')}")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        # ä½¿ç”¨fallbacké…ç½®
        llm_params = {
            "model": "gpt-4-1106-preview",
            "temperature": 0.3,
            "max_tokens": 4000,
            "timeout": 120,
        }
    
    try:
        # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ä¸åŒçš„API
        if current_model.startswith('gpt-5'):
            # GPT-5ç³»åˆ—ä½¿ç”¨Responses API
            from openai import OpenAI
            client = OpenAI()
            
            # æº–å‚™Responses APIçš„åƒæ•¸
            # ä½¿ç”¨è¨­å®šçš„max_output_tokensï¼Œä¸è‡ªå‹•æé«˜
            max_tokens = llm_params.get('max_output_tokens', 2000)
            print(f"ğŸ”§ ä½¿ç”¨è¨­å®šçš„max_output_tokens: {max_tokens}")
            
            responses_params = {
                'model': current_model,
                'input': [{'role': 'user', 'content': prompt}],
                'max_output_tokens': max_tokens
            }
            
            # æ·»åŠ å…¶ä»–åƒæ•¸ï¼ˆæ’é™¤modelã€inputå’Œmax_output_tokensï¼‰
            for key, value in llm_params.items():
                if key not in ['model', 'input', 'max_output_tokens']:
                    responses_params[key] = value
            
            print(f"ğŸ”§ ä½¿ç”¨Responses APIï¼Œåƒæ•¸ï¼š{responses_params}")
            print(f"ğŸ” DEBUG: æº–å‚™èª¿ç”¨ client.responses.create")
            

            
            # è™•ç†GPT-5çš„incompleteç‹€æ…‹
            max_retries = 3
            retry_count = 0
            current_max_tokens = max_tokens
            
            while retry_count < max_retries:
                # æ›´æ–°tokenæ•¸ï¼ˆæ¯æ¬¡é‡è©¦å¢åŠ 1500ï¼‰
                if retry_count > 0:
                    current_max_tokens += 1500
                    responses_params['max_output_tokens'] = current_max_tokens
                    print(f"ğŸ”„ é‡è©¦ {retry_count}ï¼šæé«˜max_output_tokensåˆ° {current_max_tokens}")
                
                response = client.responses.create(**responses_params)
                
                print(f"ğŸ” DEBUG: APIèª¿ç”¨å®Œæˆ (å˜—è©¦ {retry_count + 1}/{max_retries})")
                print(f"ğŸ” DEBUG: response é¡å‹: {type(response)}")
                print(f"ğŸ” DEBUG: response.status: {getattr(response, 'status', 'N/A')}")
                
                # æª¢æŸ¥æ•´é«”responseç‹€æ…‹
                if hasattr(response, 'status') and response.status == 'incomplete':
                    print(f"âš ï¸ æª¢æ¸¬åˆ°incompleteç‹€æ…‹ï¼Œç­‰å¾…å¾Œé‡è©¦...")
                    print(f"ğŸ’¡ ç•¶å‰max_output_tokens: {current_max_tokens}")
                    retry_count += 1
                    if retry_count < max_retries:
                        import time
                        time.sleep(2)  # ç­‰å¾…2ç§’å¾Œé‡è©¦
                        continue
                    else:
                        print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œä½¿ç”¨incompleteçš„çµæœ")
                
                # æå–æ–‡æœ¬å…§å®¹ï¼ˆå„ªå…ˆä½¿ç”¨output_textï¼Œå¾Œå‚™è§£æoutputé™£åˆ—ï¼‰
                output = ""
                
                # 1) å„ªå…ˆå˜—è©¦å®˜æ–¹ä¾¿æ·å±¬æ€§ output_text
                try:
                    if getattr(response, "output_text", None):
                        txt = response.output_text.strip()
                        if txt:
                            print(f"âœ… ä½¿ç”¨ output_text: {len(txt)} å­—ç¬¦")
                            output = txt
                except Exception as e:
                    print(f"âš ï¸ output_text æå–å¤±æ•—: {e}")
                
                # 2) å¦‚æœoutput_textç‚ºç©ºï¼Œå¾Œå‚™è§£æoutputé™£åˆ—
                if not output:
                    if hasattr(response, 'output') and response.output:
                        print(f"ğŸ” DEBUG: é–‹å§‹è§£æ output é™£åˆ—ï¼Œå…± {len(response.output)} å€‹é …ç›®")
                        
                        for i, item in enumerate(response.output):
                            item_type = getattr(item, "type", None)
                            item_status = getattr(item, "status", None)
                            print(f"  - [{i}] type={item_type}, status={item_status}")
                            
                            # æœ€çµ‚ç­”æ¡ˆé€šå¸¸åœ¨ type="message"
                            if item_type == "message":
                                content = getattr(item, "content", []) or []
                                print(f"    ğŸ“ message æœ‰ {len(content)} å€‹ content é …ç›®")
                                
                                for j, c in enumerate(content):
                                    # content ç‰©ä»¶é€šå¸¸æœ‰ .text
                                    textval = getattr(c, "text", None)
                                    if textval:
                                        print(f"    âœ… content[{j}] æå–åˆ°æ–‡æœ¬: {len(textval)} å­—ç¬¦")
                                        output += textval
                                    else:
                                        print(f"    âš ï¸ content[{j}] æ²’æœ‰ text å±¬æ€§")
                    else:
                        print(f"ğŸ” DEBUG: response æ²’æœ‰ output å±¬æ€§æˆ– output ç‚ºç©º")

                output = output.strip()
                print(f"ğŸ” DEBUG: æœ€çµ‚ output é•·åº¦: {len(output)}")
                print(f"ğŸ” DEBUG: æœ€çµ‚ output å…§å®¹: {output[:200]}...")

                # æª¢æŸ¥æ•´é«”responseç‹€æ…‹
                response_status = getattr(response, 'status', None)
                if response_status == 'incomplete':
                    print(f"âš ï¸ æ•´é«”éŸ¿æ‡‰ç‹€æ…‹ç‚º incomplete")
                    if output:
                        print(f"âœ… å³ä½¿incompleteç‹€æ…‹ï¼Œä»æˆåŠŸæå–æ–‡æœ¬: {len(output)} å­—ç¬¦")
                        print(f"âœ… LLM èª¿ç”¨æˆåŠŸï¼Œå›æ‡‰é•·åº¦ï¼š{len(output)} å­—ç¬¦")
                        print(f"ğŸ“ LLM å›æ‡‰é è¦½ï¼š{output[:200]}...")
                        return output
                    else:
                        print(f"âŒ incompleteç‹€æ…‹ä¸”ç„¡æ³•æå–æ–‡æœ¬")
                        retry_count += 1
                        if retry_count < max_retries:
                            import time
                            time.sleep(2)  # ç­‰å¾…2ç§’å¾Œé‡è©¦
                            continue
                        else:
                            print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                            print(f"ğŸ’¡ å·²å˜—è©¦æé«˜tokenæ•¸åˆ° {current_max_tokens}")
                            return ""
                else:
                    # æ­£å¸¸ç‹€æ…‹
                    if output:
                        print(f"âœ… æˆåŠŸæå–æ–‡æœ¬: {len(output)} å­—ç¬¦")
                        print(f"âœ… LLM èª¿ç”¨æˆåŠŸï¼Œå›æ‡‰é•·åº¦ï¼š{len(output)} å­—ç¬¦")
                        print(f"ğŸ“ LLM å›æ‡‰é è¦½ï¼š{output[:200]}...")
                        return output
                    else:
                        print(f"âŒ ç„¡æ³•æå–æ–‡æœ¬")
                        retry_count += 1
                        if retry_count < max_retries:
                            import time
                            time.sleep(2)
                            continue
                        else:
                            print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                            print(f"ğŸ’¡ å·²å˜—è©¦æé«˜tokenæ•¸åˆ° {current_max_tokens}")
                            return ""

            print(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
            return ""
            
        else:
            # GPT-4ç³»åˆ—ä½¿ç”¨Chat Completions API (LangChain)
            llm = ChatOpenAI(**llm_params)
            response = llm.invoke(prompt)
            print(f"âœ… LLM èª¿ç”¨æˆåŠŸï¼Œå›æ‡‰é•·åº¦ï¼š{len(response.content)} å­—ç¬¦")
            print(f"ğŸ“ LLM å›æ‡‰é è¦½ï¼š{response.content[:200]}...")
            return response.content
            
    except Exception as e:
        print(f"âŒ LLM èª¿ç”¨å¤±æ•—ï¼š{e}")
        return ""


def call_llm_structured_proposal(system_prompt: str, user_prompt: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–ç ”ç©¶ææ¡ˆ
    
    Args:
        system_prompt: ç³»çµ±æç¤ºè©
        user_prompt: ç”¨æˆ¶æç¤ºè©ï¼ˆåŒ…å«æ–‡ç»æ‘˜è¦å’Œç ”ç©¶ç›®æ¨™ï¼‰
    
    Returns:
        Dict[str, Any]: ç¬¦åˆRESEARCH_PROPOSAL_SCHEMAçš„çµæ§‹åŒ–ææ¡ˆ
    """
    print(f"ğŸ” èª¿ç”¨çµæ§‹åŒ–LLMï¼Œç³»çµ±æç¤ºè©é•·åº¦ï¼š{len(system_prompt)} å­—ç¬¦")
    print(f"ğŸ” ç”¨æˆ¶æç¤ºè©é•·åº¦ï¼š{len(user_prompt)} å­—ç¬¦")
    
    # ç²å–ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯å’Œåƒæ•¸
    try:
        from model_config_bridge import get_current_model, get_model_params
        current_model = get_current_model()
        llm_params = get_model_params()
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        print(f"ğŸ”§ æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        # ä½¿ç”¨fallbacké…ç½®
        current_model = "gpt-4-1106-preview"
        llm_params = {
            "model": "gpt-4-1106-preview",
            "temperature": 0.0,  # çµæ§‹åŒ–è¼¸å‡ºä½¿ç”¨0æº«åº¦
            "max_tokens": 4000,
            "timeout": 120,
        }
    
    try:
        # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ä¸åŒçš„API
        if current_model.startswith('gpt-5'):
            # GPT-5ç³»åˆ—ä½¿ç”¨Responses API with JSON Schema
            from openai import OpenAI
            client = OpenAI()
            
            # æº–å‚™Responses APIçš„åƒæ•¸
            max_tokens = llm_params.get('max_output_tokens', 4000)
            
            # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
            current_schema = create_research_proposal_schema()
            
            # ä½¿ç”¨ Responses API + JSON Schema (é©ç”¨æ–¼æ‰€æœ‰ GPT-5 ç³»åˆ—æ¨¡å‹)
            responses_params = {
                'model': current_model,
                'input': [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                'text': {
                    'format': {
                        'type': 'json_schema',
                        'name': 'ResearchProposal',
                        'strict': True,
                        'schema': current_schema,
                    },
                    'verbosity': 'low'  # ä½¿ç”¨ low verbosity
                },
                'reasoning': {'effort': 'medium'},  # ä½¿ç”¨ medium reasoning
                'max_output_tokens': max_tokens
            }
            
            print(f"ğŸ”§ ä½¿ç”¨Responses API with JSON Schemaï¼Œåƒæ•¸ï¼š{responses_params}")
            
            # è™•ç†GPT-5çš„incompleteç‹€æ…‹
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                response = client.responses.create(**responses_params)
                
                print(f"ğŸ” DEBUG: APIèª¿ç”¨å®Œæˆ (å˜—è©¦ {retry_count + 1}/{max_retries})")
                print(f"ğŸ” DEBUG: response.status: {getattr(response, 'status', 'N/A')}")
                
                # æª¢æŸ¥æ•´é«”responseç‹€æ…‹
                if hasattr(response, 'status') and response.status == 'incomplete':
                    print(f"âš ï¸ æª¢æ¸¬åˆ°incompleteç‹€æ…‹ï¼Œç­‰å¾…å¾Œé‡è©¦...")
                    retry_count += 1
                    if retry_count < max_retries:
                        import time
                        time.sleep(2)  # ç­‰å¾…2ç§’å¾Œé‡è©¦
                        continue
                    else:
                        print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                        return {}
                
                # æå–JSONå…§å®¹
                try:
                    # å„ªå…ˆä½¿ç”¨ resp.output_text
                    output_text = getattr(response, 'output_text', None)
                    if output_text:
                        print(f"âœ… ä½¿ç”¨ resp.output_text æå–å…§å®¹: {len(output_text)} å­—ç¬¦")
                        try:
                            proposal_data = json.loads(output_text)
                            print(f"âœ… æˆåŠŸè§£æJSONçµæ§‹åŒ–ææ¡ˆ")
                            
                            # æœ¬åœ° JSON Schema é©—è­‰
                            try:
                                from jsonschema import Draft202012Validator
                                from jsonschema.exceptions import ValidationError
                                Draft202012Validator(current_schema).validate(proposal_data)
                                print("âœ… æœ¬åœ° Schema é©—è­‰é€šé")
                            except ImportError:
                                print("âš ï¸ jsonschema æœªå®‰è£ï¼Œè·³éæœ¬åœ°é©—è­‰")
                            except ValidationError as e:
                                print(f"âš ï¸ æœ¬åœ° Schema é©—è­‰å¤±æ•—: {e}")
                                # ç¹¼çºŒè¿”å›çµæœï¼Œå› ç‚º API ç«¯å·²ç¶“é©—è­‰é
                            
                            return proposal_data
                        except json.JSONDecodeError as e:
                            print(f"âš ï¸ JSONè§£æå¤±æ•—: {e}")
                            print(f"âš ï¸ å˜—è©¦çš„æ–‡æœ¬: {output_text[:200]}...")
                            return {}
                    
                    # å›é€€åˆ° resp.output èšåˆæ–¹å¼
                    if hasattr(response, 'output') and response.output:
                        parts = []
                        for item in response.output:
                            if hasattr(item, "content"):
                                for content in item.content:
                                    if hasattr(content, "text"):
                                        parts.append(content.text)
                        
                        if parts:
                            text_content = "".join(parts).strip()
                            print(f"âœ… ä½¿ç”¨ resp.output èšåˆæå–å…§å®¹: {len(text_content)} å­—ç¬¦")
                            
                            try:
                                proposal_data = json.loads(text_content)
                                print(f"âœ… æˆåŠŸè§£æJSONçµæ§‹åŒ–ææ¡ˆ")
                                
                                # æœ¬åœ° JSON Schema é©—è­‰
                                try:
                                    from jsonschema import Draft202012Validator
                                    from jsonschema.exceptions import ValidationError
                                    Draft202012Validator(current_schema).validate(proposal_data)
                                    print("âœ… æœ¬åœ° Schema é©—è­‰é€šé")
                                except ImportError:
                                    print("âš ï¸ jsonschema æœªå®‰è£ï¼Œè·³éæœ¬åœ°é©—è­‰")
                                except ValidationError as e:
                                    print(f"âš ï¸ æœ¬åœ° Schema é©—è­‰å¤±æ•—: {e}")
                                    # ç¹¼çºŒè¿”å›çµæœï¼Œå› ç‚º API ç«¯å·²ç¶“é©—è­‰é
                                
                                return proposal_data
                            except json.JSONDecodeError as e:
                                print(f"âš ï¸ JSONè§£æå¤±æ•—: {e}")
                                print(f"âš ï¸ å˜—è©¦çš„æ–‡æœ¬: {text_content[:200]}...")
                                return {}
                    
                    # å¦‚æœæ²’æœ‰æ‰¾åˆ°JSONå…§å®¹
                    print(f"âš ï¸ ç„¡æ³•å¾Responses APIæå–JSONå…§å®¹")
                    return {}
                    
                except json.JSONDecodeError as e:
                    print(f"âŒ JSONè§£æå¤±æ•—: {e}")
                    retry_count += 1
                    if retry_count < max_retries:
                        import time
                        time.sleep(2)
                        continue
                    else:
                        print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                        return {}
                except Exception as e:
                    print(f"âŒ æå–JSONå…§å®¹å¤±æ•—: {e}")
                    retry_count += 1
                    if retry_count < max_retries:
                        import time
                        time.sleep(2)
                        continue
                    else:
                        print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                        return {}
            
            print(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œè¿”å›ç©ºå­—å…¸")
            return {}
            
        else:
            # GPT-4ç³»åˆ—ä½¿ç”¨Chat Completions API with function calling
            from openai import OpenAI
            client = OpenAI()
            
            # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
            current_schema = create_research_proposal_schema()
            
            # ä½¿ç”¨function callingä½œç‚ºfallback
            response = client.chat.completions.create(
                model=current_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.0,
                functions=[{
                    "name": "create_research_proposal",
                    "description": "Create a structured research proposal",
                    "parameters": current_schema
                }],
                function_call={"name": "create_research_proposal"},
                max_tokens=llm_params.get('max_tokens', 4000)
            )
            
            # æå–function callçµæœ
            if response.choices[0].message.function_call:
                function_call = response.choices[0].message.function_call
                arguments = json.loads(function_call.arguments)
                print(f"âœ… æˆåŠŸè§£æfunction callçµæ§‹åŒ–ææ¡ˆ")
                return arguments
            else:
                print(f"âŒ ç„¡æ³•å¾function callæå–çµæœ")
                return {}
            
    except Exception as e:
        print(f"âŒ çµæ§‹åŒ–LLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        return {}

def call_llm_structured_experimental_detail(chunks: List[Document], proposal: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–å¯¦é©—ç´°ç¯€
    
    Args:
        chunks: æ–‡ç»ç‰‡æ®µ
        proposal: ç ”ç©¶ææ¡ˆ
    
    Returns:
        Dict[str, Any]: ç¬¦åˆEXPERIMENTAL_DETAIL_SCHEMAçš„çµæ§‹åŒ–å¯¦é©—ç´°ç¯€
    """
    print(f"ğŸ” èª¿ç”¨çµæ§‹åŒ–å¯¦é©—ç´°ç¯€LLMï¼Œæ–‡ç»ç‰‡æ®µæ•¸é‡ï¼š{len(chunks)}")
    print(f"ğŸ” ææ¡ˆé•·åº¦ï¼š{len(proposal)} å­—ç¬¦")
    
    # ç²å–ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯å’Œåƒæ•¸
    try:
        from model_config_bridge import get_current_model, get_model_params
        current_model = get_current_model()
        llm_params = get_model_params()
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        print(f"ğŸ”§ æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        # ä½¿ç”¨fallbacké…ç½®
        current_model = "gpt-4-1106-preview"
        llm_params = {
            "model": "gpt-4-1106-preview",
            "temperature": 0.0,  # çµæ§‹åŒ–è¼¸å‡ºä½¿ç”¨0æº«åº¦
            "max_tokens": 4000,
            "timeout": 120,
        }
    
    try:
        # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ä¸åŒçš„API
        if current_model.startswith('gpt-5'):
            # GPT-5ç³»åˆ—ä½¿ç”¨Responses API with JSON Schema
            from openai import OpenAI
            client = OpenAI()
            
            # æº–å‚™Responses APIçš„åƒæ•¸
            max_tokens = llm_params.get('max_output_tokens', 6000)  # ä½¿ç”¨æ¸¬è©¦å ±å‘Šæ¨è–¦çš„ 6000
            
            # æ§‹å»ºç³»çµ±æç¤ºè©
            system_prompt = f"""
            You are a professional materials synthesis consultant, skilled at generating detailed experimental procedures based on literature and research proposals.

            Based on the following research proposal and literature information, please generate detailed experimental details:

            --- Research Proposal ---
            {proposal}

            Please generate detailed experimental details including the following four sections:
            1. Synthesis Process: Detailed synthesis steps, conditions, durations, etc.
            2. Materials and Conditions: Materials used, concentrations, temperatures, pressures, and other reaction conditions
            3. Analytical Methods: Characterization techniques such as XRD, SEM, NMR, etc.
            4. Precautions: Experimental notes and safety precautions

            """
            
            # æ§‹å»ºç”¨æˆ¶æç¤ºè©ï¼ˆåŒ…å«æ–‡ç»æ‘˜è¦ï¼‰
            context_text = ""
            citations = []
            for i, doc in enumerate(chunks):
                meta = doc.metadata
                title = meta.get("title", "Untitled")
                filename = meta.get("filename") or meta.get("source", "Unknown")
                page = meta.get("page_number") or meta.get("page", "?")
                
                context_text += f"[{i+1}] {title} | Page {page}\n{doc.page_content}\n\n"
                citations.append({
                    "label": f"[{i+1}]",
                    "title": title,
                    "source": filename,
                    "page": page
                })
            
            user_prompt = f"""
            åŸºæ–¼ä»¥ä¸‹æ–‡ç»ä¿¡æ¯ç”Ÿæˆå¯¦é©—ç´°ç¯€ï¼š
            
            --- æ–‡ç»æ‘˜è¦ ---
            {context_text}
            
            è«‹ç”Ÿæˆè©³ç´°çš„å¯¦é©—ç´°ç¯€ï¼Œç¢ºä¿æ‰€æœ‰åŒ–å­¸å“åç¨±éƒ½ä½¿ç”¨IUPACå‘½åæ³•ã€‚
            """
            
            # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
            current_schema = create_experimental_detail_schema()
            
            # ä½¿ç”¨æ¸¬è©¦å ±å‘Šæ¨è–¦çš„æœ€ä½³å¯¦è¸é…ç½®
            responses_params = {
                'model': current_model,
                'input': [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                'text': {
                    'format': {
                        'type': 'json_schema',
                        'name': 'ExperimentalDetail',
                        'strict': True,
                        'schema': current_schema,
                    },
                    'verbosity': 'low'  # ä½¿ç”¨ low verbosity é¿å…éé•·è¼¸å‡º
                },
                'reasoning': {'effort': 'minimal'},  # ä½¿ç”¨ minimal effort æé«˜é€Ÿåº¦
                'max_output_tokens': max_tokens
            }
            
            print(f"ğŸ”§ ä½¿ç”¨Responses APIï¼Œåƒæ•¸ï¼š{responses_params}")
            
            # è™•ç†GPT-5çš„incompleteç‹€æ…‹ - ä½¿ç”¨æ¸¬è©¦å ±å‘Šæ¨è–¦çš„é…ç½®
            max_retries = 2  # æ¸›å°‘é‡è©¦æ¬¡æ•¸
            retry_count = 0
            current_max_tokens = max_tokens
            
            while retry_count < max_retries:
                # æ›´æ–°tokenæ•¸ï¼ˆæ¯æ¬¡é‡è©¦å¢åŠ 1000ï¼Œè€Œä¸æ˜¯1500ï¼‰
                if retry_count > 0:
                    current_max_tokens += 1000
                    responses_params['max_output_tokens'] = current_max_tokens
                    print(f"ğŸ”„ é‡è©¦ {retry_count}ï¼šæé«˜max_output_tokensåˆ° {current_max_tokens}")
                
                response = client.responses.create(**responses_params)
                
                print(f"ğŸ” DEBUG: APIèª¿ç”¨å®Œæˆ (å˜—è©¦ {retry_count + 1}/{max_retries})")
                print(f"ğŸ” DEBUG: response é¡å‹: {type(response)}")
                print(f"ğŸ” DEBUG: response.status: {getattr(response, 'status', 'N/A')}")
                
                # æª¢æŸ¥æ•´é«”responseç‹€æ…‹
                if hasattr(response, 'status') and response.status == 'incomplete':
                    print(f"âš ï¸ æª¢æ¸¬åˆ°incompleteç‹€æ…‹ï¼Œç­‰å¾…å¾Œé‡è©¦...")
                    print(f"ğŸ’¡ ç•¶å‰max_output_tokens: {current_max_tokens}")
                    retry_count += 1
                    if retry_count < max_retries:
                        import time
                        time.sleep(1)  # æ¸›å°‘ç­‰å¾…æ™‚é–“åˆ°1ç§’
                        continue
                    else:
                        print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œä½¿ç”¨incompleteçš„çµæœ")
                
                # æå–JSONå…§å®¹
                try:
                    # å„ªå…ˆä½¿ç”¨ resp.output_text
                    output_text = getattr(response, 'output_text', None)
                    if output_text:
                        print(f"âœ… ä½¿ç”¨ resp.output_text æå–å…§å®¹: {len(output_text)} å­—ç¬¦")
                        try:
                            experimental_data = json.loads(output_text)
                            print(f"âœ… æˆåŠŸè§£æJSONçµæ§‹åŒ–å¯¦é©—ç´°ç¯€")
                            
                            # æ·»åŠ å¼•ç”¨ä¿¡æ¯
                            experimental_data['citations'] = citations
                            
                            return experimental_data
                        except json.JSONDecodeError as e:
                            print(f"âš ï¸ JSONè§£æå¤±æ•—: {e}")
                            print(f"âš ï¸ å˜—è©¦çš„æ–‡æœ¬: {output_text[:200]}...")
                            
                            # å˜—è©¦ä¿®å¾©å¸¸è¦‹çš„JSONæ ¼å¼å•é¡Œ
                            try:
                                # å˜—è©¦ä¿®å¾©æœªçµ‚æ­¢çš„å­—ç¬¦ä¸²
                                if "Unterminated string" in str(e):
                                    # æ‰¾åˆ°æœ€å¾Œä¸€å€‹å®Œæ•´çš„å¼•è™Ÿä½ç½®
                                    last_quote = output_text.rfind('"')
                                    if last_quote > 0:
                                        # æˆªæ–·åˆ°æœ€å¾Œä¸€å€‹å®Œæ•´å¼•è™Ÿä¸¦æ·»åŠ çµæŸå¼•è™Ÿ
                                        fixed_text = output_text[:last_quote+1] + '}'
                                        experimental_data = json.loads(fixed_text)
                                        print(f"âœ… ä¿®å¾©JSONæ ¼å¼å¾ŒæˆåŠŸè§£æ")
                                        
                                        # æ·»åŠ å¼•ç”¨ä¿¡æ¯
                                        experimental_data['citations'] = citations
                                        
                                        return experimental_data
                            except:
                                pass
                            
                            return {}
                    
                    # å›é€€åˆ° resp.output èšåˆæ–¹å¼
                    if hasattr(response, 'output') and response.output:
                        parts = []
                        for item in response.output:
                            if hasattr(item, "content"):
                                for content in item.content:
                                    if hasattr(content, "text"):
                                        parts.append(content.text)
                        
                        if parts:
                            combined_text = "".join(parts)
                            try:
                                experimental_data = json.loads(combined_text)
                                print(f"âœ… æˆåŠŸè§£æJSONçµæ§‹åŒ–å¯¦é©—ç´°ç¯€ï¼ˆèšåˆæ–¹å¼ï¼‰")
                                
                                # æ·»åŠ å¼•ç”¨ä¿¡æ¯
                                experimental_data['citations'] = citations
                                
                                return experimental_data
                            except json.JSONDecodeError as e:
                                print(f"âš ï¸ JSONè§£æå¤±æ•—ï¼ˆèšåˆæ–¹å¼ï¼‰: {e}")
                                return {}
                
                except Exception as e:
                    print(f"âš ï¸ å…§å®¹æå–å¤±æ•—: {e}")
                
                # å¦‚æœç„¡æ³•æå–å…§å®¹ï¼Œé‡è©¦
                retry_count += 1
                if retry_count < max_retries:
                    import time
                    time.sleep(1)  # æ¸›å°‘ç­‰å¾…æ™‚é–“
                    continue
                else:
                    print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                    print(f"ğŸ’¡ å·²å˜—è©¦æé«˜tokenæ•¸åˆ° {current_max_tokens}")
                    return {}
            
            print(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œè¿”å›ç©ºå­—å…¸")
            return {}
            
        else:
            # GPT-4ç³»åˆ—ä½¿ç”¨Chat Completions API with function calling
            from openai import OpenAI
            client = OpenAI()
            
            # æ§‹å»ºæç¤ºè©
            context_text = ""
            citations = []
            for i, doc in enumerate(chunks):
                meta = doc.metadata
                title = meta.get("title", "Untitled")
                filename = meta.get("filename") or meta.get("source", "Unknown")
                page = meta.get("page_number") or meta.get("page", "?")
                
                context_text += f"[{i+1}] {title} | Page {page}\n{doc.page_content}\n\n"
                citations.append({
                    "label": f"[{i+1}]",
                    "title": title,
                    "source": filename,
                    "page": page
                })
            
            system_prompt = f"""
            ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ææ–™åˆæˆé¡§å•ï¼Œæ“…é•·åŸºæ–¼æ–‡ç»å’Œææ¡ˆç”Ÿæˆè©³ç´°çš„å¯¦é©—ç´°ç¯€ã€‚
            
            è«‹åŸºæ–¼ä»¥ä¸‹ç ”ç©¶ææ¡ˆå’Œæ–‡ç»ä¿¡æ¯ï¼Œç”Ÿæˆè©³ç´°çš„å¯¦é©—ç´°ç¯€ï¼š
            
            --- ç ”ç©¶ææ¡ˆ ---
            {proposal}
            
            è«‹ç”ŸæˆåŒ…å«ä»¥ä¸‹å››å€‹éƒ¨åˆ†çš„è©³ç´°å¯¦é©—ç´°ç¯€ï¼š
            1. åˆæˆéç¨‹ï¼šè©³ç´°çš„åˆæˆæ­¥é©Ÿã€æ¢ä»¶ã€æ™‚é–“ç­‰
            2. ææ–™å’Œæ¢ä»¶ï¼šä½¿ç”¨çš„ææ–™ã€æ¿ƒåº¦ã€æº«åº¦ã€å£“åŠ›ç­‰åæ‡‰æ¢ä»¶
            3. åˆ†ææ–¹æ³•ï¼šXRDã€SEMã€NMRç­‰è¡¨å¾µæŠ€è¡“
            4. æ³¨æ„äº‹é …ï¼šå¯¦é©—æ³¨æ„äº‹é …å’Œå®‰å…¨é é˜²æªæ–½
            
            è«‹ç¢ºä¿æ‰€æœ‰åŒ–å­¸å“åç¨±éƒ½ä½¿ç”¨IUPACå‘½åæ³•ã€‚
            """
            
            user_prompt = f"""
            åŸºæ–¼ä»¥ä¸‹æ–‡ç»ä¿¡æ¯ç”Ÿæˆå¯¦é©—ç´°ç¯€ï¼š
            
            --- æ–‡ç»æ‘˜è¦ ---
            {context_text}
            
            è«‹ç”Ÿæˆè©³ç´°çš„å¯¦é©—ç´°ç¯€ï¼Œç¢ºä¿æ‰€æœ‰åŒ–å­¸å“åç¨±éƒ½ä½¿ç”¨IUPACå‘½åæ³•ã€‚
            """
            
            # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
            current_schema = create_experimental_detail_schema()
            
            # ä½¿ç”¨function callingä½œç‚ºfallback
            response = client.chat.completions.create(
                model=current_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.0,
                functions=[{
                    "name": "create_experimental_detail",
                    "description": "Create a structured experimental detail",
                    "parameters": current_schema
                }],
                function_call={"name": "create_experimental_detail"},
                max_tokens=llm_params.get('max_tokens', 4000)
            )
            
            # æå–function callçµæœ
            if response.choices[0].message.function_call:
                function_call = response.choices[0].message.function_call
                arguments = json.loads(function_call.arguments)
                print(f"âœ… æˆåŠŸè§£æfunction callçµæ§‹åŒ–å¯¦é©—ç´°ç¯€")
                
                # æ·»åŠ å¼•ç”¨ä¿¡æ¯
                arguments['citations'] = citations
                
                return arguments
            else:
                print(f"âŒ ç„¡æ³•å¾function callæå–çµæœ")
                return {}
            
    except Exception as e:
        print(f"âŒ çµæ§‹åŒ–å¯¦é©—ç´°ç¯€LLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        return {}

def call_llm_structured_revision_explain(user_feedback: str, proposal: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–ä¿®è¨‚èªªæ˜
    
    Args:
        user_feedback: ç”¨æˆ¶åé¥‹
        proposal: åŸå§‹ææ¡ˆ
    
    Returns:
        Dict[str, Any]: ç¬¦åˆREVISION_EXPLAIN_SCHEMAçš„çµæ§‹åŒ–ä¿®è¨‚èªªæ˜
    """
    print(f"ğŸ” èª¿ç”¨çµæ§‹åŒ–ä¿®è¨‚èªªæ˜LLMï¼Œç”¨æˆ¶åé¥‹é•·åº¦ï¼š{len(user_feedback)}")
    print(f"ğŸ” ææ¡ˆé•·åº¦ï¼š{len(proposal)} å­—ç¬¦")
    
    # ç²å–ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯å’Œåƒæ•¸
    try:
        from model_config_bridge import get_current_model, get_model_params
        current_model = get_current_model()
        llm_params = get_model_params()
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        print(f"ğŸ”§ æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        # ä½¿ç”¨fallbacké…ç½®
        current_model = "gpt-4-1106-preview"
        llm_params = {
            "model": "gpt-4-1106-preview",
            "temperature": 0.0,  # çµæ§‹åŒ–è¼¸å‡ºä½¿ç”¨0æº«åº¦
            "max_tokens": 4000,
            "timeout": 120,
        }
    
    try:
        # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ä¸åŒçš„API
        if current_model.startswith('gpt-5'):
            # GPT-5ç³»åˆ—ä½¿ç”¨Responses API with JSON Schema
            from openai import OpenAI
            client = OpenAI()
            
            # æº–å‚™Responses APIçš„åƒæ•¸
            max_tokens = llm_params.get('max_output_tokens', 4000)
            
            # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
            current_schema = create_revision_explain_schema()
            
            # æ§‹å»ºæç¤ºè©
            system_prompt = """
            You are a research proposal revision expert. Your task is to analyze the user's feedback and the original proposal, then provide a detailed explanation of the revision approach.
            
            Please provide a comprehensive explanation that includes:
            1. Analysis of the user's feedback
            2. Identification of key areas for improvement
            3. Specific revision strategies
            4. Expected outcomes and benefits
            5. Technical innovation points
            """
            
            user_prompt = f"""
            --- Original Proposal ---
            {proposal}
            
            --- User Feedback ---
            {user_feedback}
            
            Please provide a detailed revision explanation based on the above information.
            """
            
            # ä½¿ç”¨ Responses API + JSON Schema (é©ç”¨æ–¼æ‰€æœ‰ GPT-5 ç³»åˆ—æ¨¡å‹)
            responses_params = {
                'model': current_model,
                'input': [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                'text': {
                    'format': {
                        'type': 'json_schema',
                        'name': 'RevisionExplain',
                        'strict': True,
                        'schema': current_schema,
                    },
                    'verbosity': 'low'  # ä½¿ç”¨ low verbosity
                },
                'reasoning': {'effort': 'medium'},  # ä½¿ç”¨ medium reasoning
                'max_output_tokens': max_tokens
            }
            
            print(f"ğŸ”§ ä½¿ç”¨Responses API with JSON Schemaï¼Œåƒæ•¸ï¼š{responses_params}")
            
            # è™•ç†GPT-5çš„incompleteç‹€æ…‹
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                response = client.responses.create(**responses_params)
                
                print(f"ğŸ” DEBUG: APIèª¿ç”¨å®Œæˆ (å˜—è©¦ {retry_count + 1}/{max_retries})")
                print(f"ğŸ” DEBUG: response.status: {getattr(response, 'status', 'N/A')}")
                
                # æª¢æŸ¥æ•´é«”responseç‹€æ…‹
                if hasattr(response, 'status') and response.status == 'incomplete':
                    print(f"âš ï¸ æª¢æ¸¬åˆ°incompleteç‹€æ…‹ï¼Œç­‰å¾…å¾Œé‡è©¦...")
                    retry_count += 1
                    if retry_count < max_retries:
                        import time
                        time.sleep(2)  # ç­‰å¾…2ç§’å¾Œé‡è©¦
                        continue
                    else:
                        print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                        return {}
                
                # æå–JSONå…§å®¹
                try:
                    # å„ªå…ˆä½¿ç”¨ resp.output_text
                    output_text = getattr(response, 'output_text', None)
                    if output_text:
                        print(f"âœ… ä½¿ç”¨ resp.output_text æå–å…§å®¹: {len(output_text)} å­—ç¬¦")
                        try:
                            revision_data = json.loads(output_text)
                            print(f"âœ… æˆåŠŸè§£æJSONçµæ§‹åŒ–ä¿®è¨‚èªªæ˜")
                            
                            # æœ¬åœ°é©—è­‰ schema
                            try:
                                from jsonschema import validate
                                validate(instance=revision_data, schema=current_schema)
                                print(f"âœ… æœ¬åœ° Schema é©—è­‰é€šé")
                            except Exception as e:
                                print(f"âš ï¸ æœ¬åœ° Schema é©—è­‰å¤±æ•—: {e}")
                            
                            return revision_data
                        except json.JSONDecodeError as e:
                            print(f"âŒ JSONè§£æå¤±æ•—: {e}")
                            print(f"ğŸ” åŸå§‹è¼¸å‡º: {output_text}")
                            return {}
                    else:
                        print(f"âŒ ç„¡æ³•æå– output_text")
                        return {}
                        
                except Exception as e:
                    print(f"âŒ æå–JSONå…§å®¹å¤±æ•—: {e}")
                    retry_count += 1
                    if retry_count < max_retries:
                        import time
                        time.sleep(2)
                        continue
                    else:
                        print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                        return {}
            
            print(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—")
            return {}
            
        else:
            # GPT-4ç³»åˆ—ä½¿ç”¨Chat Completions API (LangChain)
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(**llm_params)
            
            # æ§‹å»ºæç¤ºè©
            system_prompt = """
            You are a research proposal revision expert. Your task is to analyze the user's feedback and the original proposal, then provide a detailed explanation of the revision approach.
            """
            
            full_prompt = f"{system_prompt}\n\n--- Original Proposal ---\n{proposal}\n\n--- User Feedback ---\n{user_feedback}\n\nPlease provide a detailed revision explanation."
            
            response = llm.invoke(full_prompt)
            print(f"âœ… å‚³çµ±LLMèª¿ç”¨æˆåŠŸï¼Œå›æ‡‰é•·åº¦ï¼š{len(response.content)} å­—ç¬¦")
            
            # è¿”å›æ–‡æœ¬æ ¼å¼
            return {
                'revision_explain': response.content
            }
            
    except Exception as e:
        print(f"âŒ çµæ§‹åŒ–ä¿®è¨‚èªªæ˜LLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        return {}

def generate_structured_experimental_detail(chunks: List[Document], proposal: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆçµæ§‹åŒ–å¯¦é©—ç´°ç¯€çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        chunks: æ–‡ç»ç‰‡æ®µ
        proposal: ç ”ç©¶ææ¡ˆ
    
    Returns:
        Dict[str, Any]: çµæ§‹åŒ–å¯¦é©—ç´°ç¯€
    """
    return call_llm_structured_experimental_detail(chunks, proposal)

def generate_structured_revision_explain(user_feedback: str, proposal: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆçµæ§‹åŒ–ä¿®è¨‚èªªæ˜çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        user_feedback: ç”¨æˆ¶åé¥‹
        proposal: åŸå§‹ææ¡ˆ
    
    Returns:
        Dict[str, Any]: çµæ§‹åŒ–ä¿®è¨‚èªªæ˜
    """
    return call_llm_structured_revision_explain(user_feedback, proposal)

def structured_experimental_detail_to_text(experimental_data: Dict[str, Any]) -> str:
    """
    å°‡çµæ§‹åŒ–å¯¦é©—ç´°ç¯€è½‰æ›ç‚ºå‚³çµ±æ–‡æœ¬æ ¼å¼
    
    Args:
        experimental_data: çµæ§‹åŒ–å¯¦é©—ç´°ç¯€æ•¸æ“š
    
    Returns:
        str: æ ¼å¼åŒ–çš„æ–‡æœ¬å¯¦é©—ç´°ç¯€
    """
    if not experimental_data:
        return ""
    
    text_parts = []
    
    
    # Synthesis Process
    if experimental_data.get('synthesis_process'):
        text_parts.append("## Synthesis Process")
        text_parts.append(f"{experimental_data['synthesis_process']}\n")
    
    # Materials and Conditions
    if experimental_data.get('materials_and_conditions'):
        text_parts.append("## Materials and Conditions")
        text_parts.append(f"{experimental_data['materials_and_conditions']}\n")
    
    # Analytical Methods
    if experimental_data.get('analytical_methods'):
        text_parts.append("## Analytical Methods")
        text_parts.append(f"{experimental_data['analytical_methods']}\n")
    
    # Precautions
    if experimental_data.get('precautions'):
        text_parts.append("## Precautions")
        text_parts.append(f"{experimental_data['precautions']}\n")
    
    return "\n".join(text_parts)

def build_inference_prompt(chunks: List[Document], question: str) -> Tuple[str, List[Dict]]:
    context_text = ""
    citations = []
    citation_map = {}
    
    for i, doc in enumerate(chunks):
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        
        # æª¢æŸ¥ï¼šé¿å…é‡è¤‡çš„ (filename, page) çµ„åˆ
        citation_key = f"{filename}_p{page}"
        if citation_key not in citation_map:
            label = f"[{len(citations) + 1}]"
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
            citation_map[citation_key] = label
        else:
            label = citation_map[citation_key]

        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
    You are a materials synthesis consultant who understands and excels at comparing the chemical and physical properties of materials. You can propose innovative suggestions based on known experimental conditions for situations where there is no clear literature.

    Please conduct extended thinking based on the following literature and experimental data:
    - You can propose new combinations, temperatures, times, or pathways.
    - Even combinations not yet documented in the literature can be suggested, but you must provide reasonable reasoning.
    - When making inferences and extended thinking, please try to mention "what literature clues this idea originates from" to support your explanation. The current literature excerpt numbers are [1] to [{len(chunks)}] (total {len(chunks)} excerpts)

    --- Literature Summary ---
    {context_text}

    --- Question ---
    {question}
    """
    return system_prompt.strip(), citations

def build_dual_inference_prompt(
    chunks_paper: List[Document],
    question: str,
    experiment_chunks: List[Document]
    ) -> Tuple[str, List[Dict]]:

    paper_context_text = ""
    exp_context_text = ""
    citations = []
    label_index = 1

    # --- Literature Summary ---
    paper_context_text += "--- Literature Summary ---\n"
    for doc in chunks_paper:
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{label_index}]"

        citations.append({
            "label": label,
            "title": title,
            "source": filename,
            "page": page,
            "snippet": snippet,
            "type": "paper"
        })

        paper_context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"
        label_index += 1

    # --- Experiment Summary ---
    exp_context_text += "--- Similar Experiment Summary ---\n"
    for doc in experiment_chunks:
        meta = doc.metadata
        filename = meta.get("filename") or meta.get("source", "Unknown")
        exp_id = meta.get("exp_id", "unknown_exp")
        snippet = doc.page_content[:80].replace("\n", " ")
        label = f"[{label_index}]"

        citations.append({
            "label": label,
            "title": exp_id,
            "source": filename,
            "page": "-",  # æ²’æœ‰é æ•¸
            "snippet": snippet,
            "type": "experiment"
        })

        exp_context_text += f"{label} Experiment {exp_id}\n{doc.page_content}\n\n"
        label_index += 1
        
    # --- Prompt Injection ---
    system_prompt = f"""
    You are a materials synthesis consultant who understands and excels at comparing the chemical and physical properties of materials.

    You will see three parts of information. Please conduct comprehensive analysis and provide specific inferences and innovative suggestions for experiments:
    1. Literature summary (with source annotations [1], [2])
    2. Similar experiment summary (from vector database)
    3. Experiment records (tables)

    Please propose new suggestions for the research question, including:
    - Adjusted synthesis pathways and conditions (such as temperature, time, ratios)
    - Factors that may affect synthesis success rate
    - Reasoning behind the causes, citing literature ([1], [2]...) or similar experiment results when necessary
    Important: You can only cite the provided literature excerpts. The current literature excerpt numbers are [1] to [{len(chunks_paper) + len(experiment_chunks)}] (total {len(chunks_paper) + len(experiment_chunks)} excerpts)

    --- Literature Knowledge Sources ---
    {paper_context_text}

    --- Experiment Records ---
    {exp_context_text}

    --- Research Question ---
    {question}
    """
    return system_prompt.strip(), citations



def expand_query(user_prompt: str) -> List[str]:
    """
    Convert user input natural language questions into multiple semantic search query statements.
    The returned English statements can be used for literature vector retrieval.
    """
    # Get dynamic model parameters
    try:
        from model_config_bridge import get_current_model, get_model_params
        current_model = get_current_model()
        llm_params = get_model_params()
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å–æ¨¡å‹åƒæ•¸ï¼š{e}")
        current_model = "gpt-4-1106-preview"
        llm_params = {
            "model": "gpt-4-1106-preview",
            "temperature": 0.3,
            "max_tokens": 4000,
            "timeout": 120,
        }

    system_prompt = """You are a scientific assistant helping expand a user's synthesis question into multiple semantic search queries. 
    Each query should be precise, relevant, and useful for retrieving related technical documents. 
    Only return a list of 3 to 6 search queries in English. Do not explain, do not include numbering if not needed."""

    full_prompt = f"{system_prompt}\n\nUser question:\n{user_prompt}"

    try:
        # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ä¸åŒçš„API
        if current_model.startswith('gpt-5'):
            # GPT-5ç³»åˆ—ä½¿ç”¨Responses API
            from openai import OpenAI
            client = OpenAI()
            
            # æº–å‚™Responses APIçš„åƒæ•¸
            responses_params = {
                'model': current_model,
                'input': [{'role': 'user', 'content': full_prompt}]
            }
            
            # æ·»åŠ å…¶ä»–åƒæ•¸ï¼ˆæ’é™¤modelå’Œinputï¼‰
            for key, value in llm_params.items():
                if key not in ['model', 'input']:
                    responses_params[key] = value
            
            # ä¿®å¾©ï¼šç§»é™¤reasoningåƒæ•¸ï¼Œé¿å…è¿”å›ResponseReasoningItem
            if 'reasoning' in responses_params:
                del responses_params['reasoning']
            
            # ç¢ºä¿ç§»é™¤reasoningåƒæ•¸
            if 'reasoning' in responses_params:
                print(f"ğŸ” DEBUG: ç§»é™¤ reasoning åƒæ•¸: {responses_params['reasoning']}")
                del responses_params['reasoning']
                print(f"ğŸ” DEBUG: æ›´æ–°å¾Œçš„åƒæ•¸: {responses_params}")
            
            response = client.responses.create(**responses_params)
            
            # ä¿®å¾©ï¼šæ ¹æ“šGPT-5 cookbookæ­£ç¢ºè™•ç†Responses APIçš„å›æ‡‰æ ¼å¼
            output = ""
            if hasattr(response, 'output') and response.output:
                for item in response.output:
                    # è·³éResponseReasoningItemå°è±¡
                    if hasattr(item, 'type') and item.type == 'reasoning':
                        continue
                    
                    if hasattr(item, "content"):
                        for content in item.content:
                            if hasattr(content, "text"):
                                output += content.text
                    elif hasattr(item, "text"):
                        # ç›´æ¥æ–‡æœ¬è¼¸å‡º
                        output += item.text
                    elif hasattr(item, "message"):
                        # messageå°è±¡
                        if hasattr(item.message, "content"):
                            output += item.message.content
                        else:
                            output += str(item.message)
                    else:
                        # å…¶ä»–æƒ…æ³ï¼Œå˜—è©¦è½‰æ›ç‚ºå­—ç¬¦ä¸²ï¼Œä½†éæ¿¾æ‰ResponseReasoningItem
                        item_str = str(item)
                        if not item_str.startswith('ResponseReasoningItem'):
                            output += item_str
            
            output = output.strip()
            
        else:
            # GPT-4ç³»åˆ—ä½¿ç”¨Chat Completions API (LangChain)
            llm = ChatOpenAI(**llm_params)
            output = llm.predict(full_prompt).strip()

        # Try to parse into query list
        if output.startswith("[") and output.endswith("]"):
            try:
                return eval(output)
            except Exception:
                pass  # fall back

        queries = [line.strip("-â€¢ ").strip() for line in output.split("\n") if line.strip()]
        return [q for q in queries if len(q) > 4]
        
    except Exception as e:
        print(f"âŒ æŸ¥è©¢æ“´å±•å¤±æ•—ï¼š{e}")
        # è¿”å›åŸå§‹æŸ¥è©¢ä½œç‚ºfallback
        return [user_prompt]


def build_proposal_prompt(question: str, chunks: List[Document]) -> Tuple[str, List[Dict]]:
    paper_context_text = ""
    citations = []
    citation_map = {}

    for i, doc in enumerate(chunks):
        
        meta = doc.metadata
        title = meta.get("title", "Untitled")
        filename = meta.get("filename") or meta.get("source", "Unknown")
        page = meta.get("page_number") or meta.get("page", "?")
        snippet = doc.page_content[:80].replace("\n", " ")
        
        # æª¢æŸ¥ï¼šé¿å…é‡è¤‡çš„ (filename, page) çµ„åˆ
        citation_key = f"{filename}_p{page}"
        if citation_key not in citation_map:
            label = f"[{len(citations) + 1}]"
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
            citation_map[citation_key] = label
        else:
            label = citation_map[citation_key]

        paper_context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
    You are a scientific research expert who excels at proposing innovative and feasible research proposals based on literature summaries and research objectives.
    Your expertise covers materials science, chemistry, physics, and engineering, and you are capable of deriving new ideas grounded in experimental evidence and theoretical principles.

    Your task is to generate a structured research proposal based on the provided literature excerpts and research objectives. The proposal should be innovative, scientifically rigorous, and feasible.

    IMPORTANT: You must respond in valid JSON format only. Do not include any text before or after the JSON object.

    The JSON must have the following structure:
    {{
        "proposal_title": "Title of the research proposal",
        "need": "Research need and current limitations",
        "solution": "Proposed design and development strategies",
        "differentiation": "Comparison with existing technologies",
        "benefit": "Expected improvements and benefits",
        "experimental_overview": "Experimental approach and methodology",
        "materials_list": ["material1", "material2", "material3"]
    }}

    Key requirements:
    1. Propose new components, structures, or mechanisms (e.g., new ligands, frameworks, catalysts, processing techniques) based on the literature
    2. Clearly explain structural or functional advantages and potential reactivity/performance
    3. All proposed designs must have a logical basis â€” avoid inventing unreasonable structures without justification
    4. Maintain scientific rigor, clarity, and avoid vague descriptions
    5. Use only the provided literature labels ([1], [2], etc.) for citations, and do not fabricate sources
    6. Ensure every claim is supported by a cited source or reasonable extension from the literature
    7. For materials_list, include ONLY IUPAC chemical names without any descriptions, notes, or parenthetical explanations. Each item must be a single chemical name only.

    Literature excerpts are provided below with labels from [1] to [{len(chunks)}] (total {len(chunks)} excerpts).
    """
    
    return system_prompt.strip(), citations

def build_detail_experimental_plan_prompt(chunks: List[Document], proposal_text: str) -> Tuple[str, List[Dict]]:
    context_text = ""
    citations = []
    citation_map = {}
    
    for i, doc in enumerate(chunks):
        # æª¢æŸ¥ï¼šdoc æ‡‰æœ‰ metadata å±¬æ€§ï¼Œä¸”ç‚º dict
        metadata = doc.metadata
        title = metadata.get("title", "Untitled")
        # æª¢æŸ¥ï¼šfilename ä¾†æºæ–¼ "filename" æˆ– "source"ï¼Œè‹¥éƒ½ç„¡å‰‡ç‚º "Unknown"
        filename = metadata.get("filename") or metadata.get("source", "Unknown")
        # æª¢æŸ¥ï¼špage ä¾†æºæ–¼ "page_number" æˆ– "page"ï¼Œè‹¥éƒ½ç„¡å‰‡ç‚º "?"
        page = metadata.get("page_number") or metadata.get("page", "?")
        # é è¦½ç‰‡æ®µï¼Œå–å‰ 80 å­—å…ƒï¼Œä¸¦å°‡æ›è¡Œæ›¿æ›ç‚ºç©ºæ ¼
        snippet = doc.page_content[:80].replace("\n", " ")

        # æª¢æŸ¥ï¼šé¿å…é‡è¤‡çš„ (filename, page) çµ„åˆ
        citation_key = f"{filename}_p{page}"
        if citation_key not in citation_map:
            label = f"[{len(citations) + 1}]"
            citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })
            citation_map[citation_key] = label
        else:
            label = citation_map[citation_key]

        # context_text ç´¯åŠ æ¯å€‹ chunk çš„å…§å®¹ï¼Œæ ¼å¼ç‚º [n] title | Page n
        context_text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

    system_prompt = f"""
    You are an experienced consultant in materials experiment design. Based on the following research proposal and related literature excerpts, please provide the researcher with a detailed set of recommended experimental procedures.

    IMPORTANT: Please provide your response in plain text format only. Do NOT use any markdown formatting, bold text, or special formatting. Use simple text with clear section headers and bullet points.

    Please include the following sections:

    SYNTHESIS PROCESS:
    Provide a step-by-step description of each experimental operation, including sequence, logic, and purpose.
    Guidelines for synthesis process:
    - Suggest specific ranges of experimental conditions (temperature, time, pressure, etc.)
    - For each reaction condition and step mentioned in the literature, cite the source ([1], [2], etc.)
    - For suggested conditions not based on literature, explain your logic clearly

    MATERIALS AND CONDITIONS:
    List the required raw materials for each step (including proportions) and the reaction conditions (temperature, time, containers).

    ANALYTICAL METHODS:
    Suggest characterization tools (such as XRD, BET, TGA) and explain the purpose of each.

    PRECAUTIONS:
    Highlight key points or parameter limitations mentioned in the literature.

    Format your response with clear section headers in CAPITAL LETTERS, followed by detailed explanations. Use simple bullet points (-) for lists.
    Use [1], [2], etc. to cite the literature sources in your response. Only cite the provided literature excerpts, numbered [1] to [{len(chunks)}] (total {len(chunks)} excerpts).

    --- literature chunks ---
    {context_text}

    --- User's Proposal ---
    {proposal_text}
    """
    return system_prompt.strip(), citations



def build_iterative_proposal_prompt(
        question: str,
        new_chunks: List[Document],
        old_chunks: List[Document],
        past_proposal: str
    ) -> Tuple[str, List[Dict]]:
    """
    Build a new research proposal prompt that combines user feedback, newly retrieved literature, old literature, and the original proposal.
    Also returns citation list.
    """
    citations = []

    def format_chunks(chunks: List[Document], label_offset=0) -> Tuple[str, List[Dict]]:
        text = ""
        local_citations = []
        for i, doc in enumerate(chunks):
            meta = doc.metadata
            title = meta.get("title", "Untitled")
            filename = meta.get("filename") or meta.get("source", "Unknown")
            page = meta.get("page_number") or meta.get("page", "?")
            snippet = doc.page_content[:80].replace("\n", " ")
            label = f"[{label_offset + i + 1}]"

            local_citations.append({
                "label": label,
                "title": title,
                "source": filename,
                "page": page,
                "snippet": snippet
            })

            text += f"{label} {title} | Page {page}\n{doc.page_content}\n\n"

        return text, local_citations

    old_text, old_citations = format_chunks(old_chunks)
    new_text, new_citations = format_chunks(new_chunks, label_offset=len(old_citations))
    citations.extend(old_citations + new_citations)

    system_prompt = f"""
    You are an experienced materials experiment design consultant. Please help modify parts of the research proposal based on user feedback, original proposal, and literature content.

    Your task is to generate a modified research proposal based on user feedback, original proposal, and literature content. The proposal should be innovative, scientifically rigorous, and feasible.

    IMPORTANT: You must respond in valid JSON format only. Do not include any text before or after the JSON object.

    The JSON must have the following structure:
    {{
        "proposal_title": "Title of the research proposal",
        "need": "Research need and current limitations",
        "solution": "Proposed design and development strategies",
        "differentiation": "Comparison with existing technologies",
        "benefit": "Expected improvements and benefits",
        "experimental_overview": "Experimental approach and methodology",
        "materials_list": ["material1", "material2", "material3"]
    }}

    Key requirements:
    1. Prioritize the areas that the user wants to modify and look for possible improvement directions from the literature
    2. Except for the areas that the user is dissatisfied with, other parts should maintain the original proposal content without changes
    3. Maintain scientific rigor, clarity, and avoid vague descriptions
    4. Use only the provided literature labels ([1], [2], etc.) for citations, and do not fabricate sources
    5. Ensure every claim is supported by a cited source or reasonable extension from the literature
    6. For materials_list, include ONLY IUPAC chemical names without any descriptions, notes, or parenthetical explanations. Each item must be a single chemical name only.

    Literature excerpts are provided below with labels from [1] to [{len(old_chunks) + len(new_chunks)}] (total {len(old_chunks) + len(new_chunks)} excerpts).
    """
    
    user_prompt = f"""
    --- User Feedback ---
    {question}

    --- Original Proposal Content ---
    {past_proposal}

    --- Literature Excerpts Based on Original Proposal ---
    {old_text}

    --- New Retrieved Excerpts Based on Feedback ---
    {new_text}
    """
    
    return system_prompt.strip(), user_prompt, citations


def generate_structured_proposal(chunks: List[Document], question: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆçµæ§‹åŒ–ç ”ç©¶ææ¡ˆ
    
    Args:
        chunks: æª¢ç´¢åˆ°çš„æ–‡ç»ç‰‡æ®µ
        question: ç”¨æˆ¶çš„ç ”ç©¶å•é¡Œ
    
    Returns:
        Dict[str, Any]: çµæ§‹åŒ–çš„ç ”ç©¶ææ¡ˆ
    """
    # æ·»åŠ èª¿è©¦æ—¥èªŒ
    print(f"ğŸ” DEBUG: generate_structured_proposal é–‹å§‹")
    print(f"ğŸ” DEBUG: chunks é•·åº¦: {len(chunks) if chunks else 0}")
    print(f"ğŸ” DEBUG: question: {question}")
    
    system_prompt, citations = build_proposal_prompt(question, chunks)
    
    # æ§‹å»ºç”¨æˆ¶æç¤ºè©ï¼ˆåŒ…å«æ–‡ç»æ‘˜è¦ï¼‰
    paper_context_text = ""
    for i, doc in enumerate(chunks):
        metadata = doc.metadata
        title = metadata.get("title", "Untitled")
        filename = metadata.get("filename") or metadata.get("source", "Unknown")
        page = metadata.get("page_number") or metadata.get("page", "?")
        
        paper_context_text += f"[{i+1}] {title} | Page {page}\n{doc.page_content}\n\n"
    
    user_prompt = f"""
    --- Literature Excerpts ---
    {paper_context_text}

    --- Research Objectives ---
    {question}
    """
    
    # èª¿ç”¨çµæ§‹åŒ–LLM
    proposal_data = call_llm_structured_proposal(system_prompt, user_prompt)
    
    # æ·»åŠ å¼•ç”¨ä¿¡æ¯åˆ°è¿”å›çµæœ
    if proposal_data:
        proposal_data['citations'] = citations
    
    return proposal_data


def generate_iterative_structured_proposal(
        question: str,
        new_chunks: List[Document],
        old_chunks: List[Document],
        past_proposal: str
    ) -> Dict[str, Any]:
    """
    ç”Ÿæˆè¿­ä»£å¼çµæ§‹åŒ–ç ”ç©¶ææ¡ˆ
    
    Args:
        question: ç”¨æˆ¶åé¥‹
        new_chunks: æ–°æª¢ç´¢åˆ°çš„æ–‡ç»ç‰‡æ®µ
        old_chunks: åŸæœ‰çš„æ–‡ç»ç‰‡æ®µ
        past_proposal: ä¹‹å‰çš„ææ¡ˆå…§å®¹
    
    Returns:
        Dict[str, Any]: ä¿®æ”¹å¾Œçš„çµæ§‹åŒ–ç ”ç©¶ææ¡ˆ
    """
    system_prompt, user_prompt, citations = build_iterative_proposal_prompt(
        question, new_chunks, old_chunks, past_proposal
    )
    
    # èª¿ç”¨çµæ§‹åŒ–LLM
    proposal_data = call_llm_structured_proposal(system_prompt, user_prompt)
    
    # æ·»åŠ å¼•ç”¨ä¿¡æ¯åˆ°è¿”å›çµæœ
    if proposal_data:
        proposal_data['citations'] = citations
    
    return proposal_data


def structured_proposal_to_text(proposal_data: Dict[str, Any]) -> str:
    """
    å°‡çµæ§‹åŒ–ææ¡ˆè½‰æ›ç‚ºå‚³çµ±æ–‡æœ¬æ ¼å¼
    
    Args:
        proposal_data: çµæ§‹åŒ–ææ¡ˆæ•¸æ“š
    
    Returns:
        str: æ ¼å¼åŒ–çš„æ–‡æœ¬ææ¡ˆ
    """
    if not proposal_data:
        return ""
    
    text_parts = []
    
    # æ¨™é¡Œ
    if proposal_data.get('proposal_title'):
        text_parts.append(f"Proposal: {proposal_data['proposal_title']}\n")
    
    # Need
    if proposal_data.get('need'):
        text_parts.append("Need:\n")
        text_parts.append(f"{proposal_data['need']}\n")
    
    # Solution
    if proposal_data.get('solution'):
        text_parts.append("Solution:\n")
        text_parts.append(f"{proposal_data['solution']}\n")
    
    # Differentiation
    if proposal_data.get('differentiation'):
        text_parts.append("Differentiation:\n")
        text_parts.append(f"{proposal_data['differentiation']}\n")
    
    # Benefit
    if proposal_data.get('benefit'):
        text_parts.append("Benefit:\n")
        text_parts.append(f"{proposal_data['benefit']}\n")
    

    
    # Experimental overview
    if proposal_data.get('experimental_overview'):
        text_parts.append("Experimental overview:\n")
        text_parts.append(f"{proposal_data['experimental_overview']}\n")
    
    # Materials list
    if proposal_data.get('materials_list'):
        materials_json = json.dumps(proposal_data['materials_list'], ensure_ascii=False, indent=2)
        text_parts.append(f"```json\n{materials_json}\n```\n")
    
    return "\n".join(text_parts)


def generate_proposal_with_fallback(chunks: List[Document], question: str) -> Tuple[str, Dict[str, Any]]:
    """
    ç”Ÿæˆç ”ç©¶ææ¡ˆï¼Œå„ªå…ˆä½¿ç”¨çµæ§‹åŒ–è¼¸å‡ºï¼Œå¤±æ•—æ™‚å›é€€åˆ°å‚³çµ±æ–‡æœ¬æ ¼å¼
    
    Args:
        chunks: æª¢ç´¢åˆ°çš„æ–‡ç»ç‰‡æ®µ
        question: ç”¨æˆ¶çš„ç ”ç©¶å•é¡Œ
    
    Returns:
        Tuple[str, Dict[str, Any]]: (æ–‡æœ¬æ ¼å¼ææ¡ˆ, çµæ§‹åŒ–ææ¡ˆæ•¸æ“š)
    """
    # æ·»åŠ è©³ç´°çš„èª¿è©¦æ—¥èªŒ
    print(f"ğŸ” DEBUG: generate_proposal_with_fallback é–‹å§‹")
    print(f"ğŸ” DEBUG: chunks é¡å‹: {type(chunks)}")
    print(f"ğŸ” DEBUG: chunks é•·åº¦: {len(chunks) if chunks else 0}")
    print(f"ğŸ” DEBUG: question: {question}")
    
    # é©—è­‰ chunks çš„æ ¼å¼
    if chunks:
        print(f"ğŸ” DEBUG: ç¬¬ä¸€å€‹ chunk é¡å‹: {type(chunks[0])}")
        if hasattr(chunks[0], 'metadata'):
            print(f"ğŸ” DEBUG: ç¬¬ä¸€å€‹ chunk æœ‰ metadata å±¬æ€§")
            print(f"ğŸ” DEBUG: ç¬¬ä¸€å€‹ chunk metadata: {chunks[0].metadata}")
        else:
            print(f"ğŸ” DEBUG: ç¬¬ä¸€å€‹ chunk æ²’æœ‰ metadata å±¬æ€§")
            print(f"ğŸ” DEBUG: ç¬¬ä¸€å€‹ chunk å…§å®¹: {chunks[0]}")
    
    # é¦–å…ˆå˜—è©¦çµæ§‹åŒ–è¼¸å‡º
    try:
        print("ğŸ”§ å˜—è©¦ä½¿ç”¨çµæ§‹åŒ–è¼¸å‡ºç”Ÿæˆææ¡ˆ...")
        print(f"ğŸ” DEBUG: èª¿ç”¨ generate_structured_proposal å‰ï¼Œchunks é•·åº¦: {len(chunks)}")
        structured_proposal = generate_structured_proposal(chunks, question)
        print(f"ğŸ” DEBUG: generate_structured_proposal è¿”å›: {type(structured_proposal)}")
        print(f"ğŸ” DEBUG: structured_proposal å…§å®¹: {structured_proposal}")
        
        if structured_proposal and all(key in structured_proposal for key in ['proposal_title', 'need', 'solution']):
            print("âœ… çµæ§‹åŒ–ææ¡ˆç”ŸæˆæˆåŠŸ")
            text_proposal = structured_proposal_to_text(structured_proposal)
            print(f"ğŸ” DEBUG: ç”Ÿæˆçš„æ–‡æœ¬ææ¡ˆé•·åº¦: {len(text_proposal)}")
            return text_proposal, structured_proposal
        else:
            print("âš ï¸ çµæ§‹åŒ–ææ¡ˆç”Ÿæˆå¤±æ•—æˆ–æ ¼å¼ä¸å®Œæ•´ï¼Œå›é€€åˆ°å‚³çµ±æ ¼å¼")
            if structured_proposal:
                print(f"ğŸ” DEBUG: ç¼ºå°‘çš„éµ: {[key for key in ['proposal_title', 'need', 'solution'] if key not in structured_proposal]}")
    except Exception as e:
        print(f"âŒ çµæ§‹åŒ–ææ¡ˆç”Ÿæˆå¤±æ•—: {e}ï¼Œå›é€€åˆ°å‚³çµ±æ ¼å¼")
        import traceback
        traceback.print_exc()
    
    # å›é€€åˆ°å‚³çµ±æ–‡æœ¬æ ¼å¼
    try:
        print("ğŸ”§ ä½¿ç”¨å‚³çµ±æ–‡æœ¬æ ¼å¼ç”Ÿæˆææ¡ˆ...")
        # ä¿®å¾©åƒæ•¸é †åºï¼šæ‡‰è©²æ˜¯ (question, chunks) è€Œä¸æ˜¯ (chunks, question)
        system_prompt, citations = build_proposal_prompt(question, chunks)
        
        # æ§‹å»ºå®Œæ•´çš„æç¤ºè©
        paper_context_text = ""
        for i, doc in enumerate(chunks):
            # æ·»åŠ é¡å¤–çš„é¡å‹æª¢æŸ¥
            if not hasattr(doc, 'metadata'):
                print(f"âŒ DEBUG: chunk {i} æ²’æœ‰ metadata å±¬æ€§ï¼Œé¡å‹: {type(doc)}")
                continue
                
            metadata = doc.metadata
            title = metadata.get("title", "Untitled")
            filename = metadata.get("filename") or metadata.get("source", "Unknown")
            page = metadata.get("page_number") or metadata.get("page", "?")
            
            paper_context_text += f"[{i+1}] {title} | Page {page}\n{doc.page_content}\n\n"
        
        full_prompt = f"{system_prompt}\n\n--- Literature Excerpts ---\n{paper_context_text}\n--- Research Objectives ---\n{question}"
        
        # èª¿ç”¨å‚³çµ±LLM
        text_proposal = call_llm(full_prompt)
        
        # å‰µå»ºä¸€å€‹åŸºæœ¬çš„çµæ§‹åŒ–æ•¸æ“šï¼ˆç”¨æ–¼å‘å¾Œå…¼å®¹ï¼‰
        basic_structured = {
            'proposal_title': 'Generated from text format',
            'need': '',
            'solution': '',
            'differentiation': '',
            'benefit': '',
            'experimental_overview': '',
            'materials_list': [],
            'citations': citations,
            'text_format': text_proposal
        }
        
        print("âœ… å‚³çµ±æ–‡æœ¬ææ¡ˆç”ŸæˆæˆåŠŸ")
        return text_proposal, basic_structured
        
    except Exception as e:
        print(f"âŒ å‚³çµ±ææ¡ˆç”Ÿæˆä¹Ÿå¤±æ•—: {e}")
        import traceback
        print(f"ğŸ” DEBUG: è©³ç´°éŒ¯èª¤ä¿¡æ¯:")
        traceback.print_exc()
        return "", {}

def call_llm_structured_revision_proposal(question: str, new_chunks: List[Document], old_chunks: List[Document], proposal: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨OpenAI Responses APIçš„JSON structured outputç”Ÿæˆçµæ§‹åŒ–ä¿®è¨‚ææ¡ˆ (åŒ…å«ä¿®è¨‚èªªæ˜)
    
    Args:
        question: ç”¨æˆ¶åé¥‹/å•é¡Œ
        new_chunks: æ–°æª¢ç´¢çš„æ–‡æª”å¡Š
        old_chunks: åŸå§‹æ–‡æª”å¡Š
        proposal: åŸå§‹ææ¡ˆ
    
    Returns:
        Dict[str, Any]: ç¬¦åˆREVISION_PROPOSAL_SCHEMAçš„çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆ
    """
    print(f"ğŸ” èª¿ç”¨çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆLLMï¼Œç”¨æˆ¶åé¥‹é•·åº¦ï¼š{len(question)}")
    print(f"ğŸ” æ–°æ–‡æª”å¡Šæ•¸é‡ï¼š{len(new_chunks)}")
    print(f"ğŸ” åŸæ–‡æª”å¡Šæ•¸é‡ï¼š{len(old_chunks)}")
    print(f"ğŸ” åŸå§‹ææ¡ˆé•·åº¦ï¼š{len(proposal)} å­—ç¬¦")
    
    # ç²å–ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯å’Œåƒæ•¸
    try:
        from model_config_bridge import get_current_model, get_model_params
        current_model = get_current_model()
        llm_params = get_model_params()
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{current_model}")
        print(f"ğŸ”§ æ¨¡å‹åƒæ•¸ï¼š{llm_params}")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç²å–æ¨¡å‹ä¿¡æ¯ï¼š{e}")
        # ä½¿ç”¨fallbacké…ç½®
        current_model = "gpt-4-1106-preview"
        llm_params = {
            "model": "gpt-4-1106-preview",
            "temperature": 0.0,  # çµæ§‹åŒ–è¼¸å‡ºä½¿ç”¨0æº«åº¦
            "max_tokens": 4000,
            "timeout": 120,
        }
    
    try:
        # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡ä¸åŒçš„API
        if current_model.startswith('gpt-5'):
            # GPT-5ç³»åˆ—ä½¿ç”¨Responses API with JSON Schema
            from openai import OpenAI
            client = OpenAI()
            
            # æº–å‚™Responses APIçš„åƒæ•¸
            max_tokens = llm_params.get('max_output_tokens', 4000)
            
            # å‹•æ…‹ç²å–æœ€æ–°çš„ schema
            current_schema = create_revision_proposal_schema()
            
            # æ§‹å»ºæç¤ºè©
            system_prompt = """
            You are an experienced materials experiment design consultant. Please help modify parts of the research proposal based on user feedback, original proposal, and literature content.

            Your task is to generate a modified research proposal based on user feedback, original proposal, and literature content. The proposal should be innovative, scientifically rigorous, and feasible.

            IMPORTANT: You must respond in valid JSON format only. Do not include any text before or after the JSON object.

            The JSON must have the following structure:
            {
                "revision_explanation": "Brief explanation of revision logic and key improvements based on user feedback",
                "proposal_title": "Title of the research proposal",
                "need": "Research need and current limitations",
                "solution": "Proposed design and development strategies",
                "differentiation": "Comparison with existing technologies",
                "benefit": "Expected improvements and benefits",
                "experimental_overview": "Experimental approach and methodology",
                "materials_list": ["material1", "material2", "material3"]
            }

            Key requirements:
            1. Prioritize the areas that the user wants to modify and look for possible improvement directions from the literature
            2. Except for the areas that the user is dissatisfied with, other parts should maintain the original proposal content without changes
            3. Maintain scientific rigor, clarity, and avoid vague descriptions
            4. Use only the provided literature labels ([1], [2], etc.) for citations, and do not fabricate sources
            5. Ensure every claim is supported by a cited source or reasonable extension from the literature
            6. For materials_list, include ONLY IUPAC chemical names without any descriptions, notes, or parenthetical explanations. Each item must be a single chemical name only.
            7. The revision_explanation should briefly explain the logic of changes and key improvements based on user feedback
            """
            
            # æ§‹å»ºæ–‡æª”å…§å®¹
            old_text = ""
            for i, doc in enumerate(old_chunks):
                metadata = doc.metadata
                title = metadata.get("title", "Untitled")
                filename = metadata.get("filename") or metadata.get("source", "Unknown")
                page = metadata.get("page_number") or metadata.get("page", "?")
                snippet = doc.page_content[:80].replace("\n", " ")
                old_text += f"    [{i+1}] {title} | Page {page}\n{snippet}\n\n"
            
            new_text = ""
            for i, doc in enumerate(new_chunks):
                metadata = doc.metadata
                title = metadata.get("title", "Untitled")
                filename = metadata.get("filename") or metadata.get("source", "Unknown")
                page = metadata.get("page_number") or metadata.get("page", "?")
                snippet = doc.page_content[:80].replace("\n", " ")
                new_text += f"    [{i+1}] {title} | Page {page}\n{snippet}\n\n"
            
            user_prompt = f"""
            --- User Feedback ---
            {question}

            --- Original Proposal Content ---
            {proposal}

            --- Literature Excerpts Based on Original Proposal ---
            {old_text}

            --- New Retrieved Excerpts Based on Feedback ---
            {new_text}
            """
            
            # ä½¿ç”¨ Responses API + JSON Schema (é©ç”¨æ–¼æ‰€æœ‰ GPT-5 ç³»åˆ—æ¨¡å‹)
            responses_params = {
                'model': current_model,
                'input': [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                'text': {
                    'format': {
                        'type': 'json_schema',
                        'name': 'RevisionProposal',
                        'strict': True,
                        'schema': current_schema,
                    },
                    'verbosity': 'low'  # ä½¿ç”¨ low verbosity
                },
                'reasoning': {'effort': 'medium'},  # ä½¿ç”¨ medium reasoning
                'max_output_tokens': max_tokens
            }
            
            print(f"ğŸ”§ ä½¿ç”¨Responses API with JSON Schemaï¼Œåƒæ•¸ï¼š{responses_params}")
            
            # è™•ç†GPT-5çš„incompleteç‹€æ…‹
            max_retries = 3
            retry_count = 0
            
            while retry_count < max_retries:
                response = client.responses.create(**responses_params)
                
                print(f"ğŸ” DEBUG: APIèª¿ç”¨å®Œæˆ (å˜—è©¦ {retry_count + 1}/{max_retries})")
                print(f"ğŸ” DEBUG: response.status: {getattr(response, 'status', 'N/A')}")
                
                # æª¢æŸ¥æ•´é«”responseç‹€æ…‹
                if hasattr(response, 'status') and response.status == 'incomplete':
                    print(f"âš ï¸ æª¢æ¸¬åˆ°incompleteç‹€æ…‹ï¼Œç­‰å¾…å¾Œé‡è©¦...")
                    retry_count += 1
                    if retry_count < max_retries:
                        import time
                        time.sleep(2)  # ç­‰å¾…2ç§’å¾Œé‡è©¦
                        continue
                    else:
                        print(f"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸")
                        return {}
                
                # æå–JSONå…§å®¹
                try:
                    # å„ªå…ˆä½¿ç”¨ resp.output_text
                    output_text = getattr(response, 'output_text', None)
                    if output_text:
                        print(f"âœ… ä½¿ç”¨ resp.output_text æå–å…§å®¹: {len(output_text)} å­—ç¬¦")
                        try:
                            revision_data = json.loads(output_text)
                            print(f"âœ… æˆåŠŸè§£æJSONçµæ§‹åŒ–ä¿®è¨‚ææ¡ˆ")
                            
                            # æœ¬åœ°é©—è­‰ schema
                            try:
                                from jsonschema import validate
                                validate(instance=revision_data, schema=current_schema)
                                print(f"âœ… æœ¬åœ° Schema é©—è­‰é€šé")
                            except Exception as e:
                                print(f"âš ï¸ æœ¬åœ° Schema é©—è­‰å¤±æ•—: {e}")
                            
                            return revision_data
                        except json.JSONDecodeError as e:
                            print(f"âŒ JSONè§£æå¤±æ•—: {e}")
                            print(f"âŒ åŸå§‹è¼¸å‡º: {output_text[:500]}...")
                            retry_count += 1
                            if retry_count < max_retries:
                                continue
                            else:
                                return {}
                    
                    # å¦‚æœæ²’æœ‰ output_textï¼Œå˜—è©¦å…¶ä»–æ–¹å¼
                    print(f"âš ï¸ æ²’æœ‰æ‰¾åˆ° output_textï¼Œå˜—è©¦å…¶ä»–æ–¹å¼...")
                    retry_count += 1
                    if retry_count < max_retries:
                        continue
                    else:
                        return {}
                        
                except Exception as e:
                    print(f"âŒ è™•ç†éŸ¿æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    retry_count += 1
                    if retry_count < max_retries:
                        continue
                    else:
                        return {}
            
            print(f"âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—")
            return {}
            
        else:
            # GPT-4ç³»åˆ—ä½¿ç”¨Chat Completions API (LangChain)
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(**llm_params)
            
            # æ§‹å»ºæç¤ºè©
            system_prompt = """
            You are an experienced materials experiment design consultant. Please help modify parts of the research proposal based on user feedback, original proposal, and literature content.
            """
            
            # æ§‹å»ºæ–‡æª”å…§å®¹
            old_text = ""
            for i, doc in enumerate(old_chunks):
                metadata = doc.metadata
                title = metadata.get("title", "Untitled")
                filename = metadata.get("filename") or metadata.get("source", "Unknown")
                page = metadata.get("page_number") or metadata.get("page", "?")
                snippet = doc.page_content[:80].replace("\n", " ")
                old_text += f"    [{i+1}] {title} | Page {page}\n{snippet}\n\n"
            
            new_text = ""
            for i, doc in enumerate(new_chunks):
                metadata = doc.metadata
                title = metadata.get("title", "Untitled")
                filename = metadata.get("filename") or metadata.get("source", "Unknown")
                page = metadata.get("page_number") or metadata.get("page", "?")
                snippet = doc.page_content[:80].replace("\n", " ")
                new_text += f"    [{i+1}] {title} | Page {page}\n{snippet}\n\n"
            
            full_prompt = f"{system_prompt}\n\n--- User Feedback ---\n{question}\n\n--- Original Proposal Content ---\n{proposal}\n\n--- Literature Excerpts Based on Original Proposal ---\n{old_text}\n\n--- New Retrieved Excerpts Based on Feedback ---\n{new_text}\n\nPlease provide a modified research proposal with revision explanation."
            
            response = llm.invoke(full_prompt)
            print(f"âœ… å‚³çµ±LLMèª¿ç”¨æˆåŠŸï¼Œå›æ‡‰é•·åº¦ï¼š{len(response.content)} å­—ç¬¦")
            
            # è¿”å›æ–‡æœ¬æ ¼å¼
            return {
                'revision_explanation': 'Revision based on user feedback',
                'proposal_title': 'Modified Research Proposal',
                'need': response.content,
                'solution': response.content,
                'differentiation': response.content,
                'benefit': response.content,
                'experimental_overview': response.content,
                'materials_list': ['sample_material']
            }
            
    except Exception as e:
        print(f"âŒ çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆLLMèª¿ç”¨å¤±æ•—ï¼š{e}")
        return {}

def generate_structured_revision_explain(user_feedback: str, proposal: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆçµæ§‹åŒ–ä¿®è¨‚èªªæ˜çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        user_feedback: ç”¨æˆ¶åé¥‹
        proposal: åŸå§‹ææ¡ˆ
    
    Returns:
        Dict[str, Any]: çµæ§‹åŒ–ä¿®è¨‚èªªæ˜
    """
    return call_llm_structured_revision_explain(user_feedback, proposal)

def generate_structured_revision_proposal(question: str, new_chunks: List[Document], old_chunks: List[Document], proposal: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆçµæ§‹åŒ–ä¿®è¨‚ææ¡ˆçš„ä¾¿æ·å‡½æ•¸ (åŒ…å«ä¿®è¨‚èªªæ˜)
    
    Args:
        question: ç”¨æˆ¶åé¥‹/å•é¡Œ
        new_chunks: æ–°æª¢ç´¢çš„æ–‡æª”å¡Š
        old_chunks: åŸå§‹æ–‡æª”å¡Š
        proposal: åŸå§‹ææ¡ˆ
    
    Returns:
        Dict[str, Any]: çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆ (åŒ…å«ä¿®è¨‚èªªæ˜)
    """
    return call_llm_structured_revision_proposal(question, new_chunks, old_chunks, proposal)

def structured_revision_proposal_to_text(revision_data: Dict[str, Any]) -> str:
    """
    å°‡çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆè½‰æ›ç‚ºå‚³çµ±æ–‡æœ¬æ ¼å¼
    
    Args:
        revision_data: çµæ§‹åŒ–ä¿®è¨‚ææ¡ˆæ•¸æ“š (åŒ…å«ä¿®è¨‚èªªæ˜)
    
    Returns:
        str: æ ¼å¼åŒ–çš„æ–‡æœ¬ææ¡ˆ
    """
    if not revision_data:
        return ""
    
    text_parts = []
    
    # ä¿®è¨‚èªªæ˜
    if revision_data.get('revision_explanation'):
        text_parts.append("Revision Explanation:")
        text_parts.append(f"{revision_data['revision_explanation']}\n")
    
    # æ¨™é¡Œ
    if revision_data.get('proposal_title'):
        text_parts.append(f"Proposal: {revision_data['proposal_title']}\n")
    
    # Need
    if revision_data.get('need'):
        text_parts.append("Need:\n")
        text_parts.append(f"{revision_data['need']}\n")
    
    # Solution
    if revision_data.get('solution'):
        text_parts.append("Solution:\n")
        text_parts.append(f"{revision_data['solution']}\n")
    
    # Differentiation
    if revision_data.get('differentiation'):
        text_parts.append("Differentiation:\n")
        text_parts.append(f"{revision_data['differentiation']}\n")
    
    # Benefit
    if revision_data.get('benefit'):
        text_parts.append("Benefit:\n")
        text_parts.append(f"{revision_data['benefit']}\n")
    
    # Experimental Overview
    if revision_data.get('experimental_overview'):
        text_parts.append("Experimental Overview:\n")
        text_parts.append(f"{revision_data['experimental_overview']}\n")
    
    # Materials list
    if revision_data.get('materials_list'):
        materials_json = json.dumps(revision_data['materials_list'], ensure_ascii=False, indent=2)
        text_parts.append(f"```json\n{materials_json}\n```\n")
    
    return "\n".join(text_parts)


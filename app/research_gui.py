import streamlit as st
from perplexity_search_fallback import ask_perplexity
from search_agent import search_and_download_only
import os
import tempfile
import re
from knowledge_agent import agent_answer
from browser import select_files, select_files_paper_mode
from file_upload import process_uploaded_files
from chunk_embedding import embed_documents_from_metadata, embed_experiment_txt_batch
import pandas as pd
from excel_to_txt_by_row import export_new_experiments_to_txt
from config import EXPERIMENT_DIR
from pubchem_handler import chemical_metadata_extractor
from rag_core import build_detail_experimental_plan_prompt
from docx import Document
from docx.shared import Inches
import io
from io import BytesIO
import requests

# SVG è½‰æ›ä¾è³´æª¢æŸ¥
try:
    from svglib.svglib import svg2rlg
    from reportlab.graphics import renderPDF
    SVGLIB_AVAILABLE = True
except ImportError as e:
    SVGLIB_AVAILABLE = False

# PyMuPDF ç”¨æ–¼ PDF åˆ° PNG è½‰æ›
try:
    import fitz
    PYMUPDF_AVAILABLE = True
except ImportError as e:
    PYMUPDF_AVAILABLE = False

import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from PIL import Image

def update_gui_with_docx_data():
    docx_data = st.session_state.get("docx_data", {})

    st.markdown("### ğŸ¤– Generated proposal")
    st.markdown(docx_data.get("proposal", ""))

    render_chemical_table(docx_data.get("chemicals", []))

    if docx_data.get("not_found"):
        st.markdown("### âš ï¸ ä»¥ä¸‹åŒ–å­¸å“æœªèƒ½æŸ¥è©¢æˆåŠŸ")
        for name in docx_data["not_found"]:
            st.markdown(f"- {name}")

    if docx_data.get("experiment_detail"):
        st.markdown("### ğŸ”¬ Suggested experiment details")
        st.markdown(docx_data["experiment_detail"])

    with st.expander("ğŸ“š Citations ", expanded=False):
        st.markdown("### ğŸ“š Citations ")
        for i, citation in enumerate(docx_data.get("citations", []), start=1):
            title = citation.get("title", "æœªçŸ¥")
            page = citation.get("page", "?")
            snippet = citation.get("snippet", "...")
            st.markdown(f"**[{i}]** {title} | é ç¢¼ï¼š{page} | æ®µè½é–‹é ­ï¼š{snippet}")
def format_references_block(text):
    refs = []
    in_reference = False
    for line in text.splitlines():
        if line.strip().lower().startswith("reference") or line.strip().lower().startswith("references"):
            in_reference = True
            continue
        if in_reference and line.strip():
            refs.append(line.strip())
    markdown_links = []
    for ref in refs:
        match = re.match(r"\[(\d+)\]\s*(.*?)(https?://\S+)", ref)
        if match:
            idx, title, url = match.groups()
            markdown_links.append(f"[{idx}] [{title.strip()}]({url.strip()})")
        else:
            markdown_links.append(ref)
    return markdown_links
def render_chemical_table(chemical_metadata_list):
    st.markdown("### ğŸ§ª Chemical Summary Table")

    for chem in chemical_metadata_list:
        cols = st.columns([2, 2, 3, 3])

        # Chemical Name
        with cols[0]:
            url = chem.get("pubchem_url")
            if url:
                st.markdown(f"[**{chem.get('name', 'Unknown')}**]({url})")
            else:
                st.markdown(f"**{chem.get('name', 'Unknown')}**")
            

        # Structure Image
        with cols[1]:
            st.image(chem.get("image_url", ""), width=200)

        # Properties block
        with cols[2]:
            st.markdown("**Properties**")
            st.markdown(f"- **Formula**: `{chem.get('formula', '-')}`")
            st.markdown(f"- **MW**: `{chem.get('weight', '-')}`")
            st.markdown(f"- **Boiling Point**: `{chem.get('boiling_point_c', '-')}`")
            st.markdown(f"- **Melting Point**: `{chem.get('melting_point_c', '-')}`")
            st.markdown(f"- **CAS No.**: `{chem.get('cas', '-')}`")
            st.markdown(f"- **SMILES**: `{chem.get('smiles', '-')}`")

        # Safety block (GHS + NFPA)
        with cols[3]:
            st.markdown("**Handling Safety**")
            if chem.get("safety_icons", {}).get("nfpa_image"):
                st.image(chem["safety_icons"]["nfpa_image"], width=60)

            ghs = chem.get("safety_icons", {}).get("ghs_icons", [])
            if ghs:
                st.image(ghs, width=50)

        st.markdown("---")
def get_nfpa_icon_image_stream(nfpa_code: str) -> io.BytesIO:
    """
    ç”¨ Headless Chrome æŠ“å– PubChem NFPA åœ–ï¼Œå›å‚³ BytesIO PNG åœ–ç‰‡
    """
    nfpa_svg_url = f"https://pubchem.ncbi.nlm.nih.gov/image/nfpa.cgi?code={nfpa_code}"

    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=300,300")
    chrome_options.add_argument("--hide-scrollbars")

    driver = webdriver.Chrome(options=chrome_options)
    driver.get(nfpa_svg_url)
    time.sleep(1)

    png_data = driver.get_screenshot_as_png()
    driver.quit()

    image = Image.open(io.BytesIO(png_data))
    cropped = image.crop((0, 0, 100, 100))  # ä½ å·²ç¶“æ¸¬è©¦å¥½çš„ç¯„åœ
    output = io.BytesIO()
    cropped.save(output, format="PNG")
    output.seek(0)
    return output

TAB_FLAGS = {
    "tab_1_proposal_generator" : True,
    "tab_2_embedding":True,
    "tab_3_search_pdf": False,
    "tab_4_perplexity_search": False,
    "tab_5_research_assitant": False
}

st.set_page_config(page_title="AI Chemist", layout="wide")
st.title("ğŸ§ª AI Chemist")

# æ ¹æ“š TAB_FLAGS å‹•æ…‹ç”Ÿæˆ tabs
tab_names = []
tab_keys = []

if TAB_FLAGS.get("tab_1_proposal_generator"):
    tab_names.append("Proposal generator")
    tab_keys.append("tab1")
if TAB_FLAGS.get("tab_2_embedding"):
    tab_names.append("ğŸ“¥ è«–æ–‡/å¯¦é©—è³‡æ–™ä¸Šå‚³")
    tab_keys.append("tab3")
if TAB_FLAGS.get("tab_3_search_pdf"):
    tab_names.append("ğŸ” æœå°‹å¤–éƒ¨æ–‡ç»ä¸¦ä¸‹è¼‰ PDF")
    tab_keys.append("tab2")
if TAB_FLAGS.get("tab_4_perplexity_search"):
    tab_names.append("ğŸŒ ä½¿ç”¨ Perplexity æœå°‹")
    tab_keys.append("tab4")
if TAB_FLAGS.get("tab_5_research_assitant"):
    tab_names.append("ğŸ“˜ çŸ¥è­˜åº«åŠ©ç†, archive functions")
    tab_keys.append("tab5")

tabs = st.tabs(tab_names)
tab_dict = dict(zip(tab_keys, tabs))


if TAB_FLAGS["tab_1_proposal_generator"]:
    with tab_dict["tab1"]:
        st.subheader("âœï¸ Proposal Generator")

        q1 = st.text_area("Please enter your research goalï¼š", height=100, key="search1")

        if st.button("âœï¸ Generate proposal", key="generate_proposal_btn"):
            with st.spinner("ğŸ“– Gathering information from knowledgebase and drafting proposal..."):
                result = agent_answer(q1, mode="make proposal")
                chemical_metadata_list, not_found_list, proposal_answer = chemical_metadata_extractor(result["answer"])

                st.session_state["docx_data"] = {
                    "proposal": proposal_answer,
                    "chemicals": chemical_metadata_list,
                    "not_found": not_found_list,
                    "citations": result.get("citations", []),
                    "experiment_detail": ""
                }
                st.session_state["proposal_chunks"] = result.get("chunks", [])
                st.rerun()

        if "docx_data" in st.session_state:
            update_gui_with_docx_data()
        
            st.markdown("### ğŸ’¡ Don't like the proposalï¼Ÿ Provide your opinion here")
            user_reason = st.text_input("How you want to revise?", key="revise_reason")

            if st.button("ğŸ’¡ Generate New Idea", key="new_idea_btn"): #ä¿®æ”¹proposal
                with st.spinner("ğŸ”„ Revise proposal based on your reason and knowledgebase..."):
                    old_chunks = st.session_state.get("proposal_chunks", [])
                    past_proposal = st.session_state["docx_data"].get("proposal", "")
                    result = agent_answer(
                        user_reason,
                        mode="generate new idea",
                        chunks=old_chunks,
                        proposal=past_proposal
                    )
                    chemical_metadata_list, not_found_list, proposal_answer = chemical_metadata_extractor(result["answer"])
                    st.session_state["docx_data"].update({
                        "proposal": proposal_answer,
                        "chemicals": chemical_metadata_list,
                        "not_found": not_found_list,
                        "citations": result.get("citations", []),
                        "experiment_detail": ""
                    })
                    st.session_state["proposal_chunks"] = result.get("chunks", [])
                    st.rerun()

            if st.button("âœ… Accept & Generate Experiment Detail", key="accept_btn"): #ç”Ÿæˆå¯¦é©—ç´°ç¯€
                        with st.spinner("ğŸ§ª Generating experimental details..."):
                            chunks = st.session_state.get("proposal_chunks", [])
                            proposal = st.session_state["docx_data"].get("proposal", "")
                            result = agent_answer(
                                "",
                                mode="expand to experiment detail",
                                chunks=chunks,
                                proposal=proposal
                            )
                            st.session_state["docx_data"]["experiment_detail"] = result["answer"]
                            st.rerun()
                            
            if st.button("ğŸ“¥ Prepare proposal.docx"): #ä¸‹è¼‰æ–‡ä»¶æŒ‰éˆ•
                with st.spinner("ğŸ”„ Preparing your proposal..."):
                    docx_data = st.session_state.get("docx_data", {})
                    doc = Document()
                    doc.add_heading("AI Generated Research Proposal", 0)

                    # æ¸…ç†æ–‡æœ¬ä»¥ç¢ºä¿XMLå…¼å®¹æ€§
                    def clean_text_for_xml(text):
                        """æ¸…ç†æ–‡æœ¬ä»¥ç¢ºä¿XMLå…¼å®¹æ€§"""
                        if not text:
                            return ""
                        # ç§»é™¤NULLå­—ç¯€å’Œæ§åˆ¶å­—ç¬¦
                        cleaned = "".join(char for char in text if ord(char) >= 32 or char in '\n\r\t')
                        # ç§»é™¤å…¶ä»–å¯èƒ½å°è‡´XMLå•é¡Œçš„å­—ç¬¦
                        cleaned = cleaned.replace('\x00', '')  # NULLå­—ç¯€
                        cleaned = cleaned.replace('\x01', '')  # SOH
                        cleaned = cleaned.replace('\x02', '')  # STX
                        cleaned = cleaned.replace('\x03', '')  # ETX
                        cleaned = cleaned.replace('\x04', '')  # EOT
                        cleaned = cleaned.replace('\x05', '')  # ENQ
                        cleaned = cleaned.replace('\x06', '')  # ACK
                        cleaned = cleaned.replace('\x07', '')  # BEL
                        cleaned = cleaned.replace('\x08', '')  # BS
                        cleaned = cleaned.replace('\x0b', '')  # VT
                        cleaned = cleaned.replace('\x0c', '')  # FF
                        cleaned = cleaned.replace('\x0e', '')  # SO
                        cleaned = cleaned.replace('\x0f', '')  # SI
                        return cleaned

                    # Proposal Section
                    doc.add_heading("Proposal", level=1)
                    proposal_text = clean_text_for_xml(docx_data.get("proposal", ""))
                    doc.add_paragraph(proposal_text)

                    # Chemical Table
                    doc.add_heading("Chemical Summary Table", level=1)
                    table = doc.add_table(rows=1, cols=8)  # å¤šå…©æ¬„ï¼šStructure + Safety
                    hdr = table.rows[0].cells
                    hdr[0].text = "Structure"
                    hdr[1].text = "Name"
                    hdr[2].text = "Formula"
                    hdr[3].text = "MW"
                    hdr[4].text = "Boiling Point (Â°C)"
                    hdr[5].text = "Melting Point (Â°C)"
                    hdr[6].text = "CAS No."
                    hdr[7].text = "Safety Icons"

                    for chem in docx_data.get("chemicals", []):
                        row = table.add_row().cells

                        # çµæ§‹åœ–ç‰‡
                        img_url = chem.get("image_url")
                        if img_url:
                            try:
                                response = requests.get(img_url, verify=False, timeout=5)
                                if response.status_code == 200:
                                    img_stream = BytesIO(response.content)
                                    row[0].paragraphs[0].add_run().add_picture(img_stream, width=Inches(1))
                                else:
                                    row[0].text = "Image not found"
                            except:
                                row[0].text = "Image error"
                        else:
                            row[0].text = "No image"

                        # æ–‡å­—æ¬„ä½ - ä½¿ç”¨æ¸…ç†å‡½æ•¸
                        row[1].text = clean_text_for_xml(chem.get("name", "-") or "-")
                        row[2].text = clean_text_for_xml(chem.get("formula", "-") or "-")
                        row[3].text = clean_text_for_xml(str(chem.get("weight", "-") or "-"))
                        row[4].text = clean_text_for_xml(str(chem.get("boiling_point_c", "-") or "-"))
                        row[5].text = clean_text_for_xml(str(chem.get("melting_point_c", "-") or "-"))
                        row[6].text = clean_text_for_xml(chem.get("cas", "-") or "-")

                        # Safety iconsï¼ˆæ”¯æ´å¤šå¼µ GHS + 1 å¼µ NFPAï¼‰
                        icons_cell = row[7].paragraphs[0]
                        ghs_icons = chem.get("safety_icons", {}).get("ghs_icons", [])
                        nfpa_icon_url = chem.get("safety_icons", {}).get("nfpa_image")
                        for icon_url in ghs_icons:
                            try:
                                icon_resp = requests.get(icon_url, verify=False, timeout=5)
                                if icon_resp.status_code == 200:
                                    # ä½¿ç”¨ svglib + PyMuPDF è½‰æ› SVG ç‚º PNG
                                    if SVGLIB_AVAILABLE and PYMUPDF_AVAILABLE:
                                        try:
                                            # æ­¥é©Ÿ1ï¼šSVG è½‰ PDF
                                            drawing = svg2rlg(io.BytesIO(icon_resp.content))
                                            if drawing:
                                                # å‰µå»ºè‡¨æ™‚ PDF æ–‡ä»¶
                                                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_pdf:
                                                    renderPDF.drawToFile(drawing, tmp_pdf.name)
                                                
                                                # æ­¥é©Ÿ2ï¼šPDF è½‰ PNG
                                                pdf_document = fitz.open(tmp_pdf.name)
                                                page = pdf_document[0]
                                                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x ç¸®æ”¾
                                                
                                                # å°‡ PNG è½‰æ›ç‚º BytesIO
                                                png_stream = io.BytesIO()
                                                png_stream.write(pix.tobytes("png"))
                                                png_stream.seek(0)
                                                
                                                # æ’å…¥åˆ° Word æ–‡æª”
                                                run = icons_cell.add_run()
                                                run.add_picture(png_stream, width=Inches(0.3))
                                                
                                                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                                                pdf_document.close()
                                                os.unlink(tmp_pdf.name)
                                                
                                            else:
                                                print(f"âš ï¸ SVG è½‰æ›å¤±æ•— - drawing ç‚º None: {icon_url}")
                                        except Exception as e:
                                            print(f"âš ï¸ SVG è½‰æ›éŒ¯èª¤: {icon_url}, {e}")
                                    else:
                                        print(f"âš ï¸ svglib æˆ– PyMuPDF ä¸å¯ç”¨ï¼Œç„¡æ³•è½‰æ› SVG: {icon_url}")
                            except Exception as e:
                                print(f"âš ï¸ Failed to convert or insert icon: {icon_url}, {e}")

                        

                        if nfpa_icon_url:
                            try:
                                # å¾ URL æ“·å–å‡º NFPA codeï¼Œä¾‹å¦‚ https://...code=130
                                from urllib.parse import urlparse, parse_qs
                                parsed = urlparse(nfpa_icon_url)
                                nfpa_code = parse_qs(parsed.query).get("code", [""])[0]

                                if nfpa_code:
                                    image_stream = get_nfpa_icon_image_stream(nfpa_code)
                                    if image_stream:
                                        run = icons_cell.add_run()
                                        run.add_picture(image_stream, width=Inches(0.3))
                                else:
                                    print(f"âš ï¸ ç„¡æ³•å¾ URL æ“·å– NFPA code: {nfpa_icon_url}")
                            except Exception as e:
                                print(f"âš ï¸ Failed to convert or insert NFPA icon: {nfpa_icon_url}, {e}")
                            
                            


                    # Not Found Chemicals
                    not_found = docx_data.get("not_found", [])
                    if not_found:
                        doc.add_heading("Not Found Chemicals", level=2)
                        for name in not_found:
                            doc.add_paragraph(f"- {clean_text_for_xml(name)}")

                    # Experiment Details
                    doc.add_heading("Experimental Plan", level=1)
                    experiment_text = clean_text_for_xml(docx_data.get("experiment_detail", ""))
                    doc.add_paragraph(experiment_text)

                    # Citations
                    doc.add_heading("Citations", level=1)
                    for i, c in enumerate(docx_data.get("citations", []), 1):
                        title = clean_text_for_xml(c.get('title', ''))
                        page = clean_text_for_xml(str(c.get('page', '')))
                        snippet = clean_text_for_xml(c.get('snippet', ''))
                        doc.add_paragraph(f"[{i}] {title} | Page {page} | Snippet: {snippet}")

                    # Save and Download
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
                        doc.save(tmp.name)
                        tmp_path = tmp.name

                    with open(tmp_path, "rb") as f:
                        st.download_button(
                            label="ğŸ“¥ Click to download",
                            data=f,
                            file_name="proposal_report.docx",
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                        )

                    os.unlink(tmp_path)
        


if TAB_FLAGS["tab_2_embedding"]:
    with tab_dict["tab3"]:
        if "processed_files" not in st.session_state:
            st.session_state.processed_files = set()


        file_info = select_files() 
        if file_info:
            file_type = file_info["type"] 
            
            st.markdown(f"ğŸ” è³‡æ–™é¡å‹ï¼š{file_type}")
            for f in file_info["files"]:
                st.write(f"ğŸ“„ {f.name}")

            if file_type == "ğŸ“„ è«–æ–‡è³‡æ–™":
                new_files = [f for f in file_info["files"] if f.name not in st.session_state.processed_files]

                if not new_files:
                    st.info("âœ… æ‰€æœ‰æª”æ¡ˆéƒ½å·²è™•ç†éï¼Œä¸é‡è¤‡è™•ç†ã€‚")
                else:
                    with st.spinner("ğŸ“¥ è™•ç†è«–æ–‡è³‡æ–™ä¸­..."):
                        new_file_paths = []
                        messages = []

                        def update_status(msg):
                            messages.append(msg)

                        for f in new_files:
                            suffix = os.path.splitext(f.name)[1]
                            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                                tmp.write(f.read())
                                tmp_path = tmp.name
                                new_file_paths.append(tmp_path)
                                st.session_state.processed_files.add(f.name)

                        metadata_list = process_uploaded_files(new_file_paths, status_callback=update_status)
                        embed_documents_from_metadata(metadata_list)

                        if messages:
                            with st.expander("ğŸ“‹ è™•ç†ç´€éŒ„", expanded=True):
                                for m in messages:
                                    st.markdown(f"- {m}")

            elif file_type == "ğŸ§ª å¯¦é©—æ•¸æ“š":
                with st.spinner("ğŸ“¥ é–‹å§‹è™•ç†å¯¦é©—è³‡æ–™..."):
                    for f in file_info["files"]:  # æ¯æ¬¡éƒ½è™•ç†ï¼Œä¸è·³é
                        temp_path = os.path.join("uploaded", f.name)
                        os.makedirs("uploaded", exist_ok=True)
                        with open(temp_path, "wb") as tmp:
                            tmp.write(f.read())

                        result_df, txt_paths = export_new_experiments_to_txt(
                            excel_path=temp_path,
                            output_dir=EXPERIMENT_DIR
                        )

                        

                    embed_experiment_txt_batch(txt_paths)
                    st.success(f"âœ… å·²è™•ç†ï¼š{f.name}ï¼Œå…± {len(txt_paths)} ç­†")
                    st.dataframe(result_df)

if TAB_FLAGS["tab_4_perplexity_search"]:
    with tab_dict["tab3"]:
        st.subheader("ğŸ” åŠŸèƒ½ 2ï¼šä½¿ç”¨é—œéµå­—æœå°‹å¤–éƒ¨æ–‡ç»ä¸¦ä¸‹è¼‰ PDF")
        q2 = st.text_area("è«‹è¼¸å…¥æŸ¥è©¢å•é¡Œï¼ˆå°‡è‡ªå‹•èƒå–é—œéµå­—ï¼‰ï¼š", height=100, key="search_pdf")
        if st.button("åŸ·è¡ŒæŸ¥è©¢èˆ‡ä¸‹è¼‰", key="downloadbtn"):
            with st.spinner("ğŸ” æŸ¥è©¢ä¸¦ä¸‹è¼‰ä¸­..."):
                pdfs = search_and_download_only(q2, top_k=5, storage_dir="data/paper")
                if pdfs:
                    st.success(f"âœ… å…±ä¸‹è¼‰ {len(pdfs)} ç¯‡ PDF")
                    for path in pdfs:
                        st.markdown(f"- `{os.path.basename(path)}`")
                else:
                    st.warning("âš ï¸ æœªæˆåŠŸä¸‹è¼‰ä»»ä½• PDF")
if TAB_FLAGS["tab_3_search_pdf"]:
    with tab_dict["tab3"]:
        if "processed_files" not in st.session_state:
            st.session_state.processed_files = set()


        file_info = select_files_paper_mode() 
        print(file_info)
        if file_info["files"]:
            file_type = file_info["type"] 
            
            st.markdown(f"ğŸ” è³‡æ–™é¡å‹ï¼š{file_type}")
            for f in file_info["files"]:
                st.write(f"ğŸ“„ {f.name}")

            new_files = [f for f in file_info["files"] if f.name not in st.session_state.processed_files]

            if not new_files:
                st.info("âœ… æ‰€æœ‰æª”æ¡ˆéƒ½å·²è™•ç†éï¼Œä¸é‡è¤‡è™•ç†ã€‚")
            else:
                with st.spinner("ğŸ“¥ è™•ç†è«–æ–‡è³‡æ–™ä¸­..."):
                    new_file_paths = []
                    messages = []

                    def update_status(msg):
                        messages.append(msg)

                    for f in new_files:
                        suffix = os.path.splitext(f.name)[1]
                        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                            tmp.write(f.read())
                            tmp_path = tmp.name
                            new_file_paths.append(tmp_path)
                            st.session_state.processed_files.add(f.name)

                    metadata_list = process_uploaded_files(new_file_paths, status_callback=update_status)
                    embed_documents_from_metadata(metadata_list)

                    if messages:
                        with st.expander("ğŸ“‹ è™•ç†ç´€éŒ„", expanded=True):
                            for m in messages:
                                st.markdown(f"- {m}")

if TAB_FLAGS["tab_4_perplexity_search"]:
    with tab_dict["tab4"]:
        st.subheader("ğŸŒ åŠŸèƒ½ 4ï¼šä½¿ç”¨ Perplexity æœå°‹æ–‡ç»")
        q4 = st.text_area("è«‹è¼¸å…¥è¦æœå°‹çš„æ–‡ç»å•é¡Œï¼š", height=100, key="search4")
        if st.button("ä½¿ç”¨ Perplexity æœå°‹", key="perplexitybtn"):
            with st.spinner("ä½¿ç”¨ Perplexity æœå°‹ä¸­..."):
                result = ask_perplexity(q4)
                if result["success"]:
                    st.markdown("### ğŸ¤– æ–‡ç»æ‘˜è¦")
                    st.write(result["answer"])

                    st.markdown("### ğŸ”— å¼•ç”¨ä¾†æº")
                    links = format_references_block(result["answer"])
                    if links:
                        for link in links:
                            st.markdown(f"- {link}")
                    else:
                        st.info("æœªåµæ¸¬åˆ°çµæ§‹åŒ– Reference å€å¡Šã€‚")
                else:
                    st.error("âŒ æœå°‹å¤±æ•—ï¼š" + result["error"])

if TAB_FLAGS["tab_5_research_assitant"]:
    with tab_dict["tab5"]   :
        st.subheader("ğŸ“˜ åŠŸèƒ½ 1ï¼šåˆ©ç”¨çŸ¥è­˜åº«å›ç­”å•é¡Œ")
        st.markdown("### âš™ï¸ é¸æ“‡å›ç­”æ¨¡å¼")
        answer_mode = st.radio(
            "è«‹é¸æ“‡ç³»çµ±å›ç­”å•é¡Œçš„æ–¹å¼ï¼š",
            options=[
                "åƒ…åš´è¬¹æ–‡ç»æº¯æº",
                "å…è¨±å»¶ä¼¸èˆ‡æ¨è«–",
                "ç´å…¥å¯¦é©—è³‡æ–™ï¼Œé€²è¡Œæ¨è«–èˆ‡å»ºè­°"
            ],
            index=0,
            key="mode_selector"
        )

        q1 = st.text_area("è«‹è¼¸å…¥ç ”ç©¶å•é¡Œï¼š", height=100, key="search_2")

        if st.button("ç”±çŸ¥è­˜åº«å›ç­”", key="knowledgebtn"):
            with st.spinner("æŸ¥è©¢çŸ¥è­˜åº«ä¸­..."):
                result = agent_answer(q1, mode = answer_mode)
                st.session_state["proposal_chunks"] = result.get("chunks", [])
                st.session_state["proposal_answer"] = result.get("answer")
                st.session_state["result"] = result
        
            st.markdown("### ğŸ¤– å›ç­”")
            st.markdown(st.session_state["proposal_answer"])




            # ğŸ“š å¼•ç”¨è³‡æ–™ï¼ˆæ”¯æ´åŸå§‹æˆ–æ”¹å¯«çš†å¯ï¼‰
            st.markdown("### ğŸ“š å¼•ç”¨è³‡æ–™")
            for i, citation in enumerate(st.session_state.get("result", {}).get("citations", []), start=1):
                title = citation.get("title", "æœªçŸ¥")
                page = citation.get("page", "?")
                snippet = citation.get("snippet", "...")
                st.markdown(f"**[{i}]** {title} | é ç¢¼ï¼š{page} | æ®µè½é–‹é ­ï¼š{snippet}")
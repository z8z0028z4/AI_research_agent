import os
import fitz
import docx
from dotenv import load_dotenv
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("âš ï¸ è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š OPENAI_API_KEY")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "..", "experiment_data", "papers")
INDEX_DIR = os.path.join(BASE_DIR, "..", "experiment_data", "vector_index")

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    return "\n".join([page.get_text() for page in doc])

def extract_text_from_docx(docx_path):
    doc = docx.Document(docx_path)
    return "\n".join([para.text for para in doc.paragraphs if para.text.strip()])

def embed_documents():
    embeddings = OpenAIEmbeddings()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    documents = []

    print(f"ğŸ“‚ æƒæè³‡æ–™å¤¾ï¼š{DATA_DIR}")
    for filename in os.listdir(DATA_DIR):
        filepath = os.path.join(DATA_DIR, filename)

        if filename.endswith(".pdf"):
            print(f"ğŸ” è®€å– PDFï¼š{filename}")
            text = extract_text_from_pdf(filepath)
        elif filename.endswith(".docx"):
            print(f"ğŸ” è®€å– Wordï¼š{filename}")
            text = extract_text_from_docx(filepath)
        else:
            print(f"âš ï¸ å¿½ç•¥ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼š{filename}")
            continue

        chunks = text_splitter.create_documents([text])
        documents.extend(chunks)

    if documents:
        db = Chroma.from_documents(documents, embeddings, persist_directory=INDEX_DIR)
        db.persist()
        print(f"âœ… åµŒå…¥å®Œæˆä¸¦å„²å­˜è‡³ï¼š{INDEX_DIR}")
    else:
        print("âš ï¸ æ²’æœ‰å¯ç”¨çš„æ–‡æª”å»ºç«‹èªæ„åµŒå…¥ã€‚")

if __name__ == "__main__":
    embed_documents()